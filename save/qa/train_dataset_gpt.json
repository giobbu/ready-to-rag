{
    "queries": {
        "b9b5d740-3d20-4d46-890b-0c18cbb7dc4a": "What is the main focus of the survey on explainable anomaly detection conducted by Zhong Li, Yuxuan Zhu, and Matthijs van Leeuwen?",
        "09d959c6-ec54-46bc-b6c1-4b1ace621611": "Why is model explainability considered a main issue in the adoption of data-driven algorithms in industrial environments, especially in safety critical domains?",
        "2147e747-6233-494a-8c77-b739e6f52035": "Explain the three roles involved in anomaly analysis and their respective definitions of what constitutes an anomaly. How do these roles differ in their perspectives on anomalies in the context of anomaly detection systems?",
        "c5893a8a-4171-44c9-bffa-356b348eebf4": "Define the concept of \"Explainable Anomaly Detection\" based on the information provided in the document.",
        "05909fc4-dd28-4c1c-aa28-13c6998ac6da": "Explain the difference between Case 1 (Model) and Case 2 (Data) in the context of explainable anomaly detection, and discuss the focus of each case in terms of what is being explained.",
        "01e8cbec-973e-4feb-bccb-81107df3d6d0": "Explain why the anomaly detection community has historically focused more on detection accuracy rather than providing explanations for anomalies.",
        "2388a10b-b564-4fca-a806-8d43cfec0a1b": "What are the four most notable existing categorizations of anomaly explanation methods in the field of Explainable Anomaly Detection (XAD)?",
        "cb5ef1da-33c5-41de-9529-675d298b5bda": "Explain the difference between pre-model techniques, in-model techniques, and post-model techniques in anomaly detection.",
        "22964743-9670-4038-8962-391669062862": "Explain the difference between perturbation-based methods and reconstruction error-based methods in explainable anomaly detection, providing examples for each category.",
        "dd685878-78bc-436d-8d71-7fba439e7c75": "Explain the structure of the core of the survey as indicated in Figure 3 and how it organizes the literature review based on six criteria.",
        "07df230a-a373-468a-9493-a22337ed75ca": "How do filter-based unsupervised feature selection methods, such as CBRW_FS and CBRW, contribute to anomaly detection in categorical data?",
        "995aa4b7-c7f1-4cc3-8327-2df3ba861a18": "How do Dissanayake et al. propose to detect abnormal heart sounds, and what are the three main steps involved in their automated detection method?",
        "aa6ad1b1-72bc-4941-aae6-0f8d8e0fb94e": "Explain the difference between model-agnostic and model-specific techniques in the context of explainable anomaly detection.",
        "6c743cc9-bb68-47c8-bbfc-b9b7a75b4554": "How do transparent models in supervised learning, such as Linear Models and Decision Trees, contribute to explainable anomaly detection?",
        "f9e91ac5-bb19-4fd5-beb3-6c742e9dbcaf": "How does the ADAR model leverage association rule mining to detect and explain anomalies in process runtime behavior?",
        "280e4ec6-f5e8-481d-89c8-7155403ba06a": "How do researchers leverage regularized Logistic Regression to identify anomalies in time series data?",
        "b460307f-91f1-4057-ad51-20c42a0e4bcf": "How do Slavic et al. utilize DBNs in the Autonomous Driving domain to identify abnormal motion behaviors?",
        "08224ff3-4300-4c9a-99bd-03b92f168c7a": "How do Keller et al. and Dang et al. differ in their approaches to subspace anomaly detection?",
        "79afba5c-7b84-4dcd-b990-3c7a3b27d04a": "Explain the three steps involved in the RefOut method proposed by Keller et al. for anomaly detection using random search strategies.",
        "a6af8e49-25dc-4c5a-9f80-61bc476243f0": "How do the subspace anomaly detection methods discussed in the text provide explanations for anomalies identified in data instances?",
        "93d1a92c-9a3f-4b5d-80d0-2293da380d00": "How do distribution-based anomaly detection techniques differ from other approaches in terms of explainability?",
        "21a844cc-10ef-4257-b68b-87ab2d53c606": "How does the construction of a Minimum Probability Flow (MPF) forest contribute to density estimation and anomaly detection in the context of the given research?",
        "8057ceb3-cd30-470d-a3f3-4a8a55922b9d": "How do Cheng et al. identify spatiotemporal anomalies in human activity using time series decomposition techniques?",
        "2d0adb33-d061-4f3d-9920-1a0d40826186": "What are the pros and cons of using frequent pattern mining for anomaly detection according to the table provided?",
        "0ac8c3d8-a465-4430-9c41-225dcb9c6318": "Explain the pros and cons of using Generalized Additive Models for anomaly detection, as discussed in the table.",
        "175042ce-1cd9-427f-892b-ef26b3aa6f75": "What are the different types of explanations provided by the anomaly detection methods discussed in the survey?",
        "07df516b-d0c2-45c9-bbcf-79bd8157233c": "Explain the challenges associated with explaining decision tree-based models in supervised learning and how feature selection and ensemble techniques can help address these challenges.",
        "9edb8484-c9bf-4d79-b8dd-fb19e2f8e8ef": "Explain the difference between subspace-based anomaly detection methods that assume the availability of anomaly detection models and those that do not, providing examples from the text.",
        "3f49b118-c6a9-49f0-84a0-bf1f86a62394": "Explain the concept of anomaly explanation in the context of OutRules framework.",
        "aa632ad6-427e-4933-a11e-450aceb4d198": "How does the LOOKOUT approach proposed by Gupta et al. aim to explain anomalies in a dataset using 2-dimensional subspaces?",
        "6372015d-90bc-430c-87c7-38d608a805dd": "What limitation of exhaustive search in exponentially large space do the authors propose to overcome in their framework?",
        "7a5dbe1e-273b-4618-86e6-c5c4bdb21f54": "How do Duan et al. approach finding the minimal outlying subspace for a given data instance in anomaly detection, and what techniques do they utilize in their method?",
        "1501b40d-d1fd-4d42-b26c-e0ad8da03b9b": "How does the GLAD algorithm identify the base-learners that contribute the most to the decision in anomaly detection ensembles?",
        "c339370a-a5cb-44e8-8a49-de516cbeefc2": "How does the ensemble-based anomaly detection model proposed by Kiefer & Pesch aim to identify and interpret anomalies in financial auditing data?",
        "9d7a180f-977b-42a6-b86d-9ba9725808b6": "Explain the COIN framework proposed by Liu et al. for explaining anomalies in tabular data.",
        "8d64785e-e7e2-4fe5-a09d-6f5f2c2ae227": "How do Babenko & Pastore leverage LFA to detect anomalies in system logs, and what is the purpose of their Automata Violation Analyzer (AVA)?",
        "e6d0023e-a10d-4fa1-a9f7-ff644fe71e72": "How does the tool described in the text use historical mobility data to identify abnormalities in transportation networks?",
        "e91f3272-439f-473e-acda-6e94104e86eb": "How do model-agnostic techniques differ from model-specific techniques in explainable anomaly detection?",
        "582ad1a8-242b-4890-8483-c5c56f613495": "What are some advantages and disadvantages of using shallow post-model techniques for anomaly detection, as outlined in Table 3 of the provided context information?",
        "e54c3929-f1fb-4bd0-8323-c2da517eac65": "Explain the difference between model-agnostic (A) and model-specific (S) methods in the context of anomaly detection, providing examples from the table.",
        "85536627-76b7-4dd2-a2b9-b2f225aa50ca": "How do AutoEncoders work in anomaly detection, and what is the underlying assumption behind their effectiveness in detecting anomalies?",
        "ef9807ae-f1d6-4466-9b09-d2cb963274a3": "Explain the SAIFE method proposed by Rajendran et al. for anomaly detection in wireless spectrum monitoring. How does it utilize the three phases of AAE training?",
        "a5b416f6-60d0-4092-ae30-d5f283b8288b": "Question: How do researchers utilize surrogate models such as LIME and rule learners to explain anomalies in anomaly detection systems, as described in the provided text?",
        "e6ca1657-8d41-4d9a-94a8-4c39ea0b8b69": "How do Szymanowicz et al. detect and explain anomalous events in videos using an encoder-decoder architecture based on U-Net?",
        "b62d382a-ad7b-40c6-84b9-b53cd6694695": "How do Li et al. utilize a genetic algorithm (GA) in their VAGA framework for anomaly detection in high-dimensional data?",
        "5b712909-bc36-4395-ba16-a952d0d437b5": "How do Hwang & Lee propose using SHAP values in their bidirectional stackable LSTM-based anomaly detection model for industrial control systems?",
        "765eb191-9773-4c8e-908b-c326cab2a203": "How do researchers utilize surrogate models such as LIME to explain anomalies in RNN-based systems, and what are the benefits of using these models?",
        "636dff4c-d22c-4c75-8838-9bc95968f5cb": "How does the AnoMili system detect anomalies and provide real-time explanations?",
        "307dd3d0-4c5d-4a2a-bf34-8caeee36e0f0": "How do surrogate models in post-hoc explanation methods potentially impact the fidelity of explanations in anomaly detection using CNNs?",
        "4b8ed432-4591-4b48-8c4d-7df0de2542b1": "Explain how the ATON model utilizes feature embedding and self-attention learning to interpret anomalies in a post-hoc fashion.",
        "f82b0d77-a0c0-4018-9c06-0335cdc6d72b": "How do deep post-model XAD techniques differ from shallow post-model XAD techniques in terms of model specificity and fidelity?",
        "cc6b7390-7a6c-4899-a011-7a5b2d7134ca": "What are the pros and cons of using reconstruction error-based feature contribution with sparse optimization in anomaly detection, as discussed in the surveyed deep post-model techniques?",
        "c35427f8-fb6b-4946-8821-3579537332ea": "Question: What are the pros and cons of using the LRP based feature relevance method in deep neural networks according to the information provided in Table 4?",
        "320d29cf-4389-473a-a5e5-01558549a629": "How does the method described in reference [160] for anomaly detection differ from the method in reference [125] in terms of model specificity and training approach?",
        "fdc4d8b3-75f6-4eba-a1c1-8cb234027539": "How can the fidelity of post-hoc explanations in anomaly detection techniques be evaluated, and what potential issues may arise from inconsistencies between different definitions?",
        "1ea72e5c-584e-47d4-bea6-7fd81fc6407b": "How do subspace anomaly detection and Shapley value-based methods contribute to explainable anomaly detection techniques, and what are the challenges associated with their high computational cost?",
        "af381815-6128-405d-9c30-89e640dfac7c": "Explain the concept of explainable artificial intelligence (XAI) as discussed in the document. What are the opportunities and challenges towards responsible AI highlighted in the text?",
        "09010e7e-f901-4c35-ab95-b8d0a32a5608": "How does abnormal subspace sparse PCA contribute to anomaly detection and interpretation according to Xingyan Bin, Ying Zhao, and Bilong Shen's research?",
        "c0bb0766-cf16-4efe-b850-164dd62b7772": "What are some common techniques used for anomaly detection according to the research papers mentioned in the context information?",
        "378affac-977a-4811-b085-3c048df38916": "Question: In the field of anomaly detection, what are some key topics covered in the research papers mentioned in the context information?",
        "981d752b-a75a-49d3-b7c7-da40acf4796e": "How do Mondrian Polya forests contribute to interpretable anomaly detection on data streams according to Charlie Dickens et al. (2020)?",
        "08424a55-d242-4f8c-9e09-0109fd24abb0": "How do convolutional neural networks contribute to unsupervised anomaly detection in text data, as discussed in the document?",
        "900f5db1-f5b2-4167-8b32-4d8056c91d19": "How does the FP-outlier method utilize frequent patterns for outlier detection, as discussed in the document?",
        "1b778f33-2062-4865-8047-c2ebdeb413f2": "Explain the concept of infusing interpretability in isolation forest for anomaly detection as discussed in the paper by Nirmal Sobha Kartha, Cl\u00e9ment Gautrais, and Vincent Vercruyssen.",
        "38214c0b-28a6-49ab-b858-572e97dc53ea": "In the field of outlier detection, what are some methods discussed in the document for interpreting and clustering outliers?",
        "cff4fd4b-6f27-490f-9b5e-657dc0f9cca1": "Explain the concept of interpretability in machine learning as discussed in the paper by Zachary C. Lipton.",
        "b26c98d4-c471-4ba0-b2db-d91f87d605fe": "How do adversarial autoencoders differ from traditional autoencoders, and what application was discussed in the document related to this topic?",
        "f90d20b3-b8d4-4906-bc90-0896858320f3": "How does the OutRules framework contribute to outlier descriptions in multiple context spaces according to Emmanuel M\u00fcller et al. (2012)?",
        "47cf483e-f8f3-4a83-9e57-d6a24508d403": "Explain the concept of explainable deep anomaly detection and discuss its importance in the field of data mining, referencing the work of Guansong Pang and Charu Aggarwal (2021).",
        "4c0546fc-f53e-406c-a9d8-a3cbbd3c7fd0": "How does the SAIFE system approach unsupervised wireless spectrum anomaly detection, and what are the key features that make it interpretable?",
        "6ca70978-369b-4a59-9cad-f284f2aff38d": "What are some examples of interpretable models that have been proposed as alternatives to black box machine learning models for high stakes decisions in the field of anomaly detection?",
        "8ce9943e-659a-43ee-941a-a32f846718d5": "How does the approach of \"Time series anomaly discovery with grammar-based compression\" differ from other methods discussed in the document?",
        "e1ecc853-99a1-42bf-a104-957818dca9b9": "Explain the concept of human-in-the-loop anomaly detection and explanation as discussed in the paper \"PANDA\" by Gr\u00e9gory Smits et al. (2022).",
        "4db53b90-902e-426a-86b8-092158996fba": "Explain the concept of anomaly localization in images as discussed in the paper by Venkataramanan et al. (2020).",
        "ac4f0ac1-0a5b-470f-ad54-e6a76c34e65c": "Explain the concept of anomaly detection in time series data and discuss the approach proposed by Luca Zancato et al. in their paper \"STRIC: Stacked residuals of interpretable components for time series anomaly detection.\""
    },
    "corpus": {
        "f5e9b333-29ef-4258-8867-5f7a1ef70c58": "23\nA Survey on Explainable Anomaly Detection\nZHONG LI, YUXUAN ZHU,a n dMATTHIJS VAN LEEUWEN, Leiden Institute of Advanced\nComputer Science (LIACS), Leiden University, The Netherlands\nIn the past two decades, most research on anomaly detection has focused on improving the accuracy of\nthe detection, while largely ignoring the explainability of the corresponding methods and thus leaving the\nexplanation of outcomes to practitioners. As anomaly detection algorithms are increasingly used in safety-\ncriticaldomains,providingexplanationsforthehigh-stakesdecisionsmadeinthosedomainshasbecomean\nethicalandregulatoryrequirement.Therefore,thisworkprovidesacomprehensiveandstructuredsurveyon\nstate-of-the-artexplainableanomalydetectiontechniques.Weproposeataxonomybasedonthemainaspects\nthatcharacteriseeachexplainableanomalydetectiontechnique,aimingtohelppractitionersandresearchers\nfind the explainable anomaly detection method that best suits their needs.\nCCS Concepts: \u2022Information systems\u2192Decision support systems; Data analytics;Data mining;\nAdditional Key Words and Phrases: Explainable anomaly detection, interpretable anomaly detection, anom-\naly explanation, anomaly detection, outlier detection, explainable machine learning, explainable artificial\nintelligence\nACM Reference format:\nZhongLi, Yuxuan Zhu,andMatthijsvan Leeuwen. 2023. ASurvey onExplainable Anomaly Detection.ACM\nTrans. Knowl. Discov. Data.18, 1, Article 23 (September 2023), 54 pages.\nhttps://doi.org/10.1145/3609333\n1 INTRODUCTION\nAn anomaly is an object that is notably different from the majority of the remaining objects. De-\npending on the specific application domain, an anomaly can also be called an outlier or a novelty.\nMoreover, it may also be known as an unusual, irregular, atypical, inconsistent, unexpected, rare,\nerroneous,faulty,fraudulent,malicious,unnatural,orstrangeobject[ 181].Exceptforafewworks\nsuchasReference[ 181],theterm outlier isoftenusedasasynonymfor anomaly inmostresearch.\nFor consistency, we will use the termanomaly in this article.\nSince the seminal work in [105], anomaly detection has been well studied and there exists a\nplethora of comprehensive surveys and reviews on it, including but not limited to References\n[1, 5, 25, 36, 37, 134, 135, 161, 165, 231]. In contrast, we only found a handful of surveys\n[162, 189, 225] about theexplainability of anomaly detection methods. As suggested by Langone\nThis publication is part of the project Digital Twin with project number P18-03 of the research programme TTW Perspec-\ntive, which is (partly) financed by the Dutch Research Council (NWO). We thank Dr. Gabriel de Albuquerque Gleizer for\nhis valuable feedback.\nAuthors\u2019 address: Z. Li, Y. Zhu, and M. van Leeuwen, Leiden Institute of Advanced Computer Science (LIACS), Leiden\nUniversity,SnelliusGebouw,NielsBohrweg1,Leiden2333CA,TheNetherlands;emails:z.li@liacs.leidenuniv.nl,y.zhu.12@\numail.leidenuniv.nl, m.van.leeuwen@liacs.leidenuniv.nl.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\ncontact the owner/author(s).\n\u00a9 2023 Copyright held by the owner/author(s).\n1556-4681/2023/09-ART23 $15.00\nhttps://doi.org/10.1145/3609333\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "98c9108d-d266-4df3-bfe7-81e4dd83c837": "23:2 Z. Li et al.\net al. [111], model explainability represents one of the main issues concerning the adoption of\ndata-driven algorithms in industrial environments. More importantly, for applications in safety\ncritical domains, providing explanations to stakeholders of AI systems has become an ethical and\nregulatory requirement [50, 217]. However, after a thorough survey of academic publications on\nexplainable anomaly detection, we found that existing surveys are either outdated, have missed\nsomeimportantwork,ortheirproposedtaxonomiesarerelativelycoarseand,therefore,unableto\ncharacterizetheincreasinglyrichsetofexplainableanomalydetectiontechniquesavailableinthe\nliterature.\nToaddressthisgapintheliterature,weconductacomprehensiveandstructuredsurveyonstate-\nof-the-art explainable anomaly detection techniques and distil a refined taxonomy that caters to\nthe increasingly rich set of techniques. Overall, this survey intends to provide both practitioners\nand researchers with an extensive overview of the different types of methods that have been pro-\nposed,withtheirprosandcons,andtohelpthemfindtheexplainableanomalydetectiontechnique\nmost suited to their needs.\nNote that some researchers [27, 145, 197] distinguish between the terms \u2018interpretation\u2019 and\n\u2018explanation\u2019, the terms \u2018interpretable\u2019 and \u2018explainable\u2019, and the terms \u2018explainability\u2019 and \u2018inter-\npretability\u2019. Specifically, Broniatowski [27] defines explainability asa model\u2019s ability to provide a\ndescriptionofhowitsoutcomecametobe anddescribesinterpretabilityas ahuman\u2019sabilitytomake\nsensefromagivenstimulussothatthehumancanmakeadecision .Moreover,Sipple&Youssef[ 197]\nargue that explainability isthe algorithmic task of generating the explanationand interpretability\nisthecognitivetaskofmergingtheexpert\u2019sknowledgewiththeexplanationtoidentifyauniquediag-\nnostic condition and to choose the appropriate treatment.Considering that most researchers in data\nmining and machine learning treat explainability and interpretability equally, we use those terms\ninterchangeablythroughoutthisarticle.Thenextsectionwillclarifywhatwemeanexactlywhen\nwe say that a technique is explainable.\n1.1 Methodology\nThis survey aims to answer the following research questions and is structured accordingly:\nQ0 What is explainable anomaly detection and why should we care about it?\nQ1 What are the most important aspects that characterise each explainable anomaly detection\ntechnique? On this basis, how to classify existing techniques?\nQ2 Howdoexistingtechniquesinterpretanomaliesandwhatarethemaindifferencesbetween\nthem?\nQ3 What are the challenges and associated opportunities in explainable anomaly detection?\nIn order to answer these research questions, we employ a comparative and iterative surveying\nprocedure that consists of three cycles. In the first cycle, we employ a methodology consisting of\ntwo main phases:\n\u2014Database Selection: we select well-known scientific databases for literature collection, i.e.,\nGoogle Scholar, IEEE Xplore, ACM Digital Library, DBLP, and Web of Science.\n\u2014Literature Selection: we select related research publications that were published between\nJanuary 1998 to February 2022 using the following keywords: Interpretable/Interpret/\nInterpretingAnomalyDetection,Explainable/Explain/ExplainingAnomalyDetection,Inter-\npretable/Interpret/Interpreting Outlier Detection, Explainable/Explain/Explaining Outlier\nDetection,AnomalyInterpretation,AnomalyExplanation,OutlierInterpretation,OutlierEx-\nplanation.Otherusefulkeywordsare:Anomaly/OutlierDescription,Anomaly/OutlierChar-\nacterization, Outlying Property Detection, Outlying Aspects Mining, Outlying Subspaces\nDetection.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "101a0975-b6e0-44e3-ad83-568263fa4a2a": "A Survey on Explainable Anomaly Detection 23:3\nFig. 1. The procedure of anomaly analysis and the different roles involved in this procedure.\nIn the second cycle, we inspect research publications that have been referenced by articles col-\nlectedinthefirstcycle.Inthethirdcycle,weexcluderesearchpublicationsthatareirrelevant,not\npublishedinwhatweconsiderhigh-qualityvenues,orapplicationsofexistingmethodstocertain\nuse cases.\nThis survey is organised as follows: To answer Q0, Section2 states the motivations for this\nwork and the terminology used. Section3 describes the proposed taxonomy for answering Q1.\nSections4\u20137surveyexistingtechniquesforexplainableanomalydetectioninaprincipledmanner\nbased on the proposed taxonomy, aiming to answer Q2. Section8 discusses the open challenges\nand related opportunities of existing work, and then concludes this survey, answering Q3.\n2 THE NEED FOR EXPLAINABLE ANOMALY DETECTION\nThis section introduces important terminology and concepts, such as anomalies and explainable\nanomaly detection, and explains why this is an important field of study.\n2.1 What is an Anomaly?\nFirst of all, we need to define what an anomaly is. Inspired by Sejr & Schneider-Kamp [189],\nwe assume that there are three roles involved in an anomaly analysis task: (1) a/anStakeholder/\nEnd-user/Data Scientist/Expertthat uses the anomaly detection system; (2) anAlgorithm Designer/\nAnomalyDetectionMethod thatdoestheactualanomalydetection;and(3)an AlgorithmExplainer/\nAnomaly Explanation Methodthat explains identified anomalies. These three roles are illustrated\nin Figure1. The different roles may have different definitions of what an anomaly is, and we dis-\ntinguish those definitions as follows:\n\u2014Oracle-Definition: the \u2018ideal\u2019 definition that defines the anomalies that the end-users of the\nanomalydetectionsystemaimtodetect.Inotherwords,thisdefinitiondefinesthe trueanom-\naliesinthereal-worldapplicationandthusstronglydependsonthecontextandisoftenhard\nto formally/precisely formulate.\n\u2014Detection-Definition: the anomalies that an anomaly detection model can actually capture.\nThisdefinitionisgivenexplicitlyorimplicitlybytheanomalydetectionmodelortechnique.\n\u2014Explanation-Definition: describes why (and when) the anomaly explanation method consid-\ners an anomaly as anomalous.\nFor example, for a credit card fraud detection system, the end-users aim to detect fraudulent\nbehaviour, which is defined as \u201cobtaining services/goods and/or money by unethical means\u201d, in-\ncluding bankruptcy fraud, theft fraud, application fraud and behavioral fraud [58]. Therefore, the\nOracle-Definition is \u201cbehaviour that aims to obtain services/goods and/or money by unethical\nmeans\u201d. However, a given credit card fraud system might only detect anomalous behaviours such\nas unprecedented high payments and/or payments at a never-before-seen location. Hence, the\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "acc7a58c-7717-48ed-9a6e-7a821b85125d": "23:4 Z. Li et al.\nDetection-Definition is \u201cunprecedented high payments and/or payments at a never-before-seen\nlocation\u201d and this is actually a theft fraud. Moreover, for an identified anomalous payment, the\nanomaly explanation method could generate the explanation \u201cthe payment is flagged as anoma-\nlousbecauseithappenedatmidnight\u201d,whichfollowsfromthe Explanation-Definition.Clearlythe\nOracle-Definition,t h eDetection-Definition, and theExplanation-Definition can be different from\neach other.\nIn general, theOracle-Definition is given based on domain knowledge, which is application-\nspecific. From this point of view, there is no universal definition of an anomaly. A commonly\naccepted definition by Hawkins [81] is that \u201can outlier is an observation that deviates so much\nfromotherobservationsastoarousesuspicionthatitwasgeneratedbyadifferentmechanism\u201d.As\nthisisinformal,eachspecificanomalydetectionmodelhasitsowndefinitionofananomaly,either\nexplicitly or implicitly. For example, KNN [172] defines objects with \u2018far\u2019k-nearest neighbours as\nanomalies,LOF[ 26]treatsobjectswithalowlocaldensityasanomalies,andIsolationForest[ 122]\nconsiders \u2018easily isolated\u2019 objects as anomalies. Importantly, thisDetection-Definition definition\ncanbedifferentfromthe Oracle-Definition,whichmayleadtoproblems.Forexample,ananomaly\ndetector may miss relevant anomalies while detecting \u2018anomalies\u2019 that are uninteresting to end-\nusers.Moreover,dependingonthetechniqueusedtoexplainananomaly,the Detection-Definition\nand Explanation-Definition can also be different, especially when the explanation approach does\nnot reflect the decision-making process behind the anomaly detection model.\n2.2 What is Explainable Anomaly Detection?\nAccording to Doshi-Velez & Kim [61], interpretability or explainability is defined as the ability\nto explain or provide meaning to humans in understandable terms. Moreover, Arrieta et al.\n[13]d e fi n eExplainable Artificial Intelligence(XAI) as \u201cGiven an audience, an explainable\nArtificial Intelligence is one that produces details or reasons to make its functioning clear or\neasy to understand.\u201d Furthermore, Murdoch et al. [150] define interpretable or eXplainable\nMachine Learning(XML) as \u201cthe extraction of relevant knowledge from a machine learning\nmodel concerning relationships either contained in data or learned by the model\u201d, where the\nknowledge is considered relevant if it provides insight into the problem faced by the target\naudience. Accordingly, we defineeXplainable Anomaly Detection(XAD)a sthe extraction of\nrelevant knowledge from an anomaly detection model concerning relationships either contained in\ndata or learned by the model, where the knowledge is considered relevant if it can provide insight\ninto the anomaly detection problem investigated by the end-user. Hereinafter, we utilize XAI and\nXML interchangeably as they practically mean the same within the scope of this manuscript.\nMiller [142] defined XAI as a human-agent interaction problem at the intersection of Artificial\nIntelligence, Human-Computer Interaction(HCI), and the Social Sciences (including Philoso-\nphy,CognitiveScience,andSocialPsychology).BeingasubfieldofXAI,XADcanalsobesituated\nat the intersection of those three domains. Therefore, in addition to considering different XAD\ntasks and problems together with their algorithmic and computational challenges, it would also\nbeofinteresttoconsiderquestionssuchas howdohumansunderstandanexplanation ,whatkindof\nexplanations are human-understandable,a n dhow do humans interact with machines to understand\nexplanations? Thoroughly addressing these questions, however, would require substantial addi-\ntional coverage and analysis of the literature; to maintain a clear scope and prevent the survey\nfrom becoming even longer, we will not address these questions. Instead, we refer to recent arti-\ncles for perspectives from HCI [202] and social science [142, 143], and leave a broader discussion\nof these aspects to a future article.\nThe anomaly analysis process consists of two equally important tasks, namely,anomaly detec-\ntion and anomaly explanation. Anomaly explanation refers to the process of finding out why an\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "b2f6cb1f-ca2d-42ba-8377-6d3abb260c84": "A Survey on Explainable Anomaly Detection 23:5\nanomaly is considered anomalous. Because the termsanomaly and outlier are used interchange-\nably, anomaly explanation is also known as outlier explanation, outlier interpretation, outlier de-\nscription, outlier characterization, outlying property detection, outlying aspects mining, outlying\nsubspaces detection, object explanation, and promotion analysis.\nAnanomalycanbeidentifiedbyananomalydetectionalgorithmor,otherwise,becomeknown\n(e.g., from an expert).\n\u2014Case1(Model). Ifananomalyisidentifiedbyananomalydetectionalgorithm, XADaimsat\nexplainingtheanomalybymakingtheanomalydetectionmethodinterpretable.Specifically,\nthere exist many approaches to make an anomaly detector interpretable. If the anomaly de-\ntector is intrinsically interpretable (e.g., logistic regression, shallow decision trees, and rule-\nbased models), it is relatively easy to deduce why the anomaly is flagged as anomalous. In\ncontrast,iftheanomalydetectorisnotintrinsicallyinterpretable(e.g.,IsolationForest[ 122],\nRNN[183],andCNN[ 74]),post-hocXAItechniquessuchasSHAP[ 128],LIME[ 174],andAn-\nchors[175]canbeusedtointerprettheanomalydetector,namely,todescribewhyitmakes\ncertain decisions. In this case, we aim at making theDetection-Definition and Explanation-\nDefinition consistent.\n\u2014Case2(Data). Ifananomalyisidentifiedbyanexpert,ananomalyexplanationmethodcan\nonly aim at explaining why the given data instance is anomalous, extracting no knowledge\nfrom any anomaly detection models. In this case, we attempt to make theOracle-Definition\n(ifany)and Explanation-Definitionconsistent.However,itisalsopossiblethattheexpertob-\ntainstheanomalybyrunninganexistinganomalydetectionalgorithm,butthedesignofthe\nalgorithm is unavailable to the expert for some reasons (such as confidentiality). Hence, the\nExplanation-Definition may be different from theDetection-Definition (which is not known).\nIn short, the biggest difference between these two cases is about what to explain: the model\n(and possibly the data) or just the data.Case 1is centred around anomaly detection models. If\nwe can understand how the anomaly detection model makes decisions, as a by-product, we can\neasily explain why an anomaly is flagged as anomalous by the model. In contrast,Case 2focuses\non anomalies and aims at explaining why they are anomalous where the detection model is not\navailable. The anomaly explanation methods corresponding to this case can be considered as sur-\nrogatemethodsfortheunavailableanomalydetectionmodels.Forcompleteness,wewillconsider\nboth cases in this survey.\n2.3 Why Should We Care About XAD?\nDue to the widespread application of anomaly detection in many domains, the interpretability\nof corresponding methods has become increasingly important [162]. For example, anomaly de-\ntection algorithms are being used to diagnose diseases in healthcare [212]. In financial services,\nmany banks use anomaly detection methods to detect abnormal behaviour in credit card trans-\nactions [6]. In addition, the self-driving car manufacturing industry applies anomaly detection\nalgorithmsoncameradatatodetectcornercases[ 23].Inothersafety-criticalareas\u2014suchasspace-\ncraft design\u2014anomaly detection algorithms are used to detect sensor faults [70]. As we can see,\nanomaly detection systems for high-stakes decisions are deeply impacting our daily lives and so-\nciety.Onenaturalquestionis, howcanwetrustthesesystemswithoutunderstandingandvalidating\nthe underlying rationale of the involved anomaly detection components?For this reason, XAD aims\nto not only provide accurate anomaly detection results but also to provide tangible explanations\nof why a specific object is detected as an anomaly [155].\nProviding anomaly detection results with corresponding explanations can help gain the trust\nof end-users in anomaly detection systems. Moreover, the explanations can also assist end-users\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "2d3e88a2-afb3-4ebc-8c48-1cc425215b85": "23:6 Z. Li et al.\nto validate the anomaly detection results in unsupervised settings. Even more, explanations can\npotentially enable end-users to find the root causes of anomalies and thereby take remedial or\npreventive actions.\nFor a long time, however, the anomaly detection community has mainly focused on detection\naccuracy,largelyignoring theinterpretationof correspondingdecisions.For instance,Micenkov\u00e1\net al. [141] criticise that \u201calmost all existing algorithms stop at the point of providing anomaly\nranking and leave the user without any explanation of why some data points deviate and how.\u201d\nAdditionally, Dang et al. [53] indicate that \u201calthough there is a large number of techniques for\ndiscovering global and local anomalous patterns, most attempts focus solely on the aspect of out-\nlieridentification,ignoringtheequallyimportantproblemofoutlierinterpretation.\u201dAggarwal[ 1]\nalso points out that \u201conly few outlier detection studies considered providing some qualitative in-\nformationtoexplaintheformofoutlierness.\u201dSimilarly,Vinhetal.[ 216]arguethat\u201ccurrentoutlier\ndetection techniques do not usually offer an explanation as to why the outliers are considered as\nsuch, or in other words, pointing out their outlying aspects.\u201d\nIn summary, the anomaly detection community has long been paying more attention to\ngiving correct answers rather than providing explanations or\u2014even better\u2014providing correct\nexplanations. With more and more applications or potential applications of anomaly detec-\ntion in high-risk decision-making systems, it has become crucial to gain or increase humans\u2019\ntrust in and acceptance of anomaly detection techniques. For this it is important to provide\ncorrect answers with correct explanations, i.e., to avoid the Clever Hans Phenomenon [112]\nthat\u2014in this context\u2014refers to anomaly detection models utilising spurious correlations and\npatterns in the data to identify anomalies. Although the identified anomalies are true, these\ncorrelations or patterns may be incorrect or undesirable (e.g., violating the laws of physics).\nSuch provably incorrect explanations are unacceptable to end-users and would only harm\ntrust.\n2.4 What is a Good XAD Method?\nOnceexplanationsaregeneratedbyanXADmethod,howcanonetrustthem?Anaturalfirststep\nistoevaluatethequalityofgeneratedexplanations.Studiesrelevanttothishavebeenconductedin\ntherealmofXAI.Forinstance,references[ 20,76]analyzetheXAIliteratureandproposeimportant\nproperties that should be considered when designing an XAI technique. Next, Barbado et al. [18]\ndefines some criteria to evaluate rule-extraction-based explanation techniques. Moreover, Zhou\netal.[ 229]performasurveyonthequalityevaluationofmachinelearningexplanations.Recently,\nSipple & Youssef [197] proposes four desiderata for anomaly explanation methods as well as a\nmethod for comparing different explanations. However, there is no consensus on what a good\nXAD technique should be. Based on related work on XAI, we find the following properties to be\nespecially relevant when designing or choosing an XAD technique:\n\u2014Accuracy: how accurate is the prediction of unseen anomalous instances as anomalies;\n\u2014Fidelity: consistency ofOracle-Definition,Detection-Definition,a n dExplanation-Definition;\n\u2014Comprehensibility: to what extent are the explanations understandable to the end-users;\n\u2014Generality: does the technique have special requirements for data type, data size, anom-\naly detection model type, anomaly detection model size, training regimes, or training\nrestrictions;\n\u2014Scalability: does it scale to large input data size and/or a large model;\n\u2014Complexity: how many hyper-parameters need to be set by end-users.\nThe practical implementation and evaluation of XAD techniques are largely dependent on the\napplication domain and end-users and are, therefore, out of the scope of this survey.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "ef9fc041-ed1e-4604-ab1a-4fe6dcfa2aac": "A Survey on Explainable Anomaly Detection 23:7\n3 A TAXONOMY OF EXPLAINABLE ANOMALY DETECTION METHODS\nBefore we introduce the taxonomy that we propose for the field of XAD, we first briefly review\nexisting surveys and taxonomies.\n3.1 Related Work\nCompared to the abundance of taxonomies of anomaly detection methods, including but not lim-\nited to these surveys throughout the years [3, 36, 37, 78, 86, 161], the categorization of anom-\naly explanation methods involving XAD techniques has received relatively little attention so far\n[162,185,189,216,225].\nWe discuss the four most notable existing categorizations. Vinh et al. [216] for the first\ntime subdivided anomaly explanation approaches into two categories:Feature selection-based\napproaches that transform the anomaly explanation task into the classical problem of feature\nselectionforclassification,and Score-and-searchapproaches thatcomparetheoutlyingnessdegree\nofananomalyacrossallsubspacesfollowedbyinspectingthesubspacewiththehighestanomaly\nscore. To the best of our knowledge, Samariya et al. [185] was the first work dedicated to the\nsurvey of anomaly explanation methods. They also subdivided related techniques into three\ncategories: Score-and-Search-based approaches, Feature selection-based approaches,a n dHybrid\napproaches. More recently, Panjei et al. [162] introduced a survey on anomaly explanation,\nwherein they divided relevant techniques into three categories:Importance Levels of Outliers,\nCausal Interactions Among Outliers,a n dOutlying Attributes. Meanwhile, Yepmo et al. [225]a l s o\npresented a review of anomaly explanation methods, categorizing existing techniques into four\ngroups, namely,Explanations by Feature Importance, Explanations by Feature Values, Explanations\nby Data Points Comparison,and Explanations by StructureAnalysis. Finally,Reference[189]i sa l so\nclosely related, wherein they have discussed what anomaly explanations are, who needs those\nexplanations, and why there are different types of anomaly explanations.\nAfterathoroughsurveyofthescientificliteratureonXADtechniques,wefindthatexistingsur-\nveys are less comprehensive than we aim to be in this manuscript. Specifically, each of the above\nsurveys contains no more than 40 relevant works in the field. In contrast, our survey has investi-\ngatedmorethan150relevantarticles.Inaddition,wefindtheexistingtaxonomiestoberelatively\ncoarse and sometimes not intuitive. For example, although anomaly score is a very natural rank-\ning of outlying degree, Panjei et al. [162] particularly treat anomaly ranking as a subcategory of\nanomalyexplanationmethods.Furthermore,although ExplanationsbyFeatureImportance andEx-\nplanationsbyFeatureValues mainlydifferinthegranularityofprovidedexplanations,Yepmoetal.\n[225]regardthemastwodistinctcategories.Inbrief,existingsurveysonlypartiallycoverexisting\nresearch, and the proposed taxonomies are insufficient to characterize the increasingly rich field\nof XAD. For this reason, we perform a comprehensive and structured survey on state-of-the-art\nXAD techniques. As new articles are published at a rapid pace, we do not claim to have covered\nall relevant research publications. Furthermore, as we intend to include a wide spectrum of XAD\nmethods,wecannotdescribeeachmethodindetail.Meanwhile,arefinedtaxonomy,distilledfrom\nexisting surveys on XAI techniques, is presented below and used to categorize XAD methods.\n3.2 Proposed Taxonomy\nSimilar to how anomaly detection is an important part of machine learning and data mining, we\narguethatXADisalsoanimportantconstituentofwhatisnowadayscalledXAI.XAIhasreceived\nextensiveattentioninthepastfewyearsduetotheemergenceandprevalenceofblack-boxmodels\nsuch as deep neural networks. After carefully scrutinizing existing surveys on XAI [13, 20, 30, 34,\n62,71,120],wefoundthatsomecriteriaareoftenusedtocategorizeexistingXAItechniques.Cap-\nitalizing on these findings, we propose six main criteria to taxonomize existing XAD techniques.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "914b7e73-72e5-4267-aac5-7f58b4d26462": "23:8 Z. Li et al.\nFirst of all, according to the anomaly detection pipeline as shown in Figure1,w ec a ns u b d i -\nvideXADtechniquesintothreecategories,namely, Pre-modeltechniques,In-modeltechniques and\nPost-model techniques. Specifically,pre-model techniques, also known asante-hoc techniques,a r e\nconstructedandimplementedbeforetheanomalydetectionprocess.Techniquessuchasfilterfea-\nture selection methods belong to this category.In-model techniquesuse inherently interpretable\nmodels and can therefore provide explanations without additional or with little effort when per-\nforming anomaly detection. For example, anomaly detection methods based on linear regression,\nwhich can simultaneously report the coefficients of the corresponding features, fall into this cat-\negory. In contrast,post-model techniques, also known aspost-hoc techniques, attempt to explain\nthe decisions made by an anomaly detection model after the construction and implementation of\nthe detection model or when anomalies are obtained from an oracle. For instance, SHAP-based\ninterpretation methods [128] are part of this category.\nSecond, we distinguish XAD techniques based on whether they provide aglobal explanation\nor local explanation. Specifically, aglobal explanationis based on the understanding of the com-\nplete \u2018model logic\u2019 or some important properties of the anomaly detection model, being able to\nexplain how all decisions are made. In contrast, alocal explanationexplains why a specific object\nis anomalous or how a specific decision is made.\nThird, XAI techniques can be further subdivided intomodel-agnostic approaches that can be\nappliedtoanyanomalydetectionmodel,and model-specific approachesthatareonlyapplicableto\nspecific anomaly detection models.\nFourth, two aspects of a tabular dataset can be used to generate explanations, i.e., a tabular\ndatasethasfeaturesandsamples.Therefore,wecansubdividetechniquesintothreesubcategories:\n\u2014Feature-basedmethods provideexplanationsbasedonfeatures.Thisgroupofmethodsgener-\nallyindicates whichfeaturesare importantand/orthe correspondingvalues ofinvestigated\nanomalies.Specifically, subspace(e.g.,asubsetorunorderedfeatures), asetofsubspaces (e.g.,\na set of feature pairs),feature importance(e.g., assigning a score or an order to each fea-\nture), andfeature values(e.g., rare combination of feature values) fall into this subcategory.\nParticularly, some studies attempt to define a set of rules based on a subset of features and\ntheir corresponding values, resulting in so-called patterns. Meanwhile, for sequential data\nsuchastimeseries,apatternconsistingofacollectionofconsecutiveobservationsisusually\nleveragedtodetectandexplainanomalies.Eachobservationcanberegardedasafeatureor\na sample depending on the context. For simplicity, we call thempattern-based methods, but\nthey are still essentiallyfeature-based methods.\n\u2014Sample-based methodsgenerate explanations based on samples. This type of method typi-\ncallycomparestheabnormalobjectdirectlytonormalobjectstodemonstratedifferences.For\ninstance, local neighbourhood(e.g., the nearest objects, which may be normal or abnormal,\nto an anomaly),counterexample (e.g., the nearest normal object to an anomaly), andcontex-\ntualanomalies (e.g.,thenearestclustertoananomaly)belongtothissubcategory.Moreover,\nexception analysisin Reference [76]a n drepresentative examplesin References [20,211]a l s o\nfall under this category.\n\u2014Feature and Sample-based methodsleverage both aspects.\nFifth,basedonthespecifictechniquesusedtogenerateexplanations,wecancategorizemodels\ninto the following subcategories, which are not mutually exclusive:\n\u2014Approximation-based methods, which approximate or mimic complex models with simpler\nones that are much easier to interpret. They are also called surrogate models. Examples\ninclude LIME [174]a n dA n c h o r s[175].\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "af07a6df-4a49-4112-a27c-ae6d4e6a3cf4": "A Survey on Explainable Anomaly Detection 23:9\nFig. 2. XAD taxonomy based on six criteria (coloured text boxes). Most existing XAD methods fall into one\ncategory for each of the six criteria.\n\u2014Perturbation-based methods, which examine the influence of output via input changes to\ngenerate explanations. Examples include Anchors [175].\n\u2014Reconstruction Error-based methods, which use reconstruction errors to explain anomalies.\nExamples include SHAP-based methods [12].\n\u2014AttentionLearning-basedmethods ,whichuseattentionlearningtolocaliseanomalies.Exam-\nples include Reference [28] and Reference [215].\n\u2014Gradient-based methods, which measure feature contribution on midput (intermediate out-\nputs) or outputs through back-propagation. Examples include Layer-wise Relevance Prop-\nagation [96, 160, 196]. Note that some of these methods may also be Reconstruction Error\nbased.\n\u2014Causal Inference-based methods, which analyze the causal relations between objects and/or\nfeatures to explain anomalies. Examples include Reference [124].\n\u2014Visualization-based methods, which use plots to explain anomalies. Examples include Ref-\nerence [126], which uses heatmaps that is a kind of saliency masks. Note that many other\ntechniques also leverage visualization to explain anomalies.\n\u2014Intrinsically Explainable methods. The above-mentioned subcategories are mainly post-\nmodel techniques that are used to explain deep learning-based anomaly detection models.\nMeanwhile, there are in-model techniques that make the anomaly detection model intrinsi-\ncally explainable. Examples include Rule-based models [83].\n\u2014Miscellaneous other methods. We assign other techniques to this subcategory by indicating\ntheirspecifictechnique.ExamplesincludePatternCompression[ 200],andSubspaceAnom-\naly Detection [186].\nSixth and last, we also indicate the types of data to which each XAD technique can be applied.\nSpecifically, the data type can be static or streaming. Furthermore, it can be tabular (numeric,\ncategorical, or mixed), sequential (time series, other sequential), image, text, video, or graph\ndata.\nOur overall proposed taxonomy is presented in Figure2: each of the six criteria can be used\nto categorize an XAD method. Together these six \u2018dimensions\u2019 can be used to provide a detailed\ncharacterization of an existing XAD method, or\u2014the other way around\u2014to find XAD methods\nsatisfying certain requirements.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "e202dd06-ea79-4aac-9b10-ea8696b82ed0": "23:10 Z. Li et al.\nFig. 3. Structure of the core of this survey, i.e., Sections4\u20137 (indicated by S4\u20137).\n3.3 Organisation of the Literature Review\nAsdescribedintheprevioussubsectionandshowninFigure 2,ourtaxonomyemployssixcriteria.\nTo organize our survey by these six criteria; however, we would have to introduce many section\nlevels and some subsections would be much longer than others. We will therefore use another\nstructure (as shown in Figure3) for the literature review in the following sections, which we will\nexplain next. We will still make ample use of our proposed taxonomy: to partially structure the\nindividual sections, to characterize the methods that we describe, and to provide a full characteri-\nzation of all methods in a large overview table at the end of each section.\nWeusethefirstmaincriteriontoclassify pre-modeltechniques (S4),in-modeltechniques (S5),and\npost-model techniques(S6-7) into different sections. As there are so many post-model techniques,\nwe split those into deep learning-based methods (S7) and other, \u2018shallow\u2019 methods (S6). Next, we\nusethecharacteristicsofeachofthesecategoriestodefinesubsections.Thatis,the pre-modeltech-\nniques section consists of subsections forfeature selectionand feature representation. Meanwhile,\nthein-model techniquessection includes subsections fortransparent models in supervised learning,\nfeaturesubset-basedmodels ,and othermodelsinunsupervisedlearning .The shallowpost-modeltech-\nniques section has subsections forsubspace-based methods, surrogate methods,a n dmiscellaneous\nmethods. Finally, thedeep post-model techniquessection contains subsectionsexplaining AutoEn-\ncoders,explaining RNNs,explaining CNNs,a n dexplaining other DNNs.\n4 LITERATURE REVIEW ON PRE-MODEL TECHNIQUES\nOpaque models are often criticized for their inexplicability. However, the features used as inputs\ntomodelsareascriticalas,ifnotmorethan,thetypeofmodelsinproducingexplainableresults.In\notherwords,byhavingmoremeaningfulandinformativefeatureswhilstretainingfewerirrelevant\nfeatures,wecanbuildsimplermodelstolearntherelationshipsexhibitedinthedatawhileensuring\ncomparable anomaly detection accuracy, see Figure3.\nTherefore,thissectionreviewsarticlesthatleverageXADtechniques before theanomalydetec-\ntion process. Specifically, the following pre-model techniques are investigated:\n\u2014Feature selection methods that select a subset of original features for anomaly detection;\n\u2014Feature representation methods that learn a set of high-level and human-understandable\nfeature representations for anomaly detection.\n4.1 Feature Selection for Anomaly Detection\nSiddiqui et al. [193] point out that the effort required to investigate an anomaly is usually\nproportional to the number of features that describe it. Therefore, dimensionality reduction\ntechniques\u2014includingfeature projectionand feature selection methods\u2014can be appliedto reduce\nthenumberoffeaturesthatdescribeanobject,thereby,facilitatinganomalyexplanation.However,\nfeature projection methods such as Principal Component Analysis convert the original features\nintoanewsetoffeatures,sacrificinginterpretability.Incontrast,featureselectionmethodsretain\na subset of original features that contain the most important information, greatly improving the\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "b1d91889-c235-4928-aedb-72607da5c600": "A Survey on Explainable Anomaly Detection 23:11\ninterpretabilityandeffectivelyalleviatingthe curseofdimensionality probleminhigh-dimensional\ndata.\nThereexistverylimitedunsupervisedfeatureselectionmethodsforanomalydetection.Specifi-\ncally,Pangetal.[ 156]andPangetal.[ 158]proposetwofilter-basedunsupervisedselectionmeth-\nods, namely, CBRW_FS and CBRW, which select a subset of features independently from subse-\nquent anomaly detection methods. These two methods work only on categorical data through\nmodelling the feature-value couplings. By assuming strong similarities between rare instances,\nHe & Carbonell [82] design an optimization framework to jointly select features and instances\nfor anomaly detection on categorical data. However, this assumption is usually not satisfied since\nanomalies are often isolated and thus distinct from each other.\nMeanwhile,Notoetal.[ 154]andPaulheim&Meusel[ 167]trytofindarelevantfeaturesubsetfor\nanomalydetectionbyexploringthecorrelationsbetweenfeatures.Theyassumethatanomaliesare\nthose instances that violate the normal dependencies between features. Therefore, only features\nthatarerelatedtootherfeaturesareconsideredrelevantforanomalydetection.Unfortunately,this\nanomaly definition is not applicable to many benchmark anomaly detectors. Moreover, Isolation\nForest [122] can also be used to select a subset of features for anomaly detection. The isolation\nforest-basedfeatureselectionmethod,IBFS[ 223],simplyselectsfeaturesthatcontributethemost\ntotheoutlyingnessofanomaliesreportedbytheIsolationForestmethod.Toourknowledge,thisis\nthefirstunsupervisedfeatureselectionmethodspecificallydesignedfor genericanomalydetection\nin numeric data. The above three methods are all filter-based, which independently select subsets\nof features regardless of subsequent anomaly detection methods. Consequently, suboptimal or\ncompletely irrelevant features may be selected for anomaly detectors.\nAplatforminformationtechnology (PIT)systemisasystemcapableofconnectingandcom-\nmunicating with other systems, subsystems and devices. To detect attacks in PIT systems, Morris\n[146] proposes to usePrincipal Component Analysis(PCA)o rIndependent Component\nAnalysis(ICA)toreducethenumberoffeaturesconsidered,therebypromotinginterpretabilityin\nthe subsequent anomaly detection process. Moreover, he suggests using ensemble learning-based\nmethods such as Random Forests to detection anomalies after the dimensionality reduction pro-\ncess. However, every feature obtained using PCA or ICA is a combination of the original features\nand is, therefore, no longer interpretable.\nSomefeatureselectionmethodsareinterleavedwiththeanomalydetectionprocess,ratherthan\nbeing applied before the anomaly detection process. We call such methods wrapper or embedded\nfeatureselectionmethodsdependingontheirimplementationsandwillintroducetheminthenext\nsection.\n4.2 Feature Representation for Anomaly Detection\nDue to the complexity entailed in data such as time series, image, video, and so on,deep neu-\nral network(DNN)-based methods have shown superiority in detecting anomalies in these data.\nHowever, DNN-based models are notoriously known for their complexity, which implies unin-\nterpretability. To alleviate this problem, Chen et al. [43]a n dW ue ta l .[219] indicate that using\nhigh-level and human-understandable feature representations for anomaly detection can reduce\nthe complexity of subsequent anomaly detection models, thereby improving their interpretability.\nExamplescanbeobservedinthedomainoftimeseriesanomalyanalysis.Forinstance,Ramirez\net al. [173] introduce an interpretable anomaly detection and classification framework to analyze\nhuman gait. Specifically, they first harness symbolic representations such as Piecewise Aggregate\nApproximationtorepresentthecollectedmultivariatetimeseriesdata.Particularly,theyconsider\nthesymbolicabstractionofthedataasthecoreoftheirXADframework,enhancinginterpretability\nof the results via feature reduction. Second, they apply two discords-based anomaly detection\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "ac2bccbf-d5b0-4c1e-8662-8180632ce95d": "23:12 Z. Li et al.\nmethods, viz. HOT-SAX [101] and RRA [191], to discovery anomalies, respectively. Third, they\ndetermine the final anomalies based on the consensus of these two detection algorithms.\nInstead of using symbolic representations, Dissanayake et al. [60] investigate the importance\nof heart sound segmentation and feature extraction for detecting abnormal heart sound. They\nsuggest that an automated detection method usually consists of three steps: Segmentation,\nFeature Extraction, and Classification. First, they apply the model proposed by Fernando et al.\n[69] to perform segmentation. Particularly, the segmentation is based on a feature representation\ncalled Mel-Frequency Cepstral Coefficients(MFCCs). They argue that pre-extracted feature\nrepresentationssuchasMFCCsorspectrogramarecommonlyusedinmedicaldomainastheyare\nclosely related to the original signal. One can gain important insights into the model prediction\nresults if explaining the feature representations in conjunction with the signal. Second, they\nutilisea ConvolutionNeuralNetwork (CNN)encodertoextractfeatures.Third,theyconstruct\na Multilayer Perceptron(MLP) Network model to perform anomaly detection. Moreover, to\ninterpret an anomaly, they combine Shapley values and Occlusion maps [227] to investigate how\ninput features impact the prediction.\nSchlegl et al. [187] construct a DNN-based model that can learn interpretable feature repre-\nsentations from unlabeled time series, facilitating the evaluation and deployment of subsequent\nanomaly detection algorithms. First, they set up a so-calleddeviation convolutionbased model to\nlearn characteristic shapes of normal time series, wherein they impose a separating constraint on\nthe neural network to make it interpretable. Second, they feed these human-interpretable shapes\nto a convolutional-RNN AutoEncoder, which attempts to reconstruct the input shapes while min-\nimising the reconstruction errors. Therefore, a test instance with a large reconstruction error is\nconsidered anomalous.\nIn the field of video anomaly analysis, Wu et al. [219]p r o p o s eaDenoising AutoEncoder\n(DAE)-based model combined with SHAP to detect and explain anomalies in videos. Since unin-\nterpretablefeaturerepresentationshidethedecision-makingprocess,theyfirstleveragepretrained\nConvolutionalNeuralNetwork (CNN)modelstoextracthigh-levelconceptandcontextualfea-\ntures. Second, they train a DAE model based on these features to predict the video frame. On this\nbasis, a test instance is considered anomalous if its actual frame is significantly different from\nits predicted frame. Third, they apply kernel SHAP [128] to find input features which cause the\nanomaly.\n4.3 Summary\nAsshowninTable 1,allpre-modelXADtechniquesaremodel-agnosticexceptforReference[ 173].\nInotherwords,mostpre-modelXADtechniquescanbeappliedtoanysubsequentanomalydetec-\ntion methods. However, fully decoupling the feature selection or feature representation learning\nfrom the subsequent anomaly detection methods may lead to sub-optimal detection accuracy.\nFurthermore, most reviewed pre-model XAD techniques are feature-based with the excep-\ntion that He & Carbonell [82] also perform instance selection to improve interpretability.\nImportantly, all pre-model XAD techniques can provide global explanations in the sense that\nthey render the subsequent anomaly detection models more transparent and interpretable by\npreserving less irrelevant or redundant features, or providing human-understandable feature\nrepresentations.\nThe ultimate goal of using XAD techniques is to ensure that the entire pipeline of anomaly\ndetectionishuman-understandable.However,wenotethathigh-levelandhuman-understandable\nfeature representations are usually obtained by an opaque model, such as a pre-trained CNN\nmodel in Reference [60], which somewhat offsets the benefits of using interpretable feature\nrepresentations for anomaly detection.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "fd1e9c74-572d-4e55-b63a-0a27837561bc": "A Survey on Explainable Anomaly Detection 23:13\nTable 1. Summary of Pre-model XAD Techniques\nRef. Spec Pers Tech Data Loc Pros Cons\n[156] A F Feature selection Static TC G Handles noisy\nfeatures well\nOnly applicable to\ncategorical data\n[158] A F Feature selection Static TC G Linear time\ncomplexity to data\nsize\nOnly applicable to\ncategorical data\n[82] A F & S Feature selection +\nInstance selection\nStatic TC G Jointly selects\nfeatures and\ninstances for AD\nAssumes strong\nsimilarities\nbetween rare\ninstances\n[154] A F Feature selection Static TN G Robust to noisy\nand\nhigh-dimensional\ndata\nOnly explores\ncorrelations\nbetween features\n[167] A F Feature selection Static TN G Changes\nunsupervised AD\ninto supervised AD\nOnly explores\ncorrelations\nbetween features\n[223] A F Feature selection Static TN G Applicable to\ng e n e r i cA Df o r\nnumeric data\nSelects features\nwithout\nconsidering\nsubsequent AD\nmethods\n[146] A F Feature selection Static TN G Applicable to\ngeneric AD\nObtained features\nare not\ninterpretable\n[219] A F Pretrained CNN models\nto extract high-level\nconcept and contextual\nfeatures; VAE + SHAP\nStatic\nvideo\nL&\nG\nExtracted features\nare easy to\nunderstand\nWeak\ninterpretability due\nto the opacity of\nCNN\n[173] S P Symbolic representation\nusing PAA\nStatic\nMTS\nGE n a b l e s\nhuman-in-the-loop\nOnly applicable to\nsymbolic based AD\nsuch as HOT-SAX\nand RRA\n[60] A F Pre-extracted feature\nrepresentations\n(MFCCs/spectrogram);\nSHAP + Occlusion maps\nHeart\nsound\nsig-\nnals/UTS\nL&\nG\nSimple, stable and\nefficient\narchitecture\nOnly applicable to\nDNN\n[187] A F Explainable feature\nrepresentations\nStatic\nMTS\nG Easy to visualize Weak\ninterpretability due\nto the opacity of\nRNN-based AD;\nFails to learn less\nfrequent shapes\nSpec indicates whether a method is model-agnostic (A) or model-specific (S).Pers specifies whether a method is\nfeature-based (F), sample-based (S) or pattern-based (P).Techindicates the techniques used in each method.Data\nindicates the data type for which the method is applicable (TN: Tabular Numeric; TC: Tabular Categorical; TM: Tabular\nMixed; UTS: Univariate Time Series; MTS: Multivariate Time Series; ES: Event Sequence).Loc shows whether a method\nprovides a local explanation (L) or global explanation (G).Pros andCons describe advantages and disadvantages of each\nmethod, respectively.\nMoreover, it can be seen that the reviewed feature selection and feature representation\ntechniques are model-based feature engineering methods, which only leverage machine learning\ntechniques. However, one can employ domain-knowledge-based feature engineering methods\nto extract features. For instance, Murdoch et al. [150] point out that combining exploratory data\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "76eb3feb-d02d-4e0a-92b1-4ee08e4a8753": "23:14 Z. Li et al.\nanalysis tools with domain knowledge is helpful for extracting meaningful features, thereby\nimproving the interpretability of subsequent anomaly detection.\n5 LITERATURE REVIEW ON IN-MODEL TECHNIQUES\nThis section presents anomaly detection models that are considered to be inherently explainable.\nThese anomaly detection models can provide insights into the relationships they have learned\nfrom the data, enabling an end-user to understand the decisions they have made. In general, the\nfollowing methods are considered intrinsically explainable:\n\u2014Commonly seen transparent models in supervised learning, including Linear Models (Lin-\nearRegression,LogisticRegression),DecisionTrees,GaussianProcess,Rule-basedLearners,\nGenerative Additive Models, and Bayesian Models;\n\u2014Feature subset based methods, including subspace anomaly detection methods, wrapper or\nembedded feature selection methods for anomaly detection;\n\u2014Miscellaneous other methods (mostly in an unsupervised setting) that reveal the rationale\nfor how anomaly scores are calculated in a comprehensible way.\n5.1 Transparent Models in Supervised Learning\nAccording to Lipton [121], a model is transparent if its intrinsic structure satisfies at least one of\nthe following three requirements:\n\u2014Simulatability:if a model can be simulated by a human, and thus whether it is possible to\nreason about its entire decision-making process.\n\u2014Decomposability: if a model can be broken down into multiple parts, and these parts are\neasy to explain individually.\n\u2014Algorithmic Transparency:if a human can understand the process by which the model\ngenerates output from a given input.\nInasupervisedsetting,commonlyseentransparentmodelsincludeLinearModels(suchasLin-\nearRegressionandLogisticRegression),DecisionTrees,Rule-basedLearnersintheformof if-then\nrules,m-of-nrules,listofrules,fallingrulelists ordecisionsets,GaussianProcess, GenerativeAddi-\ntiveModels (GAMs),andBayesianModels.Althoughanomalydetectionisoftenanunsupervised\nproblem,itcanoftenleveragethesemethodsinsomeway.However,transparencyisnotsufficient\ntoguaranteeexplainability.Specifically,whenatransparentmodelbecomesexceedinglycomplex,\nitisnothuman-understandableanymore.Therefore,anomalydetectionmodelsthataredeveloped\nbased on these transparent models are considered to be explainable as long as they are not overly\ncomplex.\nFirst,rule-basedmodelsareoftenleveragedtolearnfrequentpatternsinthedata,enablinginter-\npretable anomaly detection. For instance, He et al. [83] apply frequent pattern mining to identify\nand explain anomalies in transaction data. Specifically, they leverage the Apriori algorithm [2]t o\nfind frequent patterns, and utilise the so-called top-k contradictory frequent patterns to explain\neach identified anomaly. Similarly, Zhu et al. [230] propose a model to capture frequent motion\nand background patterns of activities in video data, treating patterns that deviate from learned\nfrequent patterns as anomalies. Likewise, Vacul\u00edk & Popel\u00ednsk`y[ 213] put forward the DRGMiner\nmodel, which mines frequent patterns in dynamic graphs and considers graphs deviating from\nthese patterns as anomalous. Besides, Mauro et al. [138] propose HyVarRec to detect and explain\nanomalous traces for context-aware software product lines. Concretely, they apply Satisfiability\nModuloTheories[ 57]toconstructaconjunctionofconstraintsthatshouldbesatisfiedbynormal\ntraceswhenconsideringtheircontexts.Asaresult,atracethatviolatesthepredefinedconstraints\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "1309d7b4-330c-43a5-bf6b-71428fb74425": "A Survey on Explainable Anomaly Detection 23:15\nis considered anomalous. Moreover, B\u00f6hmer & Rinderle-Ma [24] develop the ADAR model to de-\ntectandexplainanomaliesinprocessruntimebehaviour.Specifically,ADARleveragesassociation\nrule mining to extract a set of ordered rules that normal traces should satisfy. Hence, a test trace\nwithasmallsupportisconsideredanomalous.Importantly,theyalsoproposeavisualizationtech-\nnique called A_Viz to show the rule violation.\nSecond, decision trees and their variants have also been proposed to be used for the detection\nof anomalies, resulting in intrinsically explainable detection results. For instance, Kraiem et al.\n[108] introduce theComposition-based Decision Tree(CDT) to detect and interpret anomalies\nin time series. Specifically, after preprocessing and labelling of given time series, a CDT is con-\nstructed as an extension of a decision tree on this labelled data, extracting rules for describing\nseen anomalies and detecting unseen anomalies. Also, the authors evaluate the explanation qual-\nity in terms of the number of used patterns and the length of rules. Furthermore, Cortes [51]\npresents an anomaly detection method that performs supervised decision tree splits on features,\nwherein the one-dimensional confidence intervals of each branch are built for the target feature.\nAs a result, explanations can be obtained from the branching conditions and the general distribu-\ntion statistics of non-anomalies that fall into the same branch. Besides, Aguilar et al. [4]p r o p o s e\nthe Decision Tree-based AutoEncoder(DTAE) model to detect anomalies. Specifically, they\nuse a decision tree to depict the encoding and decoding portions of AE, determining whether an\ninstance is anomalous by comparing the input with the output. The advantage of using decision\ntrees as encoders and decoders is that each tree contains the rules for categorizing tuples, offer-\ning interpretability. Meanwhile, Itani et al. [90] develop the so-calledone-class decision tree\n(OC-Tree) model, which employs Kernel Density Estimation to divide data subsets into intervals\nof interest and then encloses the data within hyperrectangles that can be explained by a set of\nrules. Additionally, Perez & Lavalle [168] devise the alleged User Model to detect potential fraud\nin bank transactions, where they fit manually selected features into a threshold-based rule model,\nclassifying the model outputs in the form of fraud probability into five categories.\nThird, another line of research utilises regression models to perform anomaly detection, pro-\nviding explanations for identified anomalies. For example, for each data instance, Chen et al. [41]\napplyLOESSregression[ 49]bytakingeachfeatureinturnasthetargetvariableandtheremaining\nfeaturesaspredictorsbasedonitsneighbours.Aninstanceisconsideredanomalousinacertainfea-\ntureifitsactualvaluedifferssignificantlyfromitspredictedvalue.Particularly,foreachidentified\nanomaly,theyprovideanaturallanguageexplanationconsistingofitsconsideredneighboursand\ntheassociatedfeaturedifferences.Besides,inBurakGunayetal.[ 29],theheatingandcoolingload\npatterns of buildings are studied using three inverse models, including a univariate change point\nmodel,aregressiontrees-basedmodel,andaDNN-basedmodel.Particularly,changepointmodels\nandregressiontreesareeasytointerpretandcangeneraterulesfromtheiroutput.Moreover,Lan-\ngone et al. [111] leverage regularized Logistic Regression to identify anomalies in time series. In\nbrief, they first utilise a bucket-based representation to represent the data, and then implement a\nrollingwindowproceduretoextractfeatures.Onthisbasis,theyemploytheKolmogorov-Smirnov\ndistance to select relevant features for anomaly detection, and the resulting features are fed to a\nLogistic Regression with Elastic Net regularization to detect anomalies.\nFourth,someresearchersutilizeintrinsicallyinterpretablemodelssuchas GaussianProcesses\n(GPs), Generalized Additive Models(GAMs), andDynamic Bayesian Networks(DBNs)t o\ndetect anomalies. For instance, Berns et al.",
        "8ea964a5-950c-4a5c-8439-9c4d65ac7e40": "[111] leverage regularized Logistic Regression to identify anomalies in time series. In\nbrief, they first utilise a bucket-based representation to represent the data, and then implement a\nrollingwindowproceduretoextractfeatures.Onthisbasis,theyemploytheKolmogorov-Smirnov\ndistance to select relevant features for anomaly detection, and the resulting features are fed to a\nLogistic Regression with Elastic Net regularization to detect anomalies.\nFourth,someresearchersutilizeintrinsicallyinterpretablemodelssuchas GaussianProcesses\n(GPs), Generalized Additive Models(GAMs), andDynamic Bayesian Networks(DBNs)t o\ndetect anomalies. For instance, Berns et al. [21] employ GPs to detect anomalies, where a GP is\na stochastic process defined over a set of random variables such that every finite subset of these\nrandomvariablesfollowsamultivariateGaussiandistribution.Iftheactualvalueofatestinstance\ndeviates significantly from its predicted value, the GP model treats it as an anomaly. Meanwhile,\nChangetal.[ 38]presentanXADmodelnamedDIADbasedonGAMs.Specifically,aGAMmodel\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "25c862eb-25d5-4893-b1d7-5532c2444447": "23:16 Z. Li et al.\nis a linear combination of smooth functions, where each function is defined on some variables.\nGiven an anomaly, one can easily infer which features contribute the most to its outlyingness.\nMoreover, Slavic et al. [199] develop a DBNs-based model to predict the state of a moving object\nin Autonomous Driving domain, attempting to identify abnormal motion behaviours based on\nits motion direction and orthogonal direction. A test instance is considered anomalous if its pre-\ndicted state deviates significantly from its actual state. Due to the good properties of DBN, they\ncan decompose the anomalous motion along its two directions and resort to the corresponding\nparameters to interpret the anomaly.\nFinally,animportantline ofresearchattemptstointroduceinterpretablecomponentsin acom-\nplex anomaly detection model, providing weak interpretability. For instance, Zancato et al. [226]\npropose the STRIC model to detect anomalies in multivariate time series data. Specifically, STRIC\nconsistsoffourlayers.Thefirstlayerattemptstomodelthetrendoftimeseriesbyusingacascade\noflinearfilters.Thesecondlayerimplementsalinearmoduletomodelandremovetheseasonality\natmultipletimescales.Next,thethirdlayercomprisesalinearstationaryfilterbankthatisableto\napproximate any trend or seasonality. Finally, the fourth non-linear layer consists of a randomly\ninitialized Temporal Convolution Network model. Therefore, these four layers constitute a model\ncapableofpredictingtimeseries.Onthisbasis,theyextendtheCUMSUMalgorithm[ 224]todetect\nanomaliesbyusingthelikelihoodratiobetweentwowindowsofpredictionresiduals.Particularly,\nthe linear components used in STRIC provide interpretability.\nDiscussion: To take advantage of these transparent models for anomaly detection, one usually\nneeds to turn the unsupervised anomaly detection problem into a supervised or semi-supervised\nsetting. For instance, References [24, 199, 213] attempt to learn normal behaviours or patterns\nby training the model on exclusively normal data, and then identify anomalies by comparing\na test instance with the expected normal behaviours. Meanwhile, References [41, 108] either\ndirectly leverage labelled data or decompose an unsupervised problem into many supervised\nproblems [167].\n5.2 Feature Subset-based Models\nThe methods in this subsection select one or more subsets of features to detect and explain anom-\nalies. Specifically, it contains subspace anomaly detection methods and feature selection meth-\nods for anomaly detection. Subspace anomaly detection methods find anomalies that are only\ndetectable in certain subspaces, providing intrinsic explanations based on subspaces. Moreover,\nwrapper or embedded feature selection methods select a subset of original features that are rele-\nvant for anomaly detection, thereby improving the interpretability of detection results. Note that\nwrapper or embedded feature selection methods select features during the process of performing\nanomaly detection, not before the anomaly detection process (see Section4.1). Furthermore, fea-\nture selection methods can be considered as a special case of subspace anomaly detection since\nthey actually select a subspace for anomaly detection.\nFirst,subspaceanomalydetectionusuallyconsistsoftwosteps:findingsubspacesandassigning\nanomaly scores. Subspace anomaly detection has received extensive attention, resulting in a\ncollection of strategies for finding subspaces and assigning anomaly scores. In general, finding\nsubspaces and assigning anomaly scores can be decoupled or intertwined. For instance, Muller\netal.[ 149]proposeOUTRES.Foreachinstance,theyfirstusetheKolmogorov\u2013Smirnovgoodness\nof fit test to exclude some subspaces from the powerset of features. Specifically, they exclude\nsubspaces in which the local densities of the given instance and its neighbourhood are uniformly\nrandom distributed. Then, for the residual subspaces, they define a dimensionality-unbiased\nanomaly scoring function to measure the local density deviation of the given instance. Next,\nthey aggregate the anomaly scores of each instance across its non-uniformly random distributed\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "b49939b4-6343-44b4-b3bd-5159108d3d92": "A Survey on Explainable Anomaly Detection 23:17\nsubspaces.Moreover,Kelleretal.[ 99]presentHICS.Specifically,theyfirstlookforsubspaceswith\nhigh contrast by measuring the correlation between features in a subspace using statistical tests,\nviz. the difference between marginal probability density and conditional probability density. Sec-\nond, they apply an off-the-shelf anomaly ranking method such as LOF [26] on selected subspaces\nandaggregatetheresults.Itcanbeseenthatbothmethods[ 99,149]decouplesubspacesearchand\nanomaly scoring. In contrast, Dang et al. [52] leverage spectral graph theory to achieve subspace\nanomalydetection.Specifically,theyfirstconstructanundirectedgraphthatcancapturethelocal\ngeometryofallinstances.Second,theyattempttolearnanoptimalsubspacethatcanseparatewell\nan instance from its neighbours, while preserving the intrinsic geometrical structure of data. Cor-\nrespondingly, a well separated instance is considered anomalous and the corresponding subspace\nacts as an explanation. The subspace search and anomaly scoring are intertwined in this method.\nSomeresearchersattempttoleveragedimensionalityreductionorfeatureprojectiontechniques\ntoperformsubspaceanomalydetection.Forexample,givenadatainstancewithitsglobalnearest\nneighbours,Kriegel et al. [109] first projectthesedata instanceswith alld featuresinto subspaces\nofvaryingsize,wherethesubspaceisspannedbythe l largestprinciplecomponentsusingrobust\nPCA. Meanwhile, they compute the projection to the subspaces spanned by the remainingd \u2212l\nprinciple components as its error vectors. Second, they choose the error vector with the largest\nL2-norm value as its anomaly score and explanation. The rationale for using PCA is that the cor-\nrelation dimensionality is highly related to the intrinsic dimensionality of data. Meanwhile, Bin\netal.[ 22]developtheASPCAmodel.Givenadatasetwith D features,theyfirstcomputeandorder\nthe principal components using sparse PCA [94]. The firstk principle components which capture\nmost of the variance are callednormal subspace, and the remainingD \u2212k components are called\nabnormal subspace. An instance is considered anomalous if it has a large projection length in the\nabnormal subspace. Based on sparse PCA, each feature is a linear combination of a few original\nfeatures. Therefore, this method can easily obtain the original features that are responsible for an\nanomaly,resultinginanexplanation.Furthermore,Dangetal.[ 53]introducetheLODImodel.For\neach instance, they first select their neighbours using an information-theoretic tool. Second, they\nuselocaldimensionalityreductiontoselectanoptimalsubspaceinwhichthisinstancecanbemax-\nimally separated from its neighbours. An instance that is relatively easy to separate is considered\nanomalous.Moreconcretely,thelocaldimensionalityreductionproblemissolvedviamatrixeigen-\ndecomposition, which can return the corresponding original features that are most important to\nexplain an anomaly. Additionally, Pevn`y[169] presents Loda, an online anomaly detection model\nthat can also provide explanations. Specifically, Loda first leverages sparse random projections to\nobtain a collection of one-dimensional subspaces. Second, it constructs a histogram in each sub-\nspace, aiming to approximate the probability density. Third, it aggregates these one-dimensional\nhistogramstoestimatethejointprobabilitydensity.Consequently,aninstancewithlowestimated\nprobability density is considered anomalous. For each identified anomaly, Loda can rank features\naccording to their contributions to the anomaly score as an explanation.\nAs we can see, the above-mentioned methods utilise some well-defined criteria to search sub-\nspace and then assign anomaly scores. However, another line of research intends to use random\nsearch strategies to search for subspaces. For instance, Keller et al. [100] propose RefOut, which\nconsists of three steps. They first generate an initial subspace pool that is a set of randomly se-\nlected subspaces. On this basis, they utilise an off-the-shelf anomaly detection model to perform\nanomalydetection,resultinginasetofanomalyscoresforeachinstance.Second,foreachinstance,\nthey generate a refined subspace by maximizing the discrepancy of anomaly scores.",
        "dfdca2dd-1596-4ba0-a11c-de0c92753e26": "For each identified anomaly, Loda can rank features\naccording to their contributions to the anomaly score as an explanation.\nAs we can see, the above-mentioned methods utilise some well-defined criteria to search sub-\nspace and then assign anomaly scores. However, another line of research intends to use random\nsearch strategies to search for subspaces. For instance, Keller et al. [100] propose RefOut, which\nconsists of three steps. They first generate an initial subspace pool that is a set of randomly se-\nlected subspaces. On this basis, they utilise an off-the-shelf anomaly detection model to perform\nanomalydetection,resultinginasetofanomalyscoresforeachinstance.Second,foreachinstance,\nthey generate a refined subspace by maximizing the discrepancy of anomaly scores. Aggregating\nall these refined subspaces leads to a refined subspace pool. Third, they apply again the anomaly\ndetection model on the refined subspace pool to obtain an anomaly score for each instance. Con-\nsideringthatthecardinalityofeachrefinedsubspacemaybedifferent,theynormalizetheanomaly\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "2ff9a8e3-8c89-4ed8-8803-043f07eec19c": "23:18 Z. Li et al.\nscorestoensurecomparability.Accordingly,theyreturnthemaximumanomalyscoreandthecor-\nresponding subspace for each instance as an explanation. Similarly, Savkli & Schwartz [186]p u t\nforward RSMM. Concretely, they first randomly selectm subspaces of dimensionk, ensuring that\neach dimension contributes equally to the final probability model. Second, they construct a mix-\nture model such as Gaussian Mixture Model in each subspace. Third, they compute the geometric\naveraging of the probability densities in all subspaces as the joint probability density. Therefore,\nifatestinstanceislocatedinalow-densityregion,itisconsideredanomalous.Furthermore,toin-\nterpretananomaly,theyrankfeaturesaccordingtohowoftentheyconsistinthesubspaceswhere\nthe anomaly is considered an anomaly.\nSecond,despitetheprevalenceofsubspaceanomalydetection,othertechniquessuchasfeature\nselection have also been exploited to facilitate the interpretability of anomaly detection. For in-\nstance, Pang et al. [159] introduce a wrapper feature selection framework for anomaly detection.\nSpecifically, they first create an internal evaluation metric for anomaly detection and then select\nrelevant features for detecting anomalies by iteratively maximizing this metric. They have only\napplied this framework to their proposed anomaly detection model though, which only works\non categorical data. Besides, Pang et al. [157] propose an embedded feature selection method for\nanomaly detection, dubbed CINFO, which is an ensemble of sequential ensemble learners. Specif-\nically, the base learner, namely, the sequential ensemble learner, iteratively and mutually refines\nthe anomaly detection and feature selection processes. In this way, they build many similar base\nlearners, which are then aggregated to produce the final anomaly scores. Hence, the method does\nnotexplicitlyprovideanyselectedfeaturesandlacksinterpretabilityduetotheuseofanensemble\napproach.Meanwhile,Roshan&Zafar[ 178]developan AutoEncoder(AE)-basedmodelincorpo-\nratingtheSHAPtechniquetodetectandexplainanomaliesincomputernetworkdata.Specifically,\ntheyfirsttrainanAEmodelonexclusivelynormalcomputernetworkdatawithallinputfeatures,\nfollowed by applying Kernel SHAP to explain the predictions of the trained AE model. Next, they\nusethetrainedAEmodeltodetectanomaliesinanotherdatasetcontainingcyberattacksandthen\napply again Kernel SHAP to explain the predictions, aiming to select a subset of important fea-\ntures to identify anomalies. Finally, these selected features are used to train a refined AE model\nfor anomaly detection.\nDiscussion: The subspace anomaly detection methods introduced here are considered inher-\nently explainable since they only explain anomalies identified by themselves. In other words, the\nDetection-Definition and Explanation-Definition of anomaly are usually consistent. Therefore, the\ngenerated explanations are intrinsic regardless of whether the anomaly detection and subspace\nsearch processes are interleaved or decoupled. In contrast, the subspace anomaly detection\nmethods that will be presented in theshallow post-model techniquessection are distinct, as they\naim at interpreting anomalies that are identified by other detection models or experts. As a result,\nthe Detection-Definition and Explanation-Definition of an anomaly are likely to be different since\nthe Detection-Definition is generally unknown.\n5.3 Other Miscellaneous Models in Unsupervised Learning\nIn principle, an anomaly detection model that reveals the rationale for how anomaly scores are\ncalculatedinahumancomprehensiblewaycanbeconsideredintrinsicallyexplainable.Hereinafter,\nwe survey a collection of intrinsically XAD methods that do not belong to the commonly seen\ntransparentmethodsinsupervisedlearningnorfeaturesubset-basedmethods.Duetothediversity\nof these methods, i.e., they share few basic techniques, we organize them according to the type of\ndata they have been designed for.\n5.3.1 Models for Tabular Data. A plethora of models have been devised to detect anomalies in\ntabulardatawhilstprovidingintrinsicexplanations.Forinstance,asatypicalmethod,distribution\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "65115c8a-d8f8-443c-8a14-2d2503622cbd": "A Survey on Explainable Anomaly Detection 23:19\nbased anomaly detection models attempt to fit data with probabilistic distributions. Then, data\ninstances that do not conform to the fitted model are considered anomalous. According to Agye-\nmangetal.[ 5],distribution-basedanomalydetectiontechniquesareintrinsicallyexplainable.This\nis because the identified anomalies can be meaningfully interpreted from a statistical perspective\nonce the probabilistic distribution is known. Dunstan et al. [66] take a different approach and\nutilise a data cube structure to divide transaction data instances into different regions. They refer\nto each region as a context and show how each instance can be abnormal in different contexts.\nMore importantly, they create anomaly tables and anomaly lattices to explain anomalies. An\nanomaly table contains anomalous transactions alongside their contexts. Meanwhile, an anomaly\nlattice graphically displays the anomalies with their contexts. Rather than using groups to\ndefine contexts in which anomalies can be detected, Mejia [139]a d a p t sAdaptive Resonance\nTheory (ART)[ 33] to group instances into clusters such that instances residing in the smallest\nclusters are considered anomalous. By virtue of the good properties of ART, one can obtain\nthe feature differences between every two clusters, resulting in explanations for the anomalous\ninstances.\nSmets & Vreeken [200] take a more global approach to anomaly detection, i.e., they employ\nthe Minimum Description Length(MDL) principle to determine whether a data instance is\nanomalous;inbrief,thenumberofbitsrequiredtoencodeitusingcompressionisusedasanomaly\nscore. They utilise the Krimp algorithm [195] as the compressor, which is trained on exclusively\nnormalsamplestocapturenormalbehaviours.Onthisbasis,theyprovideexplanationsbyshowing\nwhich patterns were recognised in the anomalies, as well as by checking whether small changes\ncanturntheanomaliesintonormalinstances.Ifitcan,theanomaliesareobservationerrorsrather\nthan real anomalies. Instead of using patterns to represent what is normal, Park & Kim [163]p u t\nforwardamodelthatisanensembleof Region-Partition(RP)trees.EachRPtreeistrainedonly\nonnormaldataandthusrepresentsapartitionofthenormaldataregion.Hence,ifatestinstance\ncan arrive at a leaf node of any individual RP tree, it is considered normal. Otherwise, it is an\nanomaly. Considering that the size of each RP tree is small, one can easily find the hypercube in\nwhich the anomaly is stuck. On this basis, they take the intersection of hypercubes of all RP trees\nas an explanation for the anomaly.\nWhile the above-mentioned models focus on static tabular data, Dickens et al. [59]p r o p o s e\nMondrianP\u00f3lyaForest (MPF)todetectandexplainanomaliesonlargedatastreamsbycombin-\ning random trees with non-parametric density estimation approaches. Specifically, the Mondrian\nProcess[179]isafamilyofhierarchicalbinarypartitionsofdataandtheP\u00f3lyaTree[ 137]isanon-\nparametricapproachthatcanestimatethedensityfunctionofbinarypartitions.Theycombinethe\nP\u00f3lya Tree structure with a truncated Mondrian Process to deal with static data, and combine the\nP\u00f3lya Tree structure with a Mondrian Tree to handle data streams. In this way, they construct a\nforest, namely, MPF, for density estimation and anomaly detection. As a result, an instance with\nrelatively low estimated density is considered anomalous. Furthermore, with the good properties\nof MPF, the resulting anomaly scores are probabilistic and, therefore, interpretable.\n5.3.2 Models for Sequential Data. One line of research addresses the problem of detecting and\nprovidingintrinsicexplanationsintimeseriesdata,whichisatypeofsequentialdata.Techniques\nsuchassparselearningandtimeseriesdecompositionhavebeenleveragedtodevelopintrinsically\ninterpretableanomalydetectionmodelsfortimeseriesdata.Forinstance,Lietal.[ 116]apply deep\nGenerative Models(DGMs) to detect anomalies in multivariate time series data.",
        "b4ec0ab9-90e9-4fb9-982a-4cb5d69ed573": "In this way, they construct a\nforest, namely, MPF, for density estimation and anomaly detection. As a result, an instance with\nrelatively low estimated density is considered anomalous. Furthermore, with the good properties\nof MPF, the resulting anomaly scores are probabilistic and, therefore, interpretable.\n5.3.2 Models for Sequential Data. One line of research addresses the problem of detecting and\nprovidingintrinsicexplanationsintimeseriesdata,whichisatypeofsequentialdata.Techniques\nsuchassparselearningandtimeseriesdecompositionhavebeenleveragedtodevelopintrinsically\ninterpretableanomalydetectionmodelsfortimeseriesdata.Forinstance,Lietal.[ 116]apply deep\nGenerative Models(DGMs) to detect anomalies in multivariate time series data. Specifically,\nthey first set up a stackingVariational AutoEncoder (VAE)-based model that constructs a\nsingle-channel block-wise reconstruction, followed by stacking it multiple times using a weight\nsharing technique to handle channel-level similarities. Second, they utilise a graph learning\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "7155fdc6-0f1f-4e63-a7fc-363bbb083fa7": "23:20 Z. Li et al.\nmoduletolearnasparseadjacencymatrixforeverychannel,attemptingtoextractstructureinfor-\nmationforachievinganexplainablereconstructionprocess.Asaresult,atestinstancewithalarge\nreconstruction error is considered anomalous. Meanwhile, Cheng et al. [46] exploit time series\ndecompositiontechniquesfromamulti-scaleperspectivetoidentifyspatiotemporalabnormalities\nofhumanactivity.Theyfirstemploythe Seasonal-TrenddecompositionwiththeLoess (STL)\nmethod to decompose the time series to look for anomalies. As a result, the periodic as well as\ntrend components of observed data are eliminated during the time series decomposition, and the\nremainingcomponentscaptureanomalousactivitysignatures.Byexaminingtheresidualelements\nofthetimeseriesforeachspatialunit,theyareabletoidentifyspatiotemporalanomaliesinhuman\nactivity. Finally, they devise a rule to match anomalies identified at different scales in accordance\nwith their spatiotemporal influence ranges and explain anomalies based on their multi-scale\ncharacteristics.\nAnother line of research focuses on the task of identifying and offering intrinsic explanations\nin network traffic data, which can be seen as an instance of other sequential data. For example,\nGrov et al. [75] first group the network traffic data into different sessions, followed by learning\ntwo behavioural models, namely, aMarkov Chain(MC)m o d e la n daFinite State Automata\n(FSA) model, on normal sessions. Next, for an incoming session, they compute a similarity mea-\nsure with respect to these two models, resulting in an anomaly score. More concretely, the MC\nmodelreturnstwoprobabilitiesastheanomalyscoreandtheFSAmodelreturnsadistanceasthe\nanomaly score. Meanwhile, Mulinka et al. [147] present HUMAN, a hierarchical clustering-based\nmethod.Specifically,theyconsiderthreedifferentclusteringmethodstogroupdatainstancesinto\nclusters,assumingthatthenormalbehaviourisrepresentedbythelargestcluster.Therefore,data\ninstances residing in the smallest clusters are considered anomalous. Next, they explain the de-\ntectedanomaliesbydisplayingtheclusteringresults,includingthenumberofclusters,thesizeof\neach cluster, and a textual explanation of each cluster. Moreover, Marino et al. [132]p r o p o s et h e\nNetwork Transformer model(NeT). First, the network data is represented by a graph where\nits nodes represent network device IP addresses and the edges describe data packets delivered be-\ntween different devices. Second, NeT extracts hierarchical features from the graph for anomaly\ndetection. Third, based on these features, NeT employs existing anomaly detection models\u2014such\nasLOF[ 26],OCSVM[ 188],andAutoEncoders\u2014toidentifyanomaliesatvariousgranularitylevels.\nMoreover, NeT provides explanations based on the graph structure, offering a subset of hierarchi-\ncalfeaturesthatallowuserstopinpointthedevicesaffectedbytheanomaliesandtheconnections\nthat caused the anomalies.\n5.3.3 Discussion. Due to the lack of a unified definition of anomaly and the diversity of data\ntypes, a wide range of in-model XAD techniques have been explored in an unsupervised setting.\nMore concretely, techniques such as Probabilistic Models (e.g., Mondrian P\u00f3lya Forest and other\ndistributionordensityestimation-basedapproaches),DataCubestructure,IncrementalClustering,\nMDL-basedPatternCompression,RPtreesareharnessedtodetectandexplainanomaliesintabular\ndata. Meanwhile, techniques such as MC, FSA, Hierarchical Clustering, Sparse Learning in VAE,\nTime Series Decomposition, and Hierarchical Features in Graph Representation are adapted to\nidentify and interpret anomalies in sequential data.\n5.4 Summary\nAs shown in Table2, the in-model techniques presented in this section are model-specific. More-\nover, the majority of these methods provide feature-based explanations (including pattern-based\nexplanations),withtheexceptionofReferences[ 46,75,83],whichoffersample-basedexplanations,\nand References [41,53] generate explanations from both perspectives.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "dba908b2-7821-48c3-8607-d73db324c200": "A Survey on Explainable Anomaly Detection 23:21\nTable 2. Summary of In-model XAD Techniques\nRef Spec Pers Tech Data Loc Pros Cons\n[83] S P Frequent pattern mining Static TC G Performs frequent pattern mining\nand outlier discovery\nsimultaneously\nOnly applicable to categorical data\n[168] S S Rule-based model to produce\nprobability\nStatic TN G Fast Low accuracy\n[230] S P Frequent pattern mining Static video G Able to detect point anomalies,\ncontextual anomalies, and\ncollective anomalies\nNeeds labeled normal activity data\n[213] S P Frequent pattern mining Streaming\ndynamic graph\nG Able to handle dynamic graphs Computationally expensive\n[138] S P Satisfiability Modulo Theories\nbased rule models\nStatic ES G Integrates contexts to detect\nanomalies\nComputational expensive;\nExplanations may be too long to\nunderstand\n[24] S P Association rule mining +\nVisualisation\nEvent logs L Able to handle process change\nand flexible executions;\nEvaluating the quality of\nexplanations\nAssumes the availability of\ndomains, process models, and\nanomalies in evaluation\n[108] S F Composition-based decision tree Static UTS G No manual tuning of\nhyper-parameters; Evaluating the\nquality of explanations\nNeeds labeled data\n[90] S F One Class Decision Tree Static tabular G Produces compact and readable\nresults\nParameterization of the KDE is\ndifficult\n[4] S F Decision tree based AE Static TC G Intrinsically interpretable AE Not suitable for dataset with many\nfeatures; Only applicable to\ncategorical data\n[41] S F & S LOESS regression Static TM G Able to handle heterogeneous\nfeatures; Evaluating the quality of\nexplanations\nSensitivity to important parameters\nnot discussed\n[29] S F Change Point Model + Regression\nTrees\nstatic tabular G More insights from multiple\nmodels\nNo explanations for ANN\n[111] S F Logistic regression Streaming MTS G Able to predict short-term\nanomalies\nNot able to predict long-term\nanomalies\n[21] S F Gaussian Process Streaming TN G Able to handle unreliable, noisy,\nor partially missing data\nHigh computation cost; Hard to do\nmodel selection; Poor at handling\ndiscontinuities\n(Continued)\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "a7075e65-9f5a-4045-a8b8-9e36eac2d504": "23:22 Z. Li et al.\nTable 2. Continued\nRef Spec Pers Tech Data Loc Pros Cons\n[199] S F Dynamic Bayesian Networks Streaming MTS G Able to incorporate domain\nknowledge\nComputationally expensive\n[226] S F Linear components Static MTS G End-to-end training Weak interpretability due to deep\nmodels\n[38] S F Generalized Additive Models Static TM G Able to incorporate a small\namount of labeled data\nRelies on assumptions about the\ndata generating mechanism\n[149] S F Subspace AD Static TN L More scalable than other\ndensity-based methods\nWeak interpretability due to\nensemble\n[99] S F Subspace AD Static TN L Adjustable to any AD Weak interpretability due to\nensemble\n[109] S F PCA + Subspace AD Static TN L Works with arbitrarily oriented\nsubspaces\nWeakinterpretabilitybyusingerror\nv e c t o r so fP C A ;D o e sn o tw o r kw e l l\nwith high-dimensional data\n[22] S F Sparse PCA + Subspace AD Static TN G Fast Non-trivial parameters setting by\nend-users\n[53] S F & S Subspace AD Static TN L Explores interconnections\nbetween neighboring members\nHigh computational cost; Assumes\nlinear separability of anomalies\nwith their neighbours\n[100] S F Subspace AD Static tabular L Applicable to any AD model Computationally expensive\n[52] S F Subspace AD Static TN L Ensures quality of explanation via\nkeeping local geometry\nNot scalable to high-dimensional\ndata\n[186] S F Subspace AD Static TM L Applicable to mixed data;\nParallelizable; Applicable to\nhigh-dimensional data\nDifficult to initialize clusters for\nGMMs in high-dimensional\nsubspaces\n[169] S F Feature projections Streaming and\nstatic TN\nL Fast; Adapted to concept drift;\nAble to handle missing variables\nOnly considers one-dimensional\nprojections; Weak interpretability\ndue to ensemble\n[159] S F Wrapper feature selection Static TC L &\nG\nWorks well with noisy features Only applicable to categorical data\n[157] S F Embedded feature selection Static TC L Works with high-dimensional\ndata\nLacks interpretability due to the use\nof an ensemble approach\n[178] S F SHAP based feature selection Static MTS/ES G Does not need labelled data Only applicable to AE-based model;\nHigh time complexity with kernel\nSHAP\n(Continued)\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "95169330-d160-4f6b-bc95-f4d216625120": "A Survey on Explainable Anomaly Detection 23:23\nTable 2. Continued\nRef Spec Pers Tech Data Loc Pros Cons\n[5] S F Statistical distribution Static tabular L &\nG\nSound statistical foundation Assumes probabilistic distribution\nof data\n[66] S F Data cube Static tabular L Considers anomalies in multiple\ncontexts\nComputationally expensive\n[139] S F Incremental clustering Static TM G Fast Non-trivial setting of the threshold\nparameter\n[200] S P MDL-based pattern compression Static TC L Provides detailed inspection and\ncharacterisation of decisions\nOnly applicable to categorical data\n[75] S P Markov Chain; Finite State\nAutomata\nNetwork traffic\ndata\nG Robust to new unseen anomalies Trains on normal data\n[147] S S Clustering Network traffic\ndata\nG Able to integrate domain\nknowledge\nNon-trivial setting of parameters\n[59] S F Bayesian nonparametric based\ndensity estimation\nStatic and\nsteaming TM\nG Able to handle streaming data Density estimation does not work\nwell in high-dimensional data\n[163] S F RP Tree Static tabular G Automatically determines the\nthreshold on anomaly scores\nWeak interpretability due to\nensemble\n[116] S F Sparse learning + Reconstruction\nerror\nStatic MTS G Considers single-channel\nanomalies and structural\nmulti-channel anomalies\nOnly considers linear correlation\nbetween channels; Weak\ninterpretability due to VAE\n[46] S S Time series decomposition Spatial-temporal\ndata\nG Considers multi-scales to gain\nmore insights into anomalies\nDoes not work well with data\nmissing and deficiency\n[132] S F Hierarchical features in graph Static graph G Self-supervised training that does\nnot need labeled data\nWeak interpretability due to deep\nmodels\nSpec indicates whether a method is model-agnostic (A) or model-specific (S). Note that all in-model techniques are basically model-specific.Pers specifies whether a method\nisfeature-based(F),sample-based(S)orpattern-based(P). Techindicatesthetechniquesusedineachmethod. Dataindicatesthedatatypeforwhichthemethodisapplicable\n(TN: Tabular Numeric; TC: Tabular Categorical; TM: Tabular Mixed; UTS: Univariate Time Series; MTS: Multivariate Time Series; ES: Event Sequence).Loc shows whether a\nmethod provides a local explanation (L) or global explanation (G). Moreover,Pros and Cons describe the advantages and disadvantages of each method, respectively.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "0fca2d36-1cd3-44d0-b580-9f50a29fe9d5": "23:24 Z. Li et al.\nAccording to their main characteristics, we subdivide them into three high-level groups, i.e.,\ntransparent modelsinsupervisedlearning, featuresubset-basedmodels ,and miscellaneousmodelsin\nunsupervised learning, for which we make the following observations.\nFirst,fortransparentmodelsinsupervisedlearning ,wefindthatmostmethodscanprovideglobal\nexplanationsastheirentirelogiccanbeeasilyunderstoodbyhumansduetotheirtransparentna-\nture.However, decisiontree-basedmodels arehardtoexplainwhenthetreeistoodeeportoowide.\nTo alleviate this problem, feature selection can be leveraged. Meanwhile, an ensemble of decision\ntrees can avoid overfitting of the data, thereby improving the generalization performance. How-\never, an ensemble of trees is not human-understandable. Concerningrule-based models,al a r g e\nset of rules or a long rule is difficult to explain. Therefore, a human-reasonable size is required to\nmaintain interpretability. Furthermore, an inherent problem of usinglinear modelsfor interpreta-\ntion is that when the model does not fit the training data optimally, it may optimize errors using\nspurious features that may be difficult to interpret for humans [76]. Overall, for these transparent\nmodelstoretaintheirinterpretabilitycharacteristics,theymustbelimitedinsizeandthefeatures\nused should be understandable to the end-users [20].\nSecond, forfeature subset-based models, most methods can only provide local interpretations,\nthat is, only a certain output can be interpreted at a time. Besides, some subspace anomaly\ndetection methods do not provide explicit explanations due to the use of ensemble techniques\nto aggregate anomaly scores in multiple subspaces. However, the contribution of each feature is\nrelativelyeasytoobtain.Moreover,mostsubspaceanomalydetectionmethodswereoriginallyde-\nsignedtotackletheissueof curseofdimensionality whendetectinganomaliesinhigh-dimensional\ndata [231]. Therefore, promoting interpretability is not their main concern. Notably, we observe\nthatthereisextremelylimitedresearchonthewrapperorembeddedfeatureselectionforanomaly\ndetection.\nThird,for miscellaneousmodelsinunsupervisedlearning ,thesemethodsarequitedifferentfrom\neach other, as they are specifically designed for the anomaly detection and not explored in a\nsupervised setting. Given the lack of a unified definition of anomalies and the diversity of data\ntypes, it is not surprising that these approaches are very diverse. Importantly, we note that most\nofthesemethodscanprovideglobalexplanations,inthesensethatthelogicofthewholemodelis\nhuman-understandable, or some important properties of the model can be leveraged to interpret\nall decisions.\n6 LITERATURE REVIEW ON SHALLOW POST-MODEL TECHNIQUES\nPost-model methods inspect an anomaly detection model after the detection process is completed,\nor just inspect a given anomaly without being given an anomaly detection model. In other words,\nthesetechniquesdonotinterferewiththeanomalydetectionprocess,operatingonlyonthebasis\nof correlating the input of the anomaly detection model (if any) with its output. Due to the prolif-\neration of techniques in this category, in this section, we only introduce techniques designed for\nnon-deeplearningprocesses,andwecallthem shallowpost-modelanomalyexplanation techniques.\nMostshallowpost-modelanomalyexplanation methodsintendtofindasubspaceorasetofsub-\nspacesinwhichthegivenanomalydiffersthemostfromotherinstances,andwecallthesemethods\nsubspace-basedmethods.Surrogatemethods,ontheotherhand,resorttoidentifyanothermodelto\nexplain the anomaly detection model or just the given anomalies. Specifically, a surrogate model\ncan be a transparent model, such as a set of rules or a decision tree, or an opaque model, such\nas XGBoost or SVM. Importantly, if the surrogate model is an opaque model, it should be easy to\ninterpret by using XAI techniques such as SHAP. Meanwhile,miscellaneous methodssuch as com-\nparing patterns to find differences, leveraging SHAP techniques to measure feature importance,\nand visualisation also play an important role in shallow anomaly explanation.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "b341de35-568c-4366-86ae-8bbea5be7152": "A Survey on Explainable Anomaly Detection 23:25\n6.1 Subspace-based Methods\nGivenananomalyoragroupofanomalies, subspace-basedmethods aimatfindingasubspaceora\nset of subspaces in which the anomaly deviates the most. Different from the intrinsically explain-\nable subspace anomaly detectionmethods that were introduced in Section5.2, the subspace based\nmethods investigated hereinafter do not assume the availability of anomaly detection models.\nFirst, different explanation methods usually have different definitions for anomaly, dubbed\nExplanation-Definition in this survey, leading to different measures of abnormality. For instance,\nKnorr&Ng[ 106]definestrongestandweakoutliers,basedonwhichtheyuseso-calledintensional\nknowledge to explain anomalies. For each anomaly identified in the original feature space, they\nreport the minimal subspaces in which it behaves anomalously. Particularly, in their proposed al-\ngorithmCELL,foreachinstance,theyutilizethenumberofneighboursinitslocalneighbourhood\nof a given radius as the anomaly score. However, this anomaly measure can be replaced by other\nanomaly measures such as density, depth, and so on. As far as we know, their work is seminal in\nanomaly interpretation. Additionally, Zhang et al. [228] propose HOS-Miner to identify outlying\nsubspacesforagivenanomaly.Specifically,theydefinethesumofdistancesbetweentheanomaly\nand itsk-nearest neighbours as its anomaly score in each subspace, thereby returning the outly-\ning subspace with the lowest dimensionality as an explanation. Similarly, Micenkov\u00e1 et al. [141]\npropose an anomaly explanation technique that works on tabular dataset with numeric features.\nSpecifically, given an anomaly, they look for a subspace in which this instance is well separable\nfromtherest.Toachievethis,theyfirstgenerateaclassificationdatasetconsistingofacomparable\nnumberofnormalandabnormalinstances.Second,theyapplyanexistingfeatureselectionmethod\nto find a subset of features that are relevant for the classification, namely, separation. Particularly,\nthey define a measure of separability based on the probability density function of a normal dis-\ntribution as the anomaly measure. Finally, the obtained subspace serves as an explanation for the\nanomaly.\nSecond, some researchers attempt to provide explanations from multiple perspectives or con-\ntexts. For instance, Angiulli et al. [10] propose a method that is capable of providing explanations\nfrom both global and local perspectives. On the one hand, for an anomaly, they measure its\nabnormality with reference to all data instances in different subspaces, delivering the subspace\nwith the highest abnormality as a global explanation. On the other hand, for an anomaly, they\nfirstselectasubsetoffeaturesandthecorrespondingvaluestodefineareferencegroup,andthen\ncompute its abnormality with respect to this reference group in different subspaces. Accordingly,\nthe reference group and the corresponding subspace with the highest abnormality constitute a\nlocal explanation. Note that the definitions of global explanation and local explanation used in\nReference[10]differfromthosewedefinedinthissurvey.Particularly,the abnormality isdefined\nin terms of the frequency of the anomalous instance and the frequencies of referencing instances.\nFurthermore, M\u00fcller et al. [148] present OutRules, which generates multiple explanations for an\nanomaly in different contexts. Specifically, OutRules explains an anomaly by generating rules\nthat describe the deviation of this instance in contrast to its context. On the one hand, a subset\nof features is used to define a context consisting of highly clustered instances. On the other hand,\ntheyattempttofindanextendedsubsetoffeaturesinwhichoneoftheseinstancesissignificantly\ndeviating. Concretely, the anomaly measure used in their framework can be instantiated by the\nunderlying anomaly score of any anomaly detection model such as LOF. Similarly, Angiulli et al.\n[9] devise a method that consists of two steps. Given an anomaly and a dataset, for each feature,\ntheyfirstdeterminetheintervalthatincludestheanomalyandtheassociatedcondition,resulting\nin a set of conditions on all features. Second, they employ an Apriori-like strategy to search for\nexplanation-property pairsfor the anomaly.",
        "5299651d-c045-4599-8f4b-b338b55212ba": "Specifically, OutRules explains an anomaly by generating rules\nthat describe the deviation of this instance in contrast to its context. On the one hand, a subset\nof features is used to define a context consisting of highly clustered instances. On the other hand,\ntheyattempttofindanextendedsubsetoffeaturesinwhichoneoftheseinstancesissignificantly\ndeviating. Concretely, the anomaly measure used in their framework can be instantiated by the\nunderlying anomaly score of any anomaly detection model such as LOF. Similarly, Angiulli et al.\n[9] devise a method that consists of two steps. Given an anomaly and a dataset, for each feature,\ntheyfirstdeterminetheintervalthatincludestheanomalyandtheassociatedcondition,resulting\nin a set of conditions on all features. Second, they employ an Apriori-like strategy to search for\nexplanation-property pairsfor the anomaly. More concretely, anexplanation is a set of conditions\nused to define a context where the anomaly is located. Meanwhile, aproperty is an additional\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "1e61b6f3-864b-452e-8f9e-f06ba757c577": "23:26 Z. Li et al.\ncondition posed on a feature other than the features that are used to define the context, aiming\nto distinguish the anomaly from the context. In particular,they define an anomaly measure based\non the probability density function of each feature. Consequently, theexplanation-property pair\nand the corresponding anomaly score constitute an explanation for the anomaly.\nThird, other techniques such as visualisation can be leveraged to further improve the explain-\nabilityofsubspace-basedmethods.Forinstance,givenadatasetconsistingofreal-valuedfeatures\nand a list of anomalies, Gupta et al. [79] propose the so-called LOOKOUT approach to explain\nthese anomalies. Specifically, they attempt to find a limited number of 2-dimensional subspaces\nin which the given anomaly deviates the most from the rest. Particularly, if the anomaly is de-\ntectedbyananomalydetectionmodel, theyutilisetheunderlyinganomalymeasuretoobtainthe\nanomaly score. Otherwise, they employ any other off-the-shelf model such as LOF to obtain the\nanomalyscore.Moreover,theyvisualisethesesubspacesusing2-dimensionalscatterplots,known\nas focus plots in their article, and then present thesefocus plotsto the end-users as explanations.\nAlso,theabove-mentionedapproachOutRules[ 148]utilisesparallelcoordinatesplotstovisualise\nthe anomalies.\nFourth,somemethodshavebeenexploredtoexplainanomaliesinagroup.Forinstance,Angiulli\net al. [11] extend their previous work [10] to explain a group of anomalies. Furthermore, Macha\n&A k o g l u[129] propose x-PACS to explain anomalies in a group in three steps. They first utilise\nasubspaceclusteringalgorithmtoidentifyclustersthattheanomaliesform.Second,foreachsub-\nspaceclusterofanomalies,theyleverageanaxis-alignedhyper-ellipsoidtorepresentit.Third,they\nemploy the MDL criterion to identify a set of hyper-ellipsoids that are compact, non-redundant,\nand pure. Particularly, x-PACS does not require a measure of outlyingness since they address the\nanomaly explanation problem from a subspace clustering perspective.\nFifth, with the emergence of many anomaly explanation methods, some researchers endeavour\nto formally define the anomaly explanation problem or propose a taxonomy of existing methods.\nConcretely,Kuo&Davidson[ 110]formallydefinetheoutlierdescription(namely,anomalyexpla-\nnation) problem and propose aConstraint Programming(CP)-based framework to encode the\nproblem. Particularly, they utilise a neighbourhood density based criterion to measure the outly-\ningnessofaninstanceineachsubspace.Onthisbasis,theyintroduceaCPframeworktolearnthe\noptimal subspace to explain an anomaly. Their framework and variants can explain an anomaly\nin a single subspace, multiple subspaces, or by introducing the human in the loop. Meanwhile,\nVinh et al. [216] for the first time divide outlying aspects mining techniques into two categories,\nviz. feature selection-based approaches and score-and-search approaches, and additionally make\ntwo important contributions. First, they formalize the concept of dimensionality-unbiasedness\nfor anomaly scoring functions. They show that some widely used anomaly scoring functions\nsuchas distanced-basedanddensity-basedscoringmeasurementsviolate thisimportantproperty.\nMoreover, they put forward two dimensionality-unbiased anomaly scoring functions, namely,\nZ-scoreandisolationpathscore,tomeasuretheoutlyingnessofaninstanceindifferentsubspaces.\nSecond, they propose a beam search framework to overcome the limitation of exhaustive search\nin exponentially large space. Consequently, for instance, they return the subspace with the\nhighest dimensionality-unbiased anomaly score as an explanation. However, Samariya et al.\n[184] point out an issue of using Z-score normalisation of density to rank subspaces for outlying\naspectsmining.Particularly,Z-scorenormalisationhasabiastowardsdatadistribution(withhigh\nvariance subspaces) although it is dimensionality-unbiased. To tackle this issue, they propose\nanother anomaly scoring function called SiNNE for outlying aspects mining. Specifically, SiNNE\nconsists of an ensemble of models where each model is developed based on a subset of data.",
        "cccd47cd-ca94-4fd1-a218-8f33b3387898": "Second, they propose a beam search framework to overcome the limitation of exhaustive search\nin exponentially large space. Consequently, for instance, they return the subspace with the\nhighest dimensionality-unbiased anomaly score as an explanation. However, Samariya et al.\n[184] point out an issue of using Z-score normalisation of density to rank subspaces for outlying\naspectsmining.Particularly,Z-scorenormalisationhasabiastowardsdatadistribution(withhigh\nvariance subspaces) although it is dimensionality-unbiased. To tackle this issue, they propose\nanother anomaly scoring function called SiNNE for outlying aspects mining. Specifically, SiNNE\nconsists of an ensemble of models where each model is developed based on a subset of data.\nHowever,duetotheuseofensembletechniques,SiNNEcannotprovideexplanationsforidentified\nanomalies.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "eb1cc871-947a-4430-94be-36e0c72cbf09": "A Survey on Explainable Anomaly Detection 23:27\nFinally, another line of research attempts to explain any given instance that may be anomalous\nornormal.Forexample,givenadatainstance,Duanetal.[ 64]developamethodtofinditsminimal\noutlying subspace, i.e., the subspace with the lowest dimensionality where the query instance is\nmostdeviatingfromothers.Toachievethis,theyfirstassumethattheinstancesaregeneratedfrom\na probability distribution that is often unknown. Second, they utilise kernel density estimation\ntechniques to approximate the probability density of an instance in each subspace, deriving its\nanomalyscore.Moreover,theyemployheuristictechniquestoprunethesetofpossiblesubspaces\nthat need to be explored.\nDiscussion: Most subspace-based methods reviewed above suffer from two limitations: high\ncomputational costs and poor explanation fidelity. First, most methods intend to find a minimal\nsubspace in which the anomaly deviates the most. To find such a subspace, they usually need\nto go through the exponentially large search space. Although some pruning techniques such as\nbeam search are leveraged to mitigate this problem, optimality is no longer guaranteed. Second,\nalmost all methods in this category have their own definitions of anomaly (namely,Explanation-\nDefinition)whentryingtointerpretanomalousinstances.Importantly,this Explanation-Definition\nisverylikelytodifferfromthe Detection-Definition,leadingtoapoorfidelityoftheexplanation.In\notherwords,iftheanomalydetectionmodelisavailable,theprovidedexplanationmaynotreflect\nits actual decision-making process.\n6.2 Surrogate Methods\nAlineofresearchinshallowpost-modelXADtechniquesistoutilisesurrogatemodelstodescribe\ngivenanomaliesoranomalydetectionmodels.Ingeneral,thesurrogatemodelcanbeatransparent\nmodel or an opaque model. If a transparent model such as a set of rules or a decision tree is\nemployedtodepicttheanomaly,theresultisdirectlyunderstandable.However,ifanopaquemodel\nsuchasXGboostorSVMisleveragedtoapproximatetheoutputs,additionalXAItechniquessuch\nas SHAP or LIME are required to make the results understandable.\nFirst of all, model-agnostic rule learners are often leveraged to extract a set of rules or patterns\nas the surrogate model, aiming to explain anomalies. For instance, Ertoz et al. [67] present the\nMINDS framework for network intrusion detection and explain anomalies by association rules.\nSpecifically, they first utilise an off-the-shelf anomaly detection model such as LOF [26] to detect\nanomalousnetworkconnections.Second,theydevelopaDiscriminatingAssociationPatternGen-\nerator to extract patterns that exclusively characterise normal instances or anomalous instances,\nrespectively.Theextractedpatternsarehuman-comprehensibleandthusserveasexplanationsfor\nanomalies. Moreover, they attempt to assign anomalies to different groups based on the extracted\npatterns. Alternatively, Davidson [55] capitalizes on mixture modelling (a.k.a. model-based clus-\ntering) to perform anomaly detection. Specifically, for each data instance, this method calculates\nthelikelihoodofthisinstancebelongingtoeachcluster.Ifthemaximumobtainedlikelihoodisless\nthan a predefined threshold, the instance does not belong to any cluster and is therefore consid-\nered anomalous. Moreover, they describe a visualization approach to show normal and abnormal\ninstances based on scatter plots, enabling end-users to quickly understand why an instance is\nconsidered anomalous. More importantly, they try to extract rules (in Conjunctive Normal Form)\nto describe each obtained cluster and those anomalies. By comparing these rules, one can easily\nunderstand why an anomaly is anomalous.\nMeanwhile, some model-specific rule learners are proposed to extract a set of rules or patterns\nas the surrogate model, aiming to explain anomalies. For example, Das et al. [54] define a novel\nformalism, known as compact description, to extract rules to describe discovered anomalies.\nHowever, this method can only be applied to tree-based ensembles, and the extracted rules are\nrepresented using Disjunctive Normal Form(DNF). Moreover, they also develop an active\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "f38bdf2b-b902-4c9c-a533-6448ed26027f": "23:28 Z. Li et al.\nanomalyexplanationalgorithmforgenericensembles,dubbedGLAD.Foreachdetectedanomaly,\nGLAD first identifies the base-learners (namely, ensemble members) that contribute the most to\nthe decision. Second, GLAD applies a model-agnostic explanation method such as LIME on these\nimportant base-learners, respectively, to generate explanations for the anomaly. Besides, Barbado\net al. [18] apply several rule extraction techniques to OCSVM models [188] for anomaly explana-\ntion,andevaluatethequalityofgeneratedexplanationsaccordingly.Specifically,thesetechniques\nfirst apply OCSVM to obtain normal and abnormal instances. Second, they use an off-the-shelf\nclustering method such as K-Prototypes [93] to iteratively divide the non-anomalous instances\nintodifferentregionsuntilnoanomaliesarecontainedinthegeneratedregions.Third,sincethese\nregions are in the form of hypercubes, they can directly extract rules from the vertices of these\nhypercubes to explain why an instance is non-anomalous. More importantly, they define several\nmetrics including comprehensibility, representativeness, stability and diversity to evaluate the\nqualityofexplanations.Besides,theirmethodscanprovidebothlocalandglobalexplanations.Al-\nthoughitisclaimedthatthewholeprocesscanbeadaptedtoanyanomalydetectionmodel,thisis\nnot shown.\nSecond,somerulelearnersareexploredtoextractdecisiontreesasthesurrogatemodel,aiming\nat explaining anomalies. For instance, Xu et al. [222] propose an approach to detect and explain\nsystem problems by mining console logs. Concretely, they leverage aPrinciple Component\nAnalysis (PCA)-based anomaly detection method [65] to identify anomalies, followed by ex-\nplainingtheresultsusingdecisiontreestomimicthedecision-makingprocess.However,Binetal.\n[22]showthatusingdecisiontreestoexplainthePCAmodelcanbemisleading,therebyfailingto\nreveal the true decision-making process. Besides, Pevn`y&K o p p[170] introduce a method called\nExplainer to explain anomalies using DNF. Specifically, given an anomaly, Explainer first trains a\ncollectionoftreesknownas SaplingRandomForests (SRF).EachtreeinanSRFisabinarydeci-\nsiontreewiththeaimofseparatingtheanomalyfromothernormalinstances.Second,onceatree\nisbuilt,theyutiliseDNFtorepresentthepathfromtherootnodetothenodethatcontainsonlythe\nanomaly.Third,theyaggregatetheDNFsfromalltreestoacompactDNFtointerprettheanomaly.\nFurthermore, Kopp et al. [107] extend Explainer by introducing twok-means-based clustering\nmethods to interpret anomalies when these anomalous instances form natural micro-clusters.\nThird,someresearchersattempttoutilisewell-studiedopaquemodelsassurrogatemodels,and\nthen leverage additional explanation techniques such as SHAP to explain surrogate models. For\nexample, to monitor the average fuel consumption of fleet vehicles, Barbado and Corcho [17]s e t\nup an unsupervised anomaly detection process capable of explaining decisions through feature\nimportance. First, they leverage a threshold-based model to detect anomalies. Second, they uti-\nlize two types of surrogate models to explain anomalies, including black-box anomaly detection\nmodelswithapost-hoclocalexplanation(XGBoost[ 44]andLightGBM[ 98]withLIMEorSHAP),\nandtransparentanomalydetectionmodels(ElasticNet[ 232]andEBM[ 153]).Thir d,the ye valuate\nthesesurrogatemodelsintermsofpredictivepowerandexplanatorypower.Particularly,theirex-\nplanation method can also integrate domain knowledge given by business rules or counterfactual\nrecommendations. Furthermore, Kiefer & Pesch [102] put forward an ensemble-based anomaly\ndetection model combined with model-agnostic explanation technique to identify and interpret\nanomaliesinfinancialauditingdata.Specifically,theyconstructanensemblearchitecturetoincor-\nporate a wide range of unsupervised anomaly detection models, attempting to identify different\ntypes of anomalies.",
        "5d097baf-4401-4e89-87bb-573e24ab7a9c": "Furthermore, Kiefer & Pesch [102] put forward an ensemble-based anomaly\ndetection model combined with model-agnostic explanation technique to identify and interpret\nanomaliesinfinancialauditingdata.Specifically,theyconstructanensemblearchitecturetoincor-\nporate a wide range of unsupervised anomaly detection models, attempting to identify different\ntypes of anomalies. To interpret anomalies, they propose a four-step method: synthetic oversam-\npling of anomalies, supervised model approximation (using SVM or XGBoost), LIME-based local\nexplanation, and explanation post-processing (visualisation or natural language description).\nDiscussion: Ascanbeseen,mostmethodsinthiscategoryleveragerulelearnerstoextractaset\nof rules or patterns to describe anomalies. Importantly, the resulting rules are often represented\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "2007e46e-b291-4169-9933-fcd935c7879b": "A Survey on Explainable Anomaly Detection 23:29\nusing a DNF or Conjunctive Normal Form. Consequently, it might be relatively easy to evaluate\nthe quality of the resulting explanations, but this depends on many factors.\n6.3 Miscellaneous Methods\nInadditiontosubspace-basedmethodsandsurrogatemethods, miscellaneousmethods suchasvisu-\nalisationandShapleyvaluesareoftenusedtoobtainfeatureimportanceasexplanations.Moreover,\npattern comparison is commonly explored in sequential data to interpret anomalies in a post-hoc\nmanner. Due to the diversity of these methods, we again organize them according to the type of\ndata to which they are applicable.\n6.3.1 Models for Tabular Data. A wide range of methods has been proposed to explain anom-\naliesintabulardatabyshowingfeaturecontributionorselectingasubsetoffeatures.Importantly,\nsomeofthesemethodsaremodel-agnostic.Forinstance,Liuetal.[ 123]introducetheCOINframe-\nwork that consists of four main steps. For each anomaly, they first find its neighbours based on a\ndistancemeasuresuchasEuclideandistance.Second,theyleverageexistingclusteringalgorithms\nto subdivide the anomaly and its neighbours into multiple disjoint clusters. Third, they apply a\nstrategy such as synthetic sampling to expand the size of the anomaly cluster where the anoma-\nlous instance is located. Fourth, they train a simple classifier to separate these clusters, deriving\nananomalyscoreandfeaturecontributionsfromtheparametersoftheclassifier.Moreover,COIN\ncan also incorporate prior knowledge into the explanation process. Importantly, Siddiqui et al.\n[193]p r e s e n tSequential Feature Explanations(SFEs) to explain detected statistical outliers.\nConcretely, given an anomaly identified by any density-based detector, SFEs sequentially present\na feature to the analyst until the analyst can confidently identify this anomaly. As a result, these\nfeaturesusedtoidentifytheanomalyconstitutethecorrespondingexplanations.Particularly,Sid-\ndiquietal.[ 194]applyIsolationForest[ 122]todetectcyberattacksandleverageSFEstogenerate\nexplanations.\nShapley value-based methods are often leveraged to obtain feature importance as explanations.\nForexample,Parketal.[ 164]employSHAP[ 128]toexplainanomaliesbyshowingfeaturecontri-\nbutions. Concretely, they set up an anomaly detection model by using random forest and employ\ntheSHAPapproachtoexploretherelationshipbetweenmodelresultsandinputvariablestogener-\nateexplanations.Similarly,Kimetal.[ 103]utilizeIsolationForestonsensorstreamdataofmarine\nengines to keep track of unusual engine conditions. Moreover, they leverage SHAP to identify\nwhich sensor is in charge of each abnormal data event and to quantify its contribution to the ob-\nservedanomaly.Besides,usingreconstructionerrorsasameasuretodetectandexplainanomalies\nis a common practice in unsupervised anomaly detection. However, Takeishi [209] argues that by\nsimply looking at the reconstruction error of each feature, one may fail to find the true cause of\nthe anomaly. This is because a large reconstruction error in one feature may stem from another\nfeature. To mitigate this problem, a method is introduced to compute the Shapley values [205]o f\nreconstruction errors for PCA-based anomaly detection method. The numerical examples show\nthat the Shapley values are superior to reconstruction errors for explaining an anomaly.\nModel-specific techniques have also been developed to explain anomalies in tabular data, espe-\ncially for Isolation Forest [122], a state-of-the-art anomaly detection model. For example, Kartha\net al. [95] develop a method to interpret anomalies identified by Isolation Forest by exploring\nthe internal structure of an Isolation Forest to generate a feature importance vector, indicating\nthe contribution of each feature to the anomaly score. Similarly, Carletti et al. [32] propose DIFFI\nto obtain feature importance scores for explaining Isolation Forest. Specifically, DIFFI provides a\nglobal feature importance score for each feature, indicating how that feature affects the overall\ndecisionsofIsolationForestonthetrainingdata.Meanwhile,theypresentalocalversionofDIFFI,\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "9c301e2e-dbee-4836-96d1-d681a8f607e5": "23:30 Z. Li et al.\nnamed local-DIFFI, to provide a local feature importance score for each feature, describing how\neachfeatureparticipatesinmakingindividualdecisionsonthetestdata.Importantly,theydevelop\nafeatureselectionmethodforunsupervisedanomalydetectionproblemsonthisbasis.Particularly,\nCarletti et al. [31] apply DIFFI on real-world semiconductor manufacturing data to demonstrate\nits effectiveness.\n6.3.2 Models for Sequential Data. A common strategy to explain anomalies in sequential\ndata is to contrast the observed pattern with its expected pattern or normal patterns. For\ninstance, Babenko & Pastore [15] leverage LFA [131] to detect anomalies in system logs. On\nthis basis, they present the so-calledAutomata Violation Analyzer(AVA) to automatically\nexplain anomalies detected by LFA. Specifically, AVA providesbasic explanationsby comparing\nthe expected event sequence with the observed event sequence, generating relatively simple\nexplanations for the anomalous events. Furthermore, they combine thesebasic explanationsto\nobtaincomposite explanations. Finally, they order these basic and composite explanations by their\nlikelihood of explaining differences between the expected event sequences and the observed\nevent sequences. In addition, Leue & Befrouei [113] design a method to explain counterexamples\nthataresymptomsofdeadlocksinconcurrentsystems.Thesecounterexamplescanbeconsidered\nas anomalies and the authors use sequential pattern mining to produce explanations for these\nanomalies. Specifically, they extract fixed-length common substrings from anomalous sequences\nand contrast them with normal sequences to explain the occurrence of anomalies.\nMeanwhile, leveraging visualization to explain anomalies in sequential data is also a common\npractice.Forexample,Rieck&Laskov[ 176]proposeatechniqueforexplainingintrusiondetection\nresults.Specifically,theypresenttwomethodsforanomalydetection,viz.globalanomalydetection\nandlocalanomalydetection.Foreachpayload,theglobalanomalydetectionmethodcomputesits\ndistancetothecentreofallpayloadsasitsanomalyscore;Incontrast,thelocalanomalydetection\nmethodcomputestheaveragedistancetoitsk-nearestneighboursasitsanomalyscore.Toexplain\nan anomaly, they present a visualization tool to show the feature differences between the anoma-\nlouspayloadandthenormalpayloads.Alargedifferenceinafeaturemeansthatthecorresponding\nfeature value is anomalous. Furthermore, they also highlight the network content corresponding\ntothefeaturevalueintheoriginalpayload.Besides,Alizadehetal.[ 7]implementan AutoRegres-\nsive Integrated Moving Average(ARIMA)-based model together with aVirtual Reality(VR)\ntooltodetectandinterpretabnormalvehicleoperatingstates.Specifically,modernvehiclesareof-\ntenequippedwithmultiplesensorstocollectdatausedtomonitortheiroperatingstatus.Todetect\nanomaliesinsuchmulti-channeltimeseriesdata,theydevelopanARIMAmodelforeachchannel\n(i.e., for each individual univariate time series). Hence, a large difference between the actual\nvalue and the predicted value indicates an anomaly. Importantly, they build a VR tool to visualize\nresiduals from ARIMA models, aiming to better understand anomalies. Moreover, Markou et al.\n[133] create a tool for exploiting internet data to find abnormalities in transportation networks\nand connecting them to unique events. First, the baseline normality corresponding to GPS data\nfor taxi journeys in New York City is trained based on historical mobility data. Next, they scan\nvarious days to look for days where demand deviates greatly from normality in order to identify\nabnormalities. To investigate the severity of daily traffic abnormalities, they consider the Z-score\nformulaofkerneldensityvalues.ThecurrenttrafficsituationisconsideredabnormaliftheZ-score\nvalue exceeds a given threshold. To explain the anomaly, they diagram the time and place of the\nanomaly and utilize that information to look for nearby unusual events using Google Searches.\nDiscussion:ThissubsectionreviewedawiderangeofshallowXADtechniquesthatareexplored\nto interpret anomalies in a post-hoc manner.",
        "c76c1a4d-e5e1-4413-bf50-7f59b0237ba6": "[133] create a tool for exploiting internet data to find abnormalities in transportation networks\nand connecting them to unique events. First, the baseline normality corresponding to GPS data\nfor taxi journeys in New York City is trained based on historical mobility data. Next, they scan\nvarious days to look for days where demand deviates greatly from normality in order to identify\nabnormalities. To investigate the severity of daily traffic abnormalities, they consider the Z-score\nformulaofkerneldensityvalues.ThecurrenttrafficsituationisconsideredabnormaliftheZ-score\nvalue exceeds a given threshold. To explain the anomaly, they diagram the time and place of the\nanomaly and utilize that information to look for nearby unusual events using Google Searches.\nDiscussion:ThissubsectionreviewedawiderangeofshallowXADtechniquesthatareexplored\nto interpret anomalies in a post-hoc manner. In contrast tosubspace-based methodsand surrogate\nmethods,thetechniquesinvestigatedherevarybydatatype.Forinstance,PatternComparisonand\nVisualisation are commonly utilized to explain anomaly in sequentialdata. Methods in this group\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "fa6a13f9-a17a-40ac-a12c-803dd5b04c28": "A Survey on Explainable Anomaly Detection 23:31\nusually do not have an explicitExplanation-Definition of anomalies since they directly illustrate\nthe anomalies by comparing patterns or using visualisation tools. Meanwhile, feature importance\nthatisobtainedbyusingSHAPtechniques,separationorisolation-basedmeasure,playsanimpor-\ntant role in explaining anomalies in tabular data. Using Shapley values techniques such as SHAP\nto obtain feature importance usually does not require a definition of anomaly. Moreover, model-\nspecifictechniquessuchasReferences[ 7,32,95]generallyhaveconsistent Explanation-Definition\nandDetection-Definitionofanomalyastheyexploretheinternalstructureofananomalydetection\nmodel to generate explanations. On the contrary, model-agnostic techniques such as Reference\n[123] usually have anExplanation-Definition that may differ from theDetection-Definition.\n6.4 Summary\nWhile Table3provides the full characterization for all methods discussed in this section based on\nthe six criteria of our taxonomy, we here make some general observations on shallow post-model\nXAD techniques.\nFirst, nearly allsubspace-based methodsand surrogate methodsare model-agnostic in the sense\nthattheyareapplicabletoanyanomalydetectionmodelorgivenanomalies.Inotherwords,these\nmethods do not explore the internal structure of an anomaly detection model and, therefore, can-\nnot have a full grasp of the underlying decision-making mechanism, rendering the provided ex-\nplanations less useful and potentially resulting in weak interpretability. In contrast,miscellaneous\nmethods are mainly model-specific, as they explore the internal structure of anomaly detection\nmodels to generate feature importance as explanations. As a result, the obtained explanations are\nmore reliable and actionable.\nSecond, all these methods provide feature-based explanations (including pattern-based expla-\nnations) except that Davidson [55] provides sample-based explanations. We consider the lack of\nsample-basedexplanationmethodstobeagapintheliteraturethatmightbeofinterestforfuture\nresearch.\nThird, most shallow post-model XAD techniques, especiallysubspace-based methodsand mis-\ncellaneousmethods,canonlyprovidelocalexplanations.Inotherwords,theycanmerelyinterpret\nan individual anomaly at a time. As a result, the explanation may be highly sensitive to noise or\nbiasedsincetheemployedXADmethodsareshortofaholisticperspectiveonthedecision-making\nprocess and logic.\n7 LITERATURE REVIEW ON DEEP POST-MODEL TECHNIQUES\nDeeplearning,basedonartificialneuralnetworks,hasbecomeprevalentinanomalydetectiondue\ntoitscapabilitytolearnexpressivefeaturerepresentationsand/oranomalyscoresforcomplexdata\nsuch as text, audio, images, videos and graph[161]. A wealth of deepanomaly detectionmethods,\nincludingthosebasedonAE, LongShort-TermMemory (LSTM),CNN, GenerativeAdversar-\nial Network(GAN) and other neural networks, have been proposed and have been shown to be\nmore accurate than traditional methods when it comes to detecting anomalies in complex data.\nHowever, although deep anomaly detection methods tend to have high detection accuracy, they\nare often criticized for their poor interpretability. For this reason, some studies have attempted\nto leverage post-hoc XAI techniques to improve the interpretability of corresponding neural net-\nworks.Importantly,whichXAItechniquesareavailablemayvarydependingonthespecificneural\nnetwork used. For instance, AE-based models typically employ reconstruction errors to explain\nanomalies,whileLSTM-basedmodelsgenerallyleverageSHAPtechniquestointerpretanomalies.\nTherefore, we will present the review results according to the type of neural network used to per-\nform anomaly detection, which is correlated with the data type that can be used (e.g., CNNs for\nimages and RNNs for sequential data).\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "dcd38308-b6fd-45ea-b9e1-24f38dd3781c": "23:32 Z. Li et al.\nTable 3. Summary of Surveyed Shallow Post-model Techniques\nRef Spec Pers Tech Data Loc Pros Cons\n[106] A F Others (Subspace) Static tabular L Applicable to any AD; Good\nexplanation fidelity (exception in\npost-hoc)\nHigh computational cost\n[228] A F Others (Subspace) Static tabular L Applicable to any AD Poor explanation fidelity; Compares\nscores in subspaces with different\ndimensionalities\n[10] A F Others (Subspace) Static TC L &\nG\nApplicable to any AD Poor explanation fidelity; Only\napplicable to categorical data\n[148] A F Others (Subspace + Context) Static TN L Applicable to any AD; Considers\nmultiple contexts\nPoor explanation fidelity\n[141] A F Others (Subspace) Static TN L Applicable to any AD Poor explanation fidelity\n[64] A F Others (Subspace) Static TN L Applicable to any AD Poor explanation fidelity; KDE does\nnot work with high-dimensional data\n[110] A F Others (Subspace) Static TN/TC L Applicable to any AD; Enables\nhuman in the loop\nPoor explanation fidelity; Poor\nscalability\n[216] A F Others (Subspace) Static TN L Applicable to any AD; Fast search;\nDimensionality unbiased\nPoor scalability in high-dimensional\ndata; Poor explanation fidelity\n[79] A F Visualisation Static TN L Applicable to any AD; Easy to\nunderstand by non-experts\nOnly considers 2D subspace\n[9] A F Others (Subspace + Contextual\nContrast)\nStatic TM L Applicable to any AD; Considers\nboth numeric and categorical\nfeatures\nKDE does not work well in\nhigh-dimensional subspace\n[129] A F Others (Subspace + Rule Extraction) Static TN L Applicable to any AD; Explains\nanomalies in groups\nPoor explanation fidelity\n[67] A P Approximate (Rule\nExtraction/Association Rule Mining)\nStatic ES L Applicable to any AD Poor explanation fidelity\n[55] S S Approximate (Rule Extraction) +\nVisualisation\nStatic TM G Provides visual explanations Only applicable to clustering based\nAD\n[222] A F Approximate (Rule Extraction/Decision\nTrees) + Visualisation\nStatic logs G Good scalability Post-hoc explanations can be\nmisleading\n[170] A F Approximate (Rule Extraction/Decision\nTrees)\nStatic TN L Applicable to any AD; Good\nscalability\nPoor explanation fidelity\n[54] S F Approximate (Rule Extraction) Static and streaming TM L Able to handle streaming data Only applicable to tree-based\nensembles\n[18] A F Approximate (Rule Extraction) Static TM L &\nG\nEasy to evaluate the explanation\nquality\nOnly considers OCSVM\n[17] A F Approximate ( XGBoost/LightGBM +\nLIME/SHAP; ElasticNet/EBM)\nStatic MTS G Able to integrate domain knowledge Not suitable for explaining only one\nanomalous point\n(Continued)\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "9301bd08-a990-4878-878a-fd3f9d8c26d5": "A Survey on Explainable Anomaly Detection 23:33\nTable 3. Continued\nRef Spec Pers Tech Data Loc Pros Cons\n[102] A F Approximate (SVM/XGBoost + LIME) +\nVisualisation\nStatic UTS G Applicable to any AD; End-user\ndependent explanations\nPoor explanation fidelity\n[15] S P Others (Pattern comparison) Streaming ES L Able to handle streaming data Only applicable to LFA-like AD\n[113] A P Others (Pattern comparison) Static ES L Generalisable Needs many anomalies\n[176] S F Visualization Static ES L Provides visual explanations Only applicable to distance based AD\nmethods\n[123] A F Others (Neighbours + Data\nAugmentation + Classification + Feature\nContribution)\nStatic TN L Incorporates prior knowledge;\nApplicable to various anomaly\ndetectors\nPoor explanation fidelity; Important\nparameters to set by user; Only\nconsiders individual anomalies\n[193] S F Others (Separation-Based) Static TN L Provides quantitative evaluation of\nexplanation quality\nOnly applicable to density-based AD\n[95] S F Others (Isolation based feature\nimportance)\nStatic TM L High explanation fidelity Only applicable to Isolation Forest\n[32] S F Others (Isolation based feature\nimportance)\nStatic TM L &\nG\nProvides local and global\nexplanations; High explanation\nfidelity\nOnly applicable to Isolation Forest\n[7] S P Visualisation Static MTS L Interpretable statistical model and\nvisual interpretation\nOnly applicable to ARIMA\n[133] S F Visualization Static TS L Provides visual explanations Only applicable to Spatiotemporal\nmodel\n[209] S F Shapley values of reconstruction errors Static tabular L More reliable compared to\nreconstruction error based\nexplanation\nOnly applicable to PCA\n[164] A F SHAP Static tabular L Applicable to any AD Poor explanation fidelity\n[103] A F SHAP Streaming MTS L Applicable to any AD Poor explanation fidelity\nSpec indicates whether a method is model-agnostic (A) or model-specific (S).Pers specifies whether a method is feature-based (F), sample-based (S) or\npattern-based (P).Techindicates the techniques used in each method.Data represents the type of data to which each method can be applied.Data indicates the\ndata type for which the method is applicable (TN: Tabular Numeric; TC: Tabular Categorical; TM: Tabular Mixed; UTS: Univariate Time Series; MTS:\nMultivariate Time Series; ES: Event Sequence).Loc shows whether a method provides a local explanation (L) or global explanation (G).Pros andCons describe\nthe advantages and disadvantages of each method, respectively.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "99c1729c-d025-4504-a2ae-775bf01fc9ea": "23:34 Z. Li et al.\n7.1 Explaining AutoEncoders\nAn AE is a type of neural network that first encodes the given data instances into some low-\ndimensional feature representation space and then decodes them back under the constraint\nof minimizing the reconstruction error. Several types of AEs have been introduced, including\nvanilla AE such as replicator neural network,Sparse AutoEncoders(SAE), DAE,Contractive\nAutoEncoders(CAE), VAE, and other variants [16]. AEs are widely used for anomaly detection,\nbased on the assumption that anomalies are more difficult to reconstruct from the compressed\nfeature representation space than normal instances.\nFirst of all, Shapley values-based techniques such as SHAP are typically used to obtain feature\ncontributionsforexplainingAEs.Forinstance,Giurgiu&Schumann[ 72]extendSHAPtoexplain\nanomalies identified via a GRU-based AutoEncoder in multivariate time series data. Specifically,\nthey modify kernel SHAP [128] to output the windows that contribute the most to the anomaly\nand also the windows that counteract the most to the anomaly as explanations. Besides, to detect\nand explain anomalies in mobileRadio Access Network(RAN) data, Chawla et al. [40] set up\na Sparse SAE-based anomaly detection algorithm and then applies kernel SHAP to explain the\nresults.Furthermore,Jakubowskietal.[ 91]proposeaVAEmodelcombinedwithShapleyvaluesto\ndetectandinterpretanomaliesinanassetdegradationprocess.Concretely,theycomputeShapley\nvalues to generate both local and global explanations for anomalies. Additionally, Serradilla et al.\n[192] utilise different machine learning approaches to detect, predict and explain anomalies in\npressmachinestoachievepredictablemaintenance.TointerpretananomalydetectedbyAE,they\nfirst leverage t-SNE [214] to visualise the learned latent feature spaces. Next, they employ the\nGradientExplainertool[ 127],whichcombinesSHAP,IntegratedGradients[ 206],andSmoothGrad\n[201], to analyze which input features are associated with the anomaly.\nSecond,manymethodsattempttotrackreconstructionerrorstoobtainfeaturecontributionby\nexploringtheinternalstructureofAEs.Therefore,thesemethodsaregenerallymodel-specific.For\ninstance, Ikeda et al. [88]d e s i g naMultimodal AutoEncoder(MAE) model to detect anomalies\nemerging in ICT systems. More importantly, by using sparse optimization, they also propose\nan algorithm to estimate the contributing dimensions in an AE to anomalies as explanations.\nBesides, Nguyen et al. [151] introduce a framework called GEE to detect and explain anomalies\nin network traffic. Specifically, they train a VAE model on a normal dataset to learn the normal\nbehaviour of a network, and then employ gradient-based fingerprinting technique to identify the\nmain features causing the anomaly. Similarly, Memarzadeh et al. [140] propose a DGM-based on\nVAE. Particularly, they achieve model interpretability by evaluating feature importance through\nthe random-permutation method. Additionally, Chen et al. [45] put forward DAEMON, which\ntrains an Adversarial AutoEncoder (AAE) to learn the typical pattern of multivariate time\nseries, and then use the reconstruction error to identify and explain anomalies. Meanwhile, to\nmonitor wireless spectrum and identify unexpected behaviour, Rajendran et al. [171]p r e s e n ta n\nAAE-based anomaly detection method named SAIFE. Since the AAE is trained in three phases,\nviz. reconstruction, regularization, and semi-supervised [130], SAIFE attempts to localize the\nanomalous regions based on the reconstruction errors coupled with the semi-supervised features,\nproviding explanations for the anomalies. Furthermore, Ikeda et al. [89] set up an anomaly\ndetection model based on VAE, and then estimate the features that contribute the most to the\nidentified anomalies as explanations. Concretely, they present an approximative probabilistic\nmodel based on the trained VAE to estimate contributing features via exploring the so-called\ntrue latent distribution.T h etrue latent distributiondefines how an anomalous instance would be\nif it were normal.",
        "b3787f0a-38c3-4e4a-a0f9-a94a02e1ffa8": "Meanwhile, to\nmonitor wireless spectrum and identify unexpected behaviour, Rajendran et al. [171]p r e s e n ta n\nAAE-based anomaly detection method named SAIFE. Since the AAE is trained in three phases,\nviz. reconstruction, regularization, and semi-supervised [130], SAIFE attempts to localize the\nanomalous regions based on the reconstruction errors coupled with the semi-supervised features,\nproviding explanations for the anomalies. Furthermore, Ikeda et al. [89] set up an anomaly\ndetection model based on VAE, and then estimate the features that contribute the most to the\nidentified anomalies as explanations. Concretely, they present an approximative probabilistic\nmodel based on the trained VAE to estimate contributing features via exploring the so-called\ntrue latent distribution.T h etrue latent distributiondefines how an anomalous instance would be\nif it were normal. Importantly, they argue that directly estimating feature contribution based on\nthe deviating latent distribution or reconstruction errors will lead to high false positives and/or\nnegatives.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "433496c2-2c99-47ce-a6b3-f04902d07f81": "A Survey on Explainable Anomaly Detection 23:35\nThird, some researchers attempt to utilise surrogate models such as LIME and rule learners to\nexplain AEs. For example, Wu & Wang [220] propose a neural network-based model incorporat-\ning LIME techniques to detect and interpret fraudulent credit card transactions. Specifically, the\nanomalydetectionmodelcontainsanAEandanMLPclassifier,whicharetrainedinanadversarial\nmanner.Tointerpretananomaly,theyapplythreeindependentLIME-basedmodelstoexplainthe\nAE,MLP,andAE&MLPmodels,respectively.Besides,Songetal.[ 203]developtheEXADsystem\nto identify and interpret anomalies from Apache Spark traces. First, the EXAD system adapts AE\nand LSTM to perform anomaly detection. Second, they propose three ways to explain anomalies.\nThe first one is to build a conjunction of the atomic predicates, which can be solved by a greedy\nalgorithm but cannot guarantee the performance. To overcome this limitation, the second one\nattempts to use an entropy-based reward function to build atomic predicates. Furthermore, they\npresent these constructed predicates in a Conjunctive Normal Form. The third one is to approx-\nimate the anomaly detection neural networks using a decision tree. From the decision tree, they\ngenerate explanations in a DNF. Additionally, De Moura et al. [56] present theLane Change De-\ntector(LCD)modeltodetectandexplainwhenthesurroundingvehiclesofanegovehiclechange\ntheir lanes.Specifically,the LCD model consistsofthreeindependentAE models trainedon three\ndifferent datasets. On this basis, they set up a decision rule set based model by extracting rules\nfrom the reconstruction errors produced by these three separate models, to determine when an\nanomaly happens. Besides, Gnoss et al. [73] first annotate journal entries with previously trained\nAutoEncodersandthentrainthreeXAImodelsusingtheseannotations.First,theyutiliseDecision\nTreeandLinearRegression,twointrinsicallyinterpretablemodels,tosimulateAE.Thefeatureim-\nportance values of Decision Tree and the odd ratio values are calculated to show which feature is\nrelevant to the anomalies. Additionally, they also leverage SHAP to explain the AE model.\nFourth, visualisation techniques such as Heatmaps and Saliency Maps are often constructed to\nhelpexplainAEs.Forinstance,Kitamura&Nonaka[ 104]setupanencoder-decoder-basedmodel\nto detect anomalies in images. To generate explanations for an anomaly, they first develop a fea-\nture extractor that is trained on a dataset consisting of normal images and their corresponding\nreconstructed images. Second, using this feature extractor to extract latent features, their method\nattemptstofindthedifferenceinthefeature-levelbetweentheinputimageandthereconstructed\nimage. On this basis, their method localizes and visualizes abnormal regions as explanations for\nthe anomaly. Besides, Feng et al. [68] develop a Two-Stream AE-based model to detect abnormal\nevents in videos and then utilise a Feature Map Visualization method to interpret the anomalies.\nMoreover, Guo et al. [77] set up a Sequence-to-Sequence VAE-based model to detect anomalies in\neventsequences.Torevealanomalousevents,theyinvestigatethedifferencesbetweentheanoma-\nlous sequence together with its reconstructed sequence and a set of normal sequences close to\nthe anomalous sequence in the latent space. Importantly, they build a visualization tool to facil-\nitate the comparisons. In addition, Szymanowicz et al. [208] develop a method for detecting and\nautomatically explaining anomalous events in video. They first design an encoder-decoder archi-\ntecturebasedonU-Net[ 177]todetectanomalies,therebygeneratingsaliencymapsbycomputing\nper-pixeldifferencesbetweenactualandpredictedframes.Second,basedontheper-pixelsquared\nerrorsinthesaliencymaps,theyintroduceanexplanationmodulethatcanprovidespatiallocation\nand human-understandable representation of the identified anomalous event.\nFinally,awiderangeofmethodssuchasfeatureselection,MarkovChainMonteCarlo,andpro-\nviding similar historic anomalies, are also explored to facilitate the interpretability of AE-based\nanomaly detection.",
        "54918677-3e43-4ce4-9c7c-a79771f999e2": "Importantly, they build a visualization tool to facil-\nitate the comparisons. In addition, Szymanowicz et al. [208] develop a method for detecting and\nautomatically explaining anomalous events in video. They first design an encoder-decoder archi-\ntecturebasedonU-Net[ 177]todetectanomalies,therebygeneratingsaliencymapsbycomputing\nper-pixeldifferencesbetweenactualandpredictedframes.Second,basedontheper-pixelsquared\nerrorsinthesaliencymaps,theyintroduceanexplanationmodulethatcanprovidespatiallocation\nand human-understandable representation of the identified anomalous event.\nFinally,awiderangeofmethodssuchasfeatureselection,MarkovChainMonteCarlo,andpro-\nviding similar historic anomalies, are also explored to facilitate the interpretability of AE-based\nanomaly detection. For example, Chakraborttii & Litz [35] develop an AE-based model to detect\nSolid-State Drive(SSD) failures. To produce explanations, they investigate the reconstruction\nerror per feature, wherein a feature with a reconstruction error greater than the average error is\nconsideredasignificantcause.Particularly,theyapplythreetypesoffeatureselectiontechniques,\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "9c65a62d-bd3f-4e0b-9283-7cf949784f59": "23:36 Z. Li et al.\nviz. Filter, Wrapper and Embedded, to select important features to train the AE model, facilitat-\ning the interpretability of resulted anomaly detection model. Besides, Li et al. [115]d e v e l o pa\nVAE andgenetic algorithm(GA)-based framework, called VAGA, to detect anomalies in high-\ndimensional data and search corresponding abnormal subspaces. Concretely, for each identified\nanomaly,theyutilizeaGAtosearchthesubspacewheretheanomalydeviatesmost.Additionally,\nLi et al. [117] introduceInterFusion, a model based onhierarchical Variational AutoEncoder\n(HVAE)a n dMarkov Chain Monte Carlo(MCMC) for detecting and explaining anomalies in\nmultivariatetimeseriesdata.Specifically,givenananomaly,theysetupaMCMC-basedmethodto\nfind a set of the most anomalous metrics as explanations. Furthermore, Assaf et al. [14]d e v e l o pa\nConvolutional AutoEncoders(ConvAE)-based anomaly detection method and an explainabil-\nity framework to detect and explain anomalies in data storage systems, respectively. Particularly,\nfor each anomaly, they attempt to use cosine similarity over the embedding space to find similar\nhistorical anomalies, thereby explaining the anomaly through association.\nDiscussion: AEs are the most widely used deep learning method to detect anomalies in tabular\ndata, sequence data, image data, video data and graph data. As a result, a plethora of methods are\nalsoproposedtoexplainAEs.Concretely,XADtechniquessuchasreconstructionerror-basedfea-\nturecontribution,KernelSHAP,GradientExplainer,LIME,ruleextractionandfeaturemapvisuali-\nsationareoftenleveragedtoobtainexplanations.Importantly,mostoftheseexplanationmethods\nonly provide weak interpretability, as they only explain a single anomaly at a time by exploring\nsome important properties of AE-based detection models.\n7.2 Explaining Recurrent Neural Networks\nA Recurrent Neural Network(RNN) is a specific type of neural network that is capable of\nlearning features and long-term dependencies in sequential data [183]. Specifically, sequential\ndatarefersto anydatathatis orderedintosequences,includingtime series,text streams,DNAse-\nquences,audioclips,videoclips,andsoon.Toaddressthedifferentchallengesofmodellingsequen-\ntial data, various RNN architectures have been proposed. More concretely, frequently used RNNs\ninclude deep RNNs with MLP,Bidirectional RNN(BiRNN), Recurrent Convolutional Neu-\nral Networks(RCNN), Multi-Dimensional Recurrent Neural Networks(MDRNN), LSTM,\nGatedRecurrentUnit (GRU),MemoryNetworks, StructurallyConstrainedRecurrentNeu-\nralNetwork (SCRNN),Unitary RecurrentNeural Networks(Unitary RNN),andso on.Par-\nticularly,byassumingnormalinstancesaretemporallymorepredictablethananomalousinstances,\nRNNsareextensivelyusedtoidentifyanomaliesinsequentialdatabecauseoftheirabilitytomodel\ntemporal dependencies.\nFirstofall,Shapleyvalues-basedtechniquessuchasSHAParethemosttypicalmethodusedto\nobtain feature contributions, aiming to explain anomalies identified by RNNs. For instance, Zou\nand Petrosian [210] utilise Decision Trees [42] and DeepLog [63] to detect anomalies in system\nlogs, and then explain the results using the Shapley value approach. To explain an anomaly, they\ntreateacheventinthelogsasaplayerwithoutexaminingthemodelstructuretogenerateShapley\nvalues. Moreover, Hwang & Lee [87] propose a bidirectional stackable LSTM-based anomaly de-\ntection model for industrial control system anomaly detection. For each identified anomaly, they\nemploy SHAP values to obtain a contribution score of each feature as an explanation. Similarly,\nJakubowski et al. [92] examine the issue of anomaly detection when hot rolling slabs into coils.\nThey utilise LSTM to construct a modified AutoEncoder architecture in order to find anomalies.\nImportantly,theyare able to pinpointthe origin of the majority ofthe abnormalities identifiedby\nthedeeplearningmodelthroughanalysisoftheSHAPinterpretation.Furthermore,Noretal.",
        "a0928b60-400d-42b5-b536-f9216a3e4a42": "To explain an anomaly, they\ntreateacheventinthelogsasaplayerwithoutexaminingthemodelstructuretogenerateShapley\nvalues. Moreover, Hwang & Lee [87] propose a bidirectional stackable LSTM-based anomaly de-\ntection model for industrial control system anomaly detection. For each identified anomaly, they\nemploy SHAP values to obtain a contribution score of each feature as an explanation. Similarly,\nJakubowski et al. [92] examine the issue of anomaly detection when hot rolling slabs into coils.\nThey utilise LSTM to construct a modified AutoEncoder architecture in order to find anomalies.\nImportantly,theyare able to pinpointthe origin of the majority ofthe abnormalities identifiedby\nthedeeplearningmodelthroughanalysisoftheSHAPinterpretation.Furthermore,Noretal.[ 152]\npresentaprobabilisticLSTM-basedmodelcombinedwithSHAPtodetectandinterpretanomalies\nin gas turbines. More importantly, they evaluate the quality of post-hoc explanations from two\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "7e876800-dbfe-496a-a9d0-7b07a076c44f": "A Survey on Explainable Anomaly Detection 23:37\naspects, viz.local accuracyand consistency. Specifically,local accuracydescribes the relationship\nbetween feature contributions and predictions, whileconsistency checks whether the interpreta-\ntion is consistent with changes in the input features.\nSecond,someresearchersattempttoutilisesurrogatemodelssuchasLIMEtoexplainRNN.For\nexample, Herskind Sejr et al. [84] create a predictive neural network-based unsupervised system\nby training an LSTM model and using reconstruction errors to assess data abnormalities. Impor-\ntantly, the system offers two layers of anomaly interpretation: deviations from model predictions,\nand interpretations of model predictions, in order to make the process transparent to developers\nandusers.TheyemployMeanAbsoluteErrortoillustratehowobservationsdivergefromassump-\ntions at the first level. For the second level, they simulate a black-box model to provide an expla-\nnation using LIME. Additionally, Mathonsi & van Zyl [136]p r e s e n tMultivariate Exponential\nSmoothing Long Short-Term Memory(MES-LSTM) that combines statistics and deep learn-\ning. Particularly, they integrate SHAP and LIME and introduce a metric\u2014called Mean Discovery\nScore\u2014that aims at showing which predictors are most strongly associated with the anomalies.\nThird, other methods such asLayer-wise Relevance Propagation(LRP), Integrated Gradi-\nents, and Attention Mechanism, are also leveraged to explain RNN-based anomaly detection. For\ninstance, due to the complexity of log systems and the unstructured nature of the resulting logs,\nPatiletal.[ 166]useLSTMtodetectanomaliesinsuchsystems.Togenerateexplanationsforeach\nidentifiedanomaly,theyutiliseLRPtogeneraterelevancescoresforeveryfeatureateverytimestep.\nMoreover, Han et al. [80]p r e s e n tInterpretableSAD, a Negative Sampling based method for de-\ntecting and interpreting anomalies in sequential log data. First, due to the scarcity of anomalous\ninstances, they adapt a data augmentation strategy via negative sampling to generate a dataset\nthat contains sufficient anomalous samples. Second, they train an LSTM model based on this aug-\nmentedlabelleddataset.Third,theyapplyIntegratedGradientstoidentifyanomalouseventsthat\nlead to the outlyingness. Furthermore, to detect anomalies in system logs, Brown et al. [28]i m -\nplementfourattentionmechanismsinLSTMandprovethatcomparedtoBidirectionalLSTM,the\nattention mechanism augmented LSTM not only retains high performance but also provides in-\nformation about feature importance and relationship mapping between features, which provides\nexplainability.\nDiscussion: RNNs are primarily employed to detect anomalies in sequence data. Typical XAD\ntechniques for interpreting anomalies identified by RNN-based models include Shapley-value-\nbased methods, surrogate models, and other versatile techniques such as LRP, Integrated Gradi-\nents,andAttentionMechanism.Thesepost-hocexplanationmethodsareusuallycomputationally\nexpensive, making it difficult to provide real-time explanations.\n7.3 Explaining Convolutional Neural Networks\nA CNN is a specific type of neural network inspired by the visual cortex of animals. CNNs are\nwidely used in the computer vision field because of their strong ability to extract features from\nimage data with convolution structures. Moreover, CNNs have also been shown to be useful for\nextractingcomplexhiddenfeaturesinsequentialdata[ 74].Accordingly,avarietyofCNNarchitec-\ntures have been proposed, including LeNet, AlexNet, GoogleNet, VGGNet, Inception V4, ResNet,\nand so on. Some studies have attempted to utilize CNNs for anomaly detection, especially in the\nfields of intrusion detection, image anomaly detection, and so on.\nFirst, one line of research attempts to utilise surrogate models such as LIME and rule learners\nto explain CNN. For example, Cheong et al. [47] set up aSpatioTemporal Convolutional Neu-\nralNetwork\u2013basedRelationalNetwork (STCNN-RN)todetectanomalouseventsinfinancial\nmarkets. For each anomaly, they apply LIME to provide a local explanation by indicating the con-\ntributionofeachfeature.Besides,Levyetal.[ 114]proposeanend-to-endanomalydetectionmodel\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "9ff61e16-98ac-4759-a25a-beaf123efd1c": "23:38 Z. Li et al.\nnamed AnoMili, which can also provide real-time explanations. Specifically, AnoMili consists of\nfourstages.First,theyintroduceaphysicalintrusiondetectionmechanismbyusingAE.Second,if\nnoanomalousdeviceisdiscovered,theytrainaCNN-basedclassifieronvoltagesignalsofeachde-\nvice,aimingtodetectspoofingattacks.Third,theyutiliseLSTMtobuildacontext-basedanomaly\ndetection mechanism, which detects anomalous messages basedon their context. Finally, to inter-\npretananomalousmessage,theyleveragedecisiontreetolocallyapproximatethedetectionresult\nand also apply SHAP TreeExplainer [128] to identify the most important features in real-time.\nSecond, visualisation techniques are often combined with other techniques such as Gradient\nBackpropagationandLRPtoexplainCNN-basedanomalydetection.Forinstance,Saekietal.[ 182]\npresent a CNN-based method to detect and explain machinery faults based on vibration data. For\neach detected anomaly, they utilize grad-CAM [190], which is a gradient-based localization ap-\nproach, to obtain an importance map in the feature space. Fourth, they combine the results of\ngrad-CAM with a visualization approach called Guided Backpropagation [204]. Concretely, this\nmethod can visualize the predictions via backpropagation from the output space to the input\nspace, generating explanations for the anomaly. Moreover, Chong et al. [48] introduce a CNN-\nbasedTeacher\u2013StudentNetwork-basedmodelcombinedwithLRPtechniquetodetectandexplain\nanomalies. To interpret an anomaly, they provide an example-based explanation by showing its\ntopprototypes(namely,topnearestneighbours).Importantly,theyapplyLRPtoshowapixel-level\nsimilarity between the anomaly and each of its top prototypes. Additionally, Szymanowicz et al.\n[207] introduce a model to detect and explain anomalies in videos. Specifically, they implement\nR-CNNtodetectobjectsinvideo,andthenemployDualRelationGraphforhuman-objectinterac-\ntion recognition. The video is encoded with a collection ofhuman-object interaction vectors\n(HOI vectors) for each frame. When the likelihood of the HOI vector in a scenario is less than a\nthreshold, an anomaly is proclaimed. After using PCA to reduce the dimension of non-anomalies,\nthey train aGaussian Mixture Model(GMM). A video frame is deemed abnormal if any of its\nHOI vectors are lower than the threshold probability under the GMM. The distance between the\nanomalousHOIvectorandtheusualHOIvectoristhenweightedandvisualizedasa2Dheatmap\nto help understand abnormalities.\nThird,someresearchersattempttodirectlyutilisethesemanticanomalyscoresasexplanations.\nFor instance, Hinami et al. [85] utilise a general CNN model and context-sensitive anomaly detec-\ntorstoidentifyandexplainabnormaleventsinfilms.Specifically,theysetupaFastR-CNN-based\nmodel to learn multiple concepts in videos and then extract semantic features. On this basis, they\napplyacontext-sensitiveanomalydetectortoobtainsemanticanomalyscores,whichcanbeseen\nas explanations for anomalies.\nDiscussion: CNN-based anomaly detection models are mainly leveraged to detect anomalies in\nimage data. To explain anomalies identified by CNNs, XAD techniques such as surrogate models\n(LIME and rule learners), Gradient Backpropagation, LRP, and visualisations are commonly used.\nHowever,somepost-hocexplanationmethods,especiallysurrogatemodels,maysufferfrompoor\nexplanationfidelity.Inotherwords,thegeneratedexplanationsmaynotreflecttheactualanomaly\ndetection process of CNNs.\n7.4 Explaining Other Deep Neural Networks\nIn addition to AEs, RNNs and CNNs, other DNNs\u2014such as GANs, Deep OCSVM, andDeviation\nNetwork(DevNet)\u2014canalsobeusedforanomalydetection.Therefore,theinterpretationofthese\ntypes of networks is also relevant.\nFirst, some studies propose explanation methods for general DNNs. For instance, Amarasinghe\net al. [8] propose a framework for explainable DNN-based anomaly detection.",
        "d17cbaf5-d0fb-42ec-9f84-7eb368727f1b": "However,somepost-hocexplanationmethods,especiallysurrogatemodels,maysufferfrompoor\nexplanationfidelity.Inotherwords,thegeneratedexplanationsmaynotreflecttheactualanomaly\ndetection process of CNNs.\n7.4 Explaining Other Deep Neural Networks\nIn addition to AEs, RNNs and CNNs, other DNNs\u2014such as GANs, Deep OCSVM, andDeviation\nNetwork(DevNet)\u2014canalsobeusedforanomalydetection.Therefore,theinterpretationofthese\ntypes of networks is also relevant.\nFirst, some studies propose explanation methods for general DNNs. For instance, Amarasinghe\net al. [8] propose a framework for explainable DNN-based anomaly detection. Specifically,\nthey assume the anomaly detection is performed in a supervised setting and leverage LRP\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "db112105-1dc6-440d-8f4b-c7a1518056ab": "A Survey on Explainable Anomaly Detection 23:39\nto obtain the input feature relevance for making a decision. Besides, Sipple [196] trains an\nanomaly detector using Neural Network with negative sampling to detect device failures in the\nInternet of Things. For each identified anomaly, they leverage Integrated Gradients techniques to\nattribute the anomaly score to each feature and provide a contrastive nearest normal instance as\nexplanations.\nSecond, some researchers utilise techniques such as self-attention learning-based feature selec-\ntion or gradient back propagation-based feature contribution to explain a DevNet. For instance,\nXu et al. [221]p r o p o s eAttention-guided Triplet deviation network for Outlier interpre-\ntatioN (ATON) to explain anomalies in a post-hoc fashion. Specifically, ATON is composed of\ntwo main modules, viz. the feature embedding module and the customized self-attention learning\nmodule. The feature embedding module transforms the original feature space into an embedding\nspace with extended high-level information. Meanwhile, given an anomaly, the customized self-\nattention learning module can obtain the contribution of each learned feature to its separability.\nBased on the embedding module and the corresponding attention coefficients, they distil a subset\nof the original features that lead to the separability of the anomalous instance. Meanwhile, Pang\netal.[ 160]putforwardFASD,aweakly-supervisedframeworktodetectanomalieswhenafewla-\nbeledanomaliesofinterestareavailable.Specifically,theyinstantiatethisframeworkasaDevNet\nmodel, which assumes that the anomaly scores of normal instances are drawn from a Gaussian\nprior distribution and the anomaly scores of anomalies come from the upper tail of the prior. To\ninterpret an anomaly, they compute the contribution of each input feature to the final anomaly\nscore through gradient-based back propagation.\nThird, deep Taylor decomposition [144] is leveraged to explain models such as OCSVM, KDE,\nandsoon.Forexample,Kauffmannetal.[ 96]firstconverttheOCSVMmodelstoneuralnetworks,\nand then they modify the deep Taylor decomposition method to be applicable to these neural\nnetworks. In addition, they show its superiority to other explanation methods such as Distance\nDecomposition,Gradient-BasedMethod,SHAPValues,andEdgeDetection,whicharecommonly\nused in deep learning to produce pixel-wise explanations of decisions. However, this method\nitself has many parameters to tune when applied to different methods or datasets, sometimes\nrendering the explanation method itself not explainable. Moreover, it also makes many strong\nassumptions and approximations. Similarly, Kauffmann et al. [97] reveal the widespread occur-\nrence of Clever Hans phenomena in unsupervised anomaly detection models. Concretely, they\npropose an XAI procedure based on Deep Taylor Decomposition to highlight relevant features\nfor detecting anomalies and apply it on models including AutoEncoder reconstruction-based\ndetectors, Deep One-Class and KDE-based detectors, generating pixel-wise explanations of\noutlyingness.\nFinally, visualisation techniques can be leveraged to help explain anomalies. For instance, Liu\net al. [125] create the deep temporal clustering framework seq2cluster, which can cluster and\ndetectanomaliesintimeserieswithvaryinglengths.TheTemporalSegmentation,TemporalCom-\npressionnetwork,andGMMEstimationmodulesmakeupseq2cluster.Inparticular,eachsequence\nis divided into non-overlapping temporal segments via the Temporal Segmentation module. A\nlow-dimensionalrepresentationofeachtimesegmentiswhattheTemporalCompressionnetwork\naimsatachieving.Moreover,theEstimationNetworkforGMMsutilisesthelatentspacerepresen-\ntation to perform density estimation. Therefore, data instances can be clustered in latent space to\nfind anomalies based on the likelihood of each segment sample. The results of anomaly detection\ncan also be more easily interpreted when anomalies found in the latent space are adequately\nvisualized.\nDiscussion: In addition to the above-mentioned DNNs, namely, AEs, RNNs, CNNs, GANs, Deep\nOCSVM, and DevNet, other DNNs such as Graph Neural Networks [39] and Transformers [119]\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "79cef667-a8be-41e6-9c12-8b702660af8a": "23:40 Z. Li et al.\nhavebecomeprevalentinanomalydetection.Therefore,theinterpretationmethodsoftheseDNNs\nare also relevant.\n7.5 Summary\nTo wrap up our review of post-model XAD techniques for DNNs, Table4gives an overview of all\ntechniques discussed and we have several high-level observations.\nFirst, most deep post-model XAD techniques are model-specific in the sense that they are only\napplicable to a family of specific neural networks or all neural networks. This is in stark con-\ntrast with most shallow post-model XAD techniques, which are typically model-agnostic. This is\nbecause these deep post-model XAD techniques provide explanations by exploring the internal\nstructure of the neural network. By doing so, although these explanation methods cannot be gen-\neralized to other anomaly detection models, the resulting explanations are usually faithful as the\nExplanation-Definition is in compliance with theDetection-Definition. However, techniques such\nasSHAP,LIME,andsomerulelearnersaremodel-agnosticandare,therefore,morelikelytosuffer\nfrom poor fidelity.\nSecond, nearly all deep post-model XAD techniques provide only feature-based explanations;\nthe only exceptions are References [14, 48, 72], which also produce sample-based explanations.\nRegardingthetechniquesused,theShapleyvalues-basedapproachisthemostpopularone.More\nimportantly,onecanseethatmostdeeppost-modelXADtechniquesareproposedtoexplainanom-\nalies detected in sequential data such as time series and system logs.\nThird,nearlyalldeeppost-modelXADtechniquescanonlyprovidelocalexplanations.Inother\nwords,theycanonlyexplainasingleanomalyatatime.Duetothecomplexityofneuralnetworks,\nit is extremely challenging, if possible, to understand the entire decision-making process. To help\nend-usersunderstandwhyaninstanceisreportedasanomalous,deeppost-modelXADtechniques\noften inspect some important properties of the neural networks, such as feature contribution to\nreconstruction errors, thereby providing weak interpretability.\n8 CONCLUSION AND FUTURE OPPORTUNITIES\nWe reviewed more than 150 articles that harness XAD techniques to explain anomalies. Specifi-\ncally,wefirstintroducedthreedifferentdefinitionsofanomalyandthenclarifiedwhatXADisand\nwhyitisneeded.Onthisbasis,andinspiredbyexistingsurveysonXAI,weproposedataxonomy\nconsisting of six main criteria, enabling the categorization of the increasingly rich field of XAD.\nForpurposesofbrevityandorganisation,westructuredthereviewintofourhigh-levelcategories\n(correspondingtoSectionsS4\u20137)andtwelvefine-grainedcategories(correspondingtothesubsec-\ntions of S4\u20137). Throughout the survey, we identified a number of research challenges that may\noffer opportunities for future research, which we will summarize next.\n8.1 Definition of Anomaly and XAD\nA long-standing problem in anomaly analysis is the lack of a uniform definition of an anomaly,\nleading to a wide range of anomaly detection methods. The diversity of anomaly definitions and\nanomaly detection methods leads to the need for a large variety of anomaly explanation methods.\nAlthough is not necessarily problematic in itself (and maybe unavoidable), the lack of uniform\ndefinitions for anomaly detection and XAD hampers communication of researchers between dif-\nferent (sub)fields, such as computer vision, natural language processing, data mining, and social\nscience. This makes it hard to find related work and leads to the re-invention of methods, caus-\ningunnecessarydelaysinscientificprogress.Moreimportantly,theevaluationandcomparisonof\nXADmethodsbecomesdifficultandsubjective,duetothelackofauniform,objective,andprecise\ndefinition of XAD.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "0f4ced3b-2694-45b6-83d7-d9d91b03b578": "A Survey on Explainable Anomaly Detection 23:41\nTable 4. Summary of Surveyed Deep Post-model Techniques\nRef. Spec Pers Tech Data Loc Pros Cons\n[88] S (MAE) F Reconstruction error-based feature\ncontribution using sparse\noptimization\nStatic network-flow data L Able to handle cross-domain data Only applicable to AEs\n[72] A (GRU-based\nAE)\nS & P Kernel SHAP based feature\nimportance\nStatic MTS L Applicable to any AD Assumes feature independence in\nKernel SHAP\n[40] A (SAE) F Kernel SHAP based feature\nimportance\nStreaming ES L Applicable to any AD Assumes feature independence in\nKernel SHAP\n[104] S (AEs) F Feature-level reconstruction error\n+ Visualisation\nStatic image L Provides visual explanations Only applicable to AEs\n[35] S (AEs) F Reconstruction error based feature\ncontribution + Feature selection\nStatic telemetry logs L Able to handle evolving data Only applicable to AEs\n[192] S (AEs) P GradientExplainer + Visualization Static MTS L Provides visual explanations Only applicable to AEs\n[68] S (CNN-based\nAE)\nF Feature map visualization Static video L Provides visual explanations Post-hoc explanations may be\nmisleading\n[220] A (AE+MLP) F Approximate (LIME) Static TN L Applicable to any AD Poor explanation fidelity\n[203] A (AE+LSTM) F Approximate (Rule extraction) Streaming ES L Handles streaming data;\nApplicable to any AD\nPost-hoc explanations may not be\nreliable\n[151] S (VAE) F Gradient based feature\ncontribution\nStreaming ES L Handles streaming data Only applicable to reconstruction\nbased DNN\n[77] S (VAE) P Reconstruction error based pattern\ncomparison + Visualization\nStatic ES L Provides interactive visual\nexplanations\nOnly applicable to reconstruction\nbased AD\n[115] A (VAE) F GA based subspace search Static TM L Applicable to any AD High computational cost\n[91] A (VAE) F Shapley values based feature\ncontribution\nStatic MTS L & G Applicable to any AD High computational cost\n[89] S (VAE) F Others (True latent distribution\nbased feature contribution)\nStatic TN L More reliable than reconstruction\nerror-based explanations\nOnly applicable to VAE\n[117] S (VAE) F Others (MCMC-based method) Streaming MTS L Provides real-time explanations Unable to handle evolving data\n[140] A (VAE) F Perturbation based feature\nimportance\nStatic MTS L Applicable to any AD Needs a few labelled data\n[14] S (ConvAE) F & S Similar historic anomaly +\nReconstruction error based feature\ncontribution\nStatic MTS L End-to-end framework Only applicable to Convolutional\nAEs\n[56] S (AEs) F Approximate (Rule extraction from\nreconstruction error)\nStreaming MTS G Handles streaming data Needs many anomalous samples;\nOnly applicable to\nreconstruction-based AD\n[45] S (AAE) F Reconstruction error based feature\ncontribution\nStreaming MTS L Provides quantitative evaluation of\nexplanation quality\nOnly applicable to\nreconstruction-based AD\n(Continued)\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "b811ca39-9ae9-4793-9138-bf94b3a34015": "23:42 Z. Li et al.\nTable 4. Continued\nRef. Spec Pers Tech Data Loc Pros Cons\n[171] S (AAE) F Reconstruction errors coupled\nwith the semi-supervised features\nin AAE\nStatic MTS L Works in both unsupervised and\nsemi-supervised settings\nOnly applicable to AAE\n[208] S (U-Net based\nAEs)\nF Saliency maps +\nHuman-understandable\nrepresentation\nStatic video L Provides visual explanations Only applicable to AEs\n[73] A (AEs) F SHAP + Approximate (Decision\ntree + Linear regression)\nStatic TM L Applicable to any AD Poor explanation fidelity; Manual\nevaluation of explanation quality\n[8] S (DNN) F LRP based feature relevance Any data type L & G Not easy to understand for\nnon-experts\nNeeds labelled data; Only\napplicable to certain DNNs\n[166] S (LSTM) F LRP based feature relevancy Static ES L Not easy to understand for\nnon-experts\nNeeds labelled data; Only\napplicable to certain DNNs\n[210] A (DT + LSTM) F Shapley values based feature\nimportance\nStatic ES L Applicable to any AD Poor explanation fidelity\n[87,\n92,\n152]\nA (LSTM) F SHAP MTS L Applicable to any AD Poor explanation fidelity\n[84] A (LSTM) F Approximate (LIME) Streaming UTS L Applicable to any AD; Handles\nstreaming data\nNot easy to understand for\nnon-experts\n[80] S (LSTM) F Integrated Gradients Static ES L Not needs labelled data Only applicable to DNN; Unable to\nhandle evolving data\n[28] S (LSTM) F Attention mechanism Streaming ES L Handles streaming data Only applicable to RNNs\n[136] A (MES-LSTM) F Approximate (LIME) + SHAP Static MTS L Applicable to any AD Poor explanation fidelity\n[182] S (CNN) F grad-CAM + Visualization Static UTS L Provides visual explanations Only applicable to CNNs\n[47] A (CNN) F Approximate (LIME) Static UTS L Applicable to any AD Poor explanation fidelity\n[48] S (CNN) F & S LRP based feature importance +\nPrototypes explanation +\nVisualisation\nStatic image L Provides visual explanations Hard to set the number of\nprototypes\n[114]A ( A E + C N N +\nLSTM)\nP Approximate (Decision Tree) +\nSHAP TreeExplainer\nStreaming MTS L Applicable to any AD; Provides\nreal-time detection and\nexplanations\nPoor explanation fidelity\n[207] S (R-CNN) F Others (PCA + GMM) +\nVisualisation\nStatic video L Provides visual explanations Unable to handle unseen data in\ntraining phrase\n[85] S (R-CNN) F Others (Semantic Anomaly Score) Static video L Joint abnormal event detection and\nrecounting\nWeak interpretability using only\nsemantic anomaly scores\n[196] S (DNN) F & S Integrated Gradients Streaming TS L Handles streaming data Only applicable to DNN\n[221] A (DevNet) F Others (Self-attention learning\nbased feature selection)\nStatic TM L Applicable to any AD Poor explanation fidelity\n(Continued)\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "003bc32e-9559-41c2-bf7c-ef3418dc3389": "A Survey on Explainable Anomaly Detection 23:43\nTable 4. Continued\nRef. Spec Pers Tech Data Loc Pros Cons\n[160] S (DevNet) F Gradient back propagation based\nfeature contribution\nStatic image L End-to-end training Only applicable to specific DNN\n[96,\n97]\nS (OCSVM) F Approximate (NN) + LRP-type\nBack-propagation based feature\nimportance at pixel-level\nStatic image L Better performance compared to\nothers\nOnly applicable to OCSVM and\npotentially some distance-based\nmethods\n[125] S (DNN) F Attention mechanism + GMM +\nVisualisation\nStatic UTS L Handles time series with varying\nlengths; Provides visual\nexplanations\nOnly applicable to specific DNN\nSpec indicates whether a method is model-agnostic (A) or model-specific (S). If it is model-specific, we also indicate the models to which it applies. However,\nfor completeness, we also indicate the involved DNN framework for model-agnostic techniques.Pers specifies whether a method is feature-based (F),\nsample-based (S) or pattern-based (P).Data indicates the data type for which the method is applicable (TN: Tabular Numeric; TC: Tabular Categorical; TM:\nTabular Mixed; UTS: Univariate Time Series; MTS: Multivariate Time Series; ES: Event Sequence).Loc shows whether a method provides a local explanation\n(L) or global explanation (G).\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "bea90290-45df-4b13-8ae8-f605b1e1cba4": "23:44 Z. Li et al.\n8 . 2 E v a l u a t i o no fX A D\nDespite the clearly stated needs for XAD in various domains, and especially those domains in-\nvolving high-stakes decisions, the question of how XAD techniques should be evaluated remains\nunanswered. Over the past few years, the long-standing problem of measuring and assessing ma-\nchine learning explainability has received certain attention [34]. However, most of these methods\narespecificallydesignedforclassificationorclusteringproblems,andextendingthesemethodsto\nanomaly detection problems is non-trivial.\nParticularly, the fidelity of post-hoc explanations merits close scrutiny when evaluating XAD\ntechniques. Inconsistency between theOracle-Definition and Detection-Definition of an anomaly\nmay lead to the identification of anomalies that are not of interest to the end-users. Moreover, in-\nconsistencybetweenthe Detection-Definition(ifavailable) andExplanation-Definitionof an anom-\nalymayleadtopoorexplanationfidelity.Inotherwords,theexplanationsdonotreflecttheactual\ndecision-makingprocessofananomalydetectionmodel.Ingeneral,pre-modelandin-modelXAD\ntechniquesdonotsufferfromthisproblem,whilemostpost-hocXADtechniques\u2014thatonlycorre-\nlateinputswithoutputs,withoutexploringtheinternalstructureofdetectionmodels\u2014areafflicted\nwith this problem. For this reason, some researchers [180] appeal not to use opaque models and\nthen explain them in a post-hoc manner for high-stakes decisions. Instead, an intrinsically ex-\nplainable model should be used. We emphasize that the same argument also applies to anomaly\ndetection and explanation.\n8.3 XAD with Prior Knowledge\nMost XAD techniques attempt to provide explanations solely based on information contained in\nexistingdatainstances(anomalousornot)and/oranomalydetectionmodels.However,sometimes\nadditional knowledge about data instances or anomaly detection models may be acquired, in the\nform of algebraic equations, simulation results, logic rules, knowledge graphs, human feedback,\nandsoon.Importantly,thispriorknowledgecanbeintegratedtoaugmenttrainingdata,choosea\nnetwork architecture, initialize parameters or validate model outputs. This paradigm of integrat-\ning prior knowledge into machine learning is calledInformed Machine Learning[218], which has\nreceivedincreasingattentionoverthepastfewyears.Particularly,Beckhetal.[ 19]haveperformed\nasurveyonmethodsthatintegratepriorknowledgeintomachinelearningforimprovingexplain-\nability, wherein they subdivide these methods into three categories, including the integration of\nknowledgeintomachinelearningproblemssuchasclassification,regression,clustering,oranom-\nalydetection,theintegrationofknowledgeintoexplanationmethod,andderivingknowledgefrom\ntheexplanationresultsandthenintegratingitintothemachinelearningpipeline.Therefore,repur-\nposingthesemethodsforanomalydetectionisundoubtedlybeneficialtoimproveinterpretability,\nand thus is a promising future direction.\n8.4 Adversarial Attacks in XAD\nBelle&Papantonis[ 20]pointoutthatsomewidelyusedXAItechniquesarevulnerabletoadversar-\nialattacks.Inparticular,post-hocXAItechniquessuchasLIMEandSHAPareeasilymanipulated\n[198]. From the reviewed results, we can see that most of these methods in XAI have been repur-\nposed for XAD. As a result, anomaly explanations obtained by using these techniques have the\npossibilityofbeingmanipulatedorattacked.Tocircumventthisproblem,attentionshouldbepaid\nwhen selecting an XAD method. In particular, more efforts should be made to develop interpreta-\ntion methods that take into account adversarial attacks in the future.\n8.5 Scalability of XAD\nLastbutnotleast,thescalabilityofXADplaysanimportantroleinsomeapplications.Forexample,\nlargeinternetcompaniesoftendevelopananomalydetectionsystemtomonitoralargenumberof\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "7ef56596-0b89-40ec-a5e2-5eff2bc1307b": "A Survey on Explainable Anomaly Detection 23:45\nkey performance indicators, aiming to ensure the reliability of their service platform [118]. How-\never, after identifying anomalies, they have to find the root causes and then take remedial actions\nas soon as possible. To achieve this in an automated manner, an anomaly interpretation method\nthatcanprocesslargeamountsofdataandprovidenearreal-timeinterpretationisrequired.How-\never,mostexistingXADtechniques\u2014suchassubspaceanomalydetectionandShapleyvalue-based\nmethods\u2014haveahighcomputationalcost.Therefore,thedevelopmentofscalableXADtechniques\n(with high fidelity) is an important direction for future research.\nREFERENCES\n[1] Charu C. Aggarwal. 2015. Outlier analysis. InProceedings of the Data Mining. Springer, 237\u2013263.\n[2] RakeshAgrawalandRamakrishnanSrikant.1994.Fastalgorithmsforminingassociationrules.In Proceedingsofthe\n20th International Conference on Very Large Data Bases, VLDB, Vol. 1215. Santiago, Chile, 487\u2013499.\n[3] Shikha Agrawal and Jitendra Agrawal. 2015. Survey on anomaly detection using data mining techniques.Procedia\nComputer Science60 (2015), 708\u2013713.\n[4] Diana Laura Aguilar, Miguel Angel Medina Perez, Octavio Loyola-Gonzalez, Kim-Kwang Raymond Choo, and\nEdoardo Bucheli-Susarrey. 2023. Towards an interpretable autoencoder: A decision tree-based autoencoder and its\napplication in anomaly detection.IEEE Transactions on Dependable and Secure Computing20, 02 (2023), 1048\u20131059.\n[5] Malik Agyemang, Ken Barker, and Rada Alhajj. 2006. A comprehensive survey of numeric and symbolic outlier\nmining techniques.Intelligent Data Analysis10, 6 (2006), 521\u2013538.\n[6] MohiuddinAhmed,AbdunNaserMahmood,andMdRafiqulIslam.2016.Asurveyofanomalydetectiontechniques\nin financial domain.Future Generation Computer Systems55 (2016), 278\u2013288.\n[7] Morteza Alizadeh, Michael Hamilton, Parker Jones, Junfeng Ma, and Raed Jaradat. 2021. Vehicle operating state\nanomaly detection and results virtual reality interpretation.Expert Systems with Applications177 (2021), 114928.\n[8] KasunAmarasinghe,KevinKenney,andMilosManic.2018.Towardexplainabledeepneuralnetworkbasedanomaly\ndetection.In Proceedings ofthe2018 11th International Conferenceon Human System Interaction (HSI). IEEE,311\u2013317.\n[9] Fabrizio Angiulli, Fabio Fassetti, Giuseppe Manco, and Luigi Palopoli. 2017. Outlying property detection with nu-\nmerical attributes.Data Mining and Knowledge Discovery31, 1 (2017), 134\u2013163.\n[10] FabrizioAngiulli,FabioFassetti,andLuigiPalopoli.2009.Detectingoutlyingpropertiesofexceptionalobjects. ACM\nTransactions on Database Systems34, 1 (2009), 1\u201362.\n[11] FabrizioAngiulli,FabioFassetti,andLuigiPalopoli.2012.Discoveringcharacterizationsofthebehaviorofanomalous\nsubpopulations. IEEE Transactions on Knowledge and Data Engineering25, 6 (2012), 1280\u20131292.\n[12] Liat Antwarg, Ronnie Mindlin Miller, Bracha Shapira, and Lior Rokach. 2021. Explaining anomalies detected by\nautoencoders using Shapley Additive Explanations.Expert Systems with Applications186 (2021), 115736.\n[13] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado,\nSalvador Garc\u00eda, Sergio Gil-L\u00f3pez, Daniel Molina, Richard Benjamins, Chatila Raja, and Herrera Francisco. 2020.\nExplainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI.",
        "3dbf6521-44ea-407f-acb2-4dcf131e9828": "IEEE Transactions on Knowledge and Data Engineering25, 6 (2012), 1280\u20131292.\n[12] Liat Antwarg, Ronnie Mindlin Miller, Bracha Shapira, and Lior Rokach. 2021. Explaining anomalies detected by\nautoencoders using Shapley Additive Explanations.Expert Systems with Applications186 (2021), 115736.\n[13] Alejandro Barredo Arrieta, Natalia D\u00edaz-Rodr\u00edguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado,\nSalvador Garc\u00eda, Sergio Gil-L\u00f3pez, Daniel Molina, Richard Benjamins, Chatila Raja, and Herrera Francisco. 2020.\nExplainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI.\nInformation Fusion58 (2020), 82\u2013115.\n[14] Roy Assaf, Ioana Giurgiu, Jonas Pfefferle, Serge Monney, Haris Pozidis, and Anika Schumann. 2021. An anomaly\ndetection and explainability framework using convolutional autoencoders for data storage systems. InProceedings\nof the 29th International Conference on International Joint Conferences on Artificial Intelligence. 5228\u20135230.\n[15] Anton Babenko, Leonardo Mariani, and Fabrizio Pastore. 2009. Ava: Automated interpretation of dynamically de-\ntected anomalies. InProceedings of the 18th International Symposium on Software Testing and Analysis. 237\u2013248.\n[16] Dor Bank, Noam Koenigstein, and Raja Giryes. 2020. Autoencoders.CoRR abs/2003.05991 (2020). arXiv:2003.05991\n[17] Alberto Barbado and \u00d3scar Corcho. 2022. Interpretable machine learning models for predicting and explaining ve-\nhicle fuel consumption anomalies.Engineering Applications of Artificial Intelligence115 (2022), 105222.\n[18] Alberto Barbado, \u00d3scar Corcho, and Richard Benjamins. 2022. Rule extraction in unsupervised anomaly detection\nfor model explainability: Application to OneClass SVM.Expert Systems with Applications189 (2022), 116100.\n[19] Katharina Beckh, Sebastian M\u00fcller, Matthias Jakobs, Vanessa Toborek, Hanxiao Tan, Raphael Fischer, Pascal Welke,\nSebastianHouben,andLauravonRueden.2023.HarnessingPriorKnowledgeforExplainableMachineLearning:An\nOverview. In2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). IEEE, 450\u2013463.\n[20] VaishakBelleandIoannisPapantonis.2021.Principlesandpracticeofexplainablemachinelearning. FrontiersinBig\nData 4 (2021), 39.\n[21] FabianBerns,MarkusLange-Hegermann,andChristianBeecks.2020.Towardsgaussianprocessesforautomaticand\ninterpretable anomaly detection in industry 4.0. InProceedings of the IN4PL. 87\u201392.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "62e1c0b6-afe8-46f4-9144-16df8bcf82a0": "23:46 Z. Li et al.\n[22] Xingyan Bin, Ying Zhao, and Bilong Shen. 2016. Abnormal subspace sparse PCA for anomaly detection and inter-\npretation. InODDx3\u201915:ACM SIGKDD 2015 Workshop. Association for Computing Machinery.\n[23] DanielBogdoll,MaximilianNitsche,andJ.MariusZ\u00f6llner.2022.Anomalydetectioninautonomousdriving:Asurvey.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4488\u20134499.\n[24] Kristof B\u00f6hmer and StefanieRinderle-Ma.2020.Miningassociation rules for anomalydetection indynamicprocess\nruntime behavior and explaining the root cause to users.Information Systems90 (2020), 101438.\n[25] Azzedine Boukerche, Lining Zheng, and Omar Alfandi. 2020. Outlier detection: Methods, models, and classification.\nACM Computing Surveys53, 3 (2020), 1\u201337.\n[26] MarkusM.Breunig,Hans-PeterKriegel,RaymondT.Ng,andJ\u00f6rgSander.2000.LOF:identifyingdensity-basedlocal\noutliers. InProceedings of the 2000 ACM SIGMOD International Conference on Management of Data. 93\u2013104.\n[27] David Broniatowski. 2021. Psychological foundations of explainability and interpretability in artificial intelligence.\n(2021).\n[28] Andy Brown, Aaron Tuor, Brian Hutchinson, and Nicole Nichols. 2018. Recurrent neural network attention mech-\nanisms for interpretable system log anomaly detection. InProceedings of the 1st Workshop on Machine Learning for\nComputing Systems.1 \u2013 8 .\n[29] H.BurakGunay,WeimingShen,GuyNewsham,andArazAshouri.2019.Detectionandinterpretationofanomalies\nin building energy use through inverse modeling.Science and Technology for the Built Environment25, 4 (2019),\n488\u2013503.\n[30] Nadia Burkart and Marco F. Huber. 2021. A survey on the explainability of supervised machine learning.Journal of\nArtificial Intelligence Research70 (2021), 245\u2013317.\n[31] Mattia Carletti, Marco Maggipinto, Alessandro Beghi, Gian Antonio Susto, Natalie Gentner, Yao Yang, and Andreas\nKyek. 2020. Interpretable anomaly detection for knowledge discovery in semiconductor manufacturing. InProceed-\nings of the 2020 Winter Simulation Conference (WSC\u201920). IEEE, 1875\u20131885.\n[32] MattiaCarletti,MatteoTerzi,andGianAntonioSusto.2023.Interpretableanomalydetectionwithdiffi:Depth-based\nfeature importance of isolation forest.Engineering Applications of Artificial Intelligence119 (2023), 105730.\n[33] Gail A. Carpenter and Stephen Grossberg. 1987. Neural dynamics of category learning and recognition: Attention,\nmemory consolidation, and amnesia. InProceedings of the Advances in Psychology. Elsevier, 239\u2013286.\n[34] Diogo V. Carvalho, Eduardo M. Pereira, and Jaime S. Cardoso. 2019. Machine learning interpretability: A survey on\nmethods and metrics.Electronics8, 8 (2019), 832.\n[35] Chandranil Chakraborttii and Heiner Litz. 2020. Explaining SSD failures using anomaly detection. InProceedings of\nthe Non-Volatile Memory Workshop.1 .\n[36] Raghavendra Chalapathy and Sanjay Chawla. 2019. Deep learning for anomaly detection: A survey. CoRR\nabs/1901.03407 (2019). arXiv:1901.03407\n[37] VarunChandola,ArindamBanerjee,andVipinKumar.2009.Anomalydetection:Asurvey. ACMComputingSurveys\n41, 3 (2009), 1\u201358.\n[38] Chun-Hao Chang, Jinsung Yoon, Sercan Arik, Madeleine Udell, and Tomas Pfister. 2022.",
        "11751f37-026c-46cb-8bad-d0343ebcbd5d": "[35] Chandranil Chakraborttii and Heiner Litz. 2020. Explaining SSD failures using anomaly detection. InProceedings of\nthe Non-Volatile Memory Workshop.1 .\n[36] Raghavendra Chalapathy and Sanjay Chawla. 2019. Deep learning for anomaly detection: A survey. CoRR\nabs/1901.03407 (2019). arXiv:1901.03407\n[37] VarunChandola,ArindamBanerjee,andVipinKumar.2009.Anomalydetection:Asurvey. ACMComputingSurveys\n41, 3 (2009), 1\u201358.\n[38] Chun-Hao Chang, Jinsung Yoon, Sercan Arik, Madeleine Udell, and Tomas Pfister. 2022. Data-efficient and inter-\npretable tabular anomaly detection.CoRR abs/2203.02034 (2022). arXiv:2203.02034\n[39] Anshika Chaudhary, Himangi Mittal, and Anuja Arora. 2019. Anomaly detection using graph neural networks. In\nProceedingsofthe2019InternationalConferenceonMachineLearning,BigData,CloudandParallelComputing(COMIT-\nCon\u201919). IEEE, 346\u2013350.\n[40] Ashima Chawla, Paul Jacob, Saman Feghhi, Devashish Rughwani, Sven van der Meer, and Sheila Fallon. 2020. Inter-\npretable unsupervised anomaly detection for RAN cell trace analysis. InProceedings of the 2020 16th International\nConference on Network and Service Management (CNSM). IEEE, 1\u20135.\n[41] Liang-Chieh Chen, Tsung-Ting Kuo, Wei-Chi Lai, Shou-De Lin, and Chi-Hung Tsai. 2012. Prediction-based outlier\ndetection with explanations. InProceedings of the 2012 IEEE International Conference on Granular Computing. IEEE,\n44\u201349.\n[42] Mike Chen, Alice X. Zheng, Jim Lloyd, Michael I. Jordan, and Eric Brewer. 2004. Failure diagnosis using decision\ntrees. InProceedings of the International Conference on Autonomic Computing, 2004.IEEE, 36\u201343.\n[43] N. F. Chen, Zhiyuan Du, and Khin Hua Ng. 2018. Scene graphs for interpretable video anomaly classification. In\nProceedings of the Conference on Neural Information Processing Systems Workshop on Visually Grounded Interaction\nand Language.\n[44] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. InProceedings of the 22nd ACM\nSigkdd International Conference on Knowledge Discovery and Data Mining. 785\u2013794.\n[45] Xuanhao Chen, Liwei Deng, Feiteng Huang, Chengwei Zhang, Zongquan Zhang, Yan Zhao, and Kai Zheng. 2021.\nDaemon: Unsupervised anomaly detection and interpretation for multivariate time series. InProceedings of the 2021\nIEEE 37th International Conference on Data Engineering (ICDE\u201921). IEEE, 2225\u20132230.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "4f6fe7a0-5396-47b7-8fbb-e8857cbdb240": "A Survey on Explainable Anomaly Detection 23:47\n[46] Ximeng Cheng, Zhiqian Wang, Xuexi Yang, Liyan Xu, and Yu Liu. 2021. Multi-scale detection and interpretation\nof spatio-temporal anomalies of human activities represented by time-series.Computers, Environment and Urban\nSystems88 (2021), 101627.\n[47] Mei-See Cheong, Mei-Chen Wu, and Szu-Hao Huang. 2021. Interpretable stock anomaly detection based on spatio-\ntemporal relation networks with genetic algorithm.IEEE Access9 (2021), 68302\u201368319.\n[48] Penny Chong, Ngai-Man Cheung, Yuval Elovici, and Alexander Binder. 2021. Toward scalable and unified example-\nbased explanation and outlier detection.IEEE Transactions on Image Processing31 (2021), 525\u2013540.\n[49] William S. Cleveland. 1979. Robust locally weighted regression and smoothing scatterplots.Journal of the American\nStatistical Association74, 368 (1979), 829\u2013836.\n[50] European Commission. 2020. On Artificial Intelligence\u2013A European Approach to Excellence and Trust. Avail-\nable online at: https://commission.europa.eu/publications/white-paper-artificial-intelligence-european-approach-\nexcellence-and-trust_en (accessed on 19 May 2022).\n[51] David Cortes. 2020. Explainable outlier detection through decision tree conditioning.CoRR abs/2001.00636 (2020).\narXiv:2001.00636\n[52] Xuan Hong Dang, Ira Assent, Raymond T. Ng, Arthur Zimek, and Erich Schubert. 2014. Discriminative features for\nidentifyingandinterpretingoutliers.In Proceedingsofthe2014IEEE30thInternationalConferenceonDataEngineering .\nIEEE, 88\u201399.\n[53] Xuan Hong Dang, Barbora Micenkov\u00e1, Ira Assent, and Raymond T. Ng. 2013. Local outlier detection with interpre-\ntation. InProceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases.\nSpringer, 304\u2013320.\n[54] ShubhomoyDas,MdRakibulIslam,NitthilanKannappanJayakodi,andJanardhanRaoDoppa.2019.Activeanomaly\ndetection via ensembles: Insights, algorithms, and interpretability.CoRR abs/1901.08930 (2019). arXiv:1901.08930\n[55] Ian Davidson. 2007. Anomaly detection, explanation and visualization.SGI, Tokyo, Japan, Tech. Rep(2007).\n[56] Oliver De Candido, Maximilian Binder, and Wolfgang Utschick. 2021. An interpretable lane change detector algo-\nrithm based on deep autoencoder anomaly detection. InProceedings of the 2021 IEEE Intelligent Vehicles Symposium\n(IV\u201921). IEEE, 516\u2013523.\n[57] Leonardo De Moura and Nikolaj Bj\u00f8rner. 2011. Satisfiability modulo theories: Introduction and applications.Com-\nmunications of the ACM54, 9 (2011), 69\u201377.\n[58] Linda Delamaire, Hussein Abdou, and John Pointon. 2009. Credit card fraud and detection techniques: A review.\nBanks and Bank Systems4, 2 (2009), 57\u201368.\n[59] CharlieDickens,EricMeissner,PabloG.Moreno,andTomDiethe.2020.Interpretableanomalydetectionwithmon-\ndrian polya forests on data streams.CoRR abs/2008.01505 (2020). arXiv:2008.01505\n[60] Theekshana Dissanayake, Tharindu Fernando, Simon Denman, Sridha Sridharan, Houman Ghaemmaghami, and\nClintonFookes.2020.Arobustinterpretabledeeplearningclassifierforheartanomalydetectionwithoutsegmenta-\ntion.IEEE Journal of Biomedical and Health Informatics25, 6 (2020), 2162\u20132171.\n[61] Finale Doshi-Velez and Been Kim. 2017.",
        "66b413c3-f304-4eb3-bcc9-7b40f9a0926a": "Banks and Bank Systems4, 2 (2009), 57\u201368.\n[59] CharlieDickens,EricMeissner,PabloG.Moreno,andTomDiethe.2020.Interpretableanomalydetectionwithmon-\ndrian polya forests on data streams.CoRR abs/2008.01505 (2020). arXiv:2008.01505\n[60] Theekshana Dissanayake, Tharindu Fernando, Simon Denman, Sridha Sridharan, Houman Ghaemmaghami, and\nClintonFookes.2020.Arobustinterpretabledeeplearningclassifierforheartanomalydetectionwithoutsegmenta-\ntion.IEEE Journal of Biomedical and Health Informatics25, 6 (2020), 2162\u20132171.\n[61] Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning.CoRR\nabs/1702.08608 (2017). arXiv:1702.08608\n[62] FilipKarloDo\u0161ilovi\u0107,MarioBr\u010di\u0107,andNikicaHlupi\u0107.2018.Explainableartificialintelligence:Asurvey.In Proceedings\nof the 2018 41st International Convention on Information and Communication Technology, Electronics, and Microelec-\ntronics (MIPRO\u201918). IEEE, 0210\u20130215.\n[63] MinDu,FeifeiLi,GuinengZheng,andVivekSrikumar.2017.Deeplog:Anomalydetectionanddiagnosisfromsystem\nlogs through deep learning. InProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications\nSecurity. 1285\u20131298.\n[64] LeiDuan,GuantingTang,JianPei,JamesBailey,AkikoCampbell,andChangjieTang.2015.Miningoutlyingaspects\non numeric data.Data Mining and Knowledge Discovery29, 5 (2015), 1116\u20131151.\n[65] Ricardo Dunia and S. Joe Qin. 1997. Multi-dimensional fault diagnosis using a subspace approach. InProceedings of\nthe American Control Conference.\n[66] N.Dunstan,I.Despi,andC.Watson.2009.Anomaliesinmultidimensionalcontexts. WITTransactionsonInformation\nand Communication Technologies42 (2009), 173.\n[67] LeventErtoz,EricEilertson,AleksandarLazarevic,Pang-NingTan,VipinKumar,JaideepSrivastava,andPaulDokas.\n2004. Minds-minnesota intrusion detection system.Next Generation Data Mining(2004), 199\u2013218.\n[68] JiangfanFeng,YukunLiang,andLinLi.2021.Anomalydetectioninvideosusingtwo-streamautoencoderwithpost\nhoc interpretability.Computational Intelligence and Neuroscience2021 (2021).\n[69] Tharindu Fernando, Houman Ghaemmaghami, Simon Denman, Sridha Sridharan, Nayyar Hussain, and Clinton\nFookes. 2019. Heart sound segmentation using bidirectional LSTMs with attention.IEEE Journal of Biomedical and\nHealth Informatics24, 6 (2019), 1601\u20131609.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "5b2b0021-6130-4b1b-88fe-d5bac9c48778": "23:48 Z. Li et al.\n[70] SylvainFuertes,GillesPicart,Jean-YvesTourneret,LotfiChaari,Andr\u00e9Ferrari,andC\u00e9dricRichard.2016.Improving\nspacecraft health monitoring with automatic anomaly detection techniques. InProceedings of the 14th International\nConference on Space Operations. 2430.\n[71] Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining ex-\nplanations: An overview of interpretability of machine learning. InProceedings of the 2018 IEEE 5th International\nConference on Data Science and Advanced Analytics (DSAA\u201918). IEEE, 80\u201389.\n[72] IoanaGiurgiuandAnikaSchumann.2019.Additiveexplanationsforanomaliesdetectedfrommultivariatetemporal\ndata.In Proceedingsofthe28thACMInternationalConferenceonInformationandKnowledgeManagement .2245\u20132248.\n[73] NicoGnoss,MartinSchultz,andMarinaTropmann-Frick.2022.XAIintheauditdomain-explaininganautoencoder\nmodel for anomaly detection. In17th International Conference on Wirtschaftsinformatik. AISeL.\n[74] Oleg Gorokhov, Mikhail Petrovskiy, and Igor Mashechkin. 2017. Convolutional neural networks for unsupervised\nanomaly detection in text data. InProceedings of the International Conference on Intelligent Data Engineering and\nAutomated Learning. Springer, 500\u2013507.\n[75] GudmundGrov,MarcSabate,WeiChen,andDavidAspinall.2019.Towardsintelligiblerobustanomalydetectionby\nlearning interpretable behavioural models.NISK J32 (2019), 1\u201316.\n[76] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. A\nsurvey of methods for explaining black box models.ACM Computing Surveys (CSUR)51, 5 (2018), 1\u201342.\n[77] Shunan Guo, Zhuochen Jin, Qing Chen, David Gotz, Hongyuan Zha, and Nan Cao. 2022. Interpretable anomaly\ndetection in event sequences via sequence matching and visual comparison.IEEE Transactions on Visualization and\nComputer Graphics28 (2022), 4531\u20134545.\n[78] Manish Gupta, Jing Gao, Charu C. Aggarwal, and Jiawei Han. 2013. Outlier detection for temporal data: A survey.\nIEEE Transactions on Knowledge and Data Engineering26, 9 (2013), 2250\u20132267.\n[79] Nikhil Gupta, Dhivya Eswaran, Neil Shah, Leman Akoglu, and Christos Faloutsos. 2018. Beyond outlier detection:\nLookoutforpictorialexplanation.In ProceedingsoftheJointEuropeanConferenceonMachineLearningandKnowledge\nDiscovery in Databases. Springer, 122\u2013138.\n[80] Xiao Han, He Cheng, Depeng Xu, and Shuhan Yuan. 2021. InterpretableSAD: Interpretable anomaly detection\nin sequential log data. InProceedings of the 2021 IEEE International Conference on Big Data (Big Data\u201921). IEEE,\n1183\u20131192.\n[81] Douglas M. Hawkins. 1980.Identification of Outliers. Springer.\n[82] JingruiHeandJaimeCarbonell.2010.Co-selectionoffeaturesandinstancesforunsupervisedrarecategoryanalysis.\nIn Proceedings of the 2010 SIAM International Conference on Data Mining. SIAM, 525\u2013536.\n[83] Zengyou He, Xiaofei Xu, Zhexue Joshua Huang, and Shengchun Deng. 2005. FP-outlier: Frequent pattern based\noutlier detection.Computer Science and Information Systems2, 1 (2005), 103\u2013118.",
        "942fcfb1-9910-487d-bd0c-aa5d095958cb": "2021. InterpretableSAD: Interpretable anomaly detection\nin sequential log data. InProceedings of the 2021 IEEE International Conference on Big Data (Big Data\u201921). IEEE,\n1183\u20131192.\n[81] Douglas M. Hawkins. 1980.Identification of Outliers. Springer.\n[82] JingruiHeandJaimeCarbonell.2010.Co-selectionoffeaturesandinstancesforunsupervisedrarecategoryanalysis.\nIn Proceedings of the 2010 SIAM International Conference on Data Mining. SIAM, 525\u2013536.\n[83] Zengyou He, Xiaofei Xu, Zhexue Joshua Huang, and Shengchun Deng. 2005. FP-outlier: Frequent pattern based\noutlier detection.Computer Science and Information Systems2, 1 (2005), 103\u2013118.\n[84] Jonas Herskind Sejr, Thorbj\u00f8rn Christiansen, Nicolai Dvinge, Dan Hougesen, Peter Schneider-Kamp, and Arthur\nZimek.2021.Outlierdetectionwithexplanationsonmusicstreamingdata:Acasestudywithdanmarkmusicgroup\nLtd. Applied Sciences11, 5 (2021), 2270.\n[85] Ryota Hinami, Tao Mei, and Shin\u2019ichi Satoh. 2017. Joint detection and recounting of abnormal events by learning\ndeep generic knowledge. InProceedings of the IEEE International Conference on Computer Vision. 3619\u20133627.\n[86] Victoria Hodge and Jim Austin. 2004. A survey of outlier detection methodologies.Artificial Intelligence Review22,\n2 (2004), 85\u2013126.\n[87] Chanwoong Hwang and Taejin Lee. 2021. E-sfd: Explainable sensor fault detection in the ics anomaly detection\nsystem. IEEE Access9 (2021), 140470\u2013140486.\n[88] Yasuhiro Ikeda, Keisuke Ishibashi, Yuusuke Nakano, Keishiro Watanabe, and Ryoichi Kawahara. 2018. Anomaly\ndetection and interpretation using multimodal autoencoder and sparse optimization.CoRR abs/1812.07136 (2018).\narXiv:1812.07136\n[89] Yasuhiro Ikeda, Kengo Tajiri, Yuusuke Nakano, Keishiro Watanabe, and Keisuke Ishibashi. 2018. Estimation\nof dimensions contributing to detected anomalies with variational autoencoders. CoRR abs/1811.04576 (2018).\narXiv:1811.04576\n[90] Sarah Itani, Fabian Lecron, and Philippe Fortemps. 2020. A one-class classification decision tree based on kernel\ndensity estimation.Applied Soft Computing91 (2020), 106250.\n[91] Jakub Jakubowski, Przemys\u0142aw Stanisz, Szymon Bobek, and Grzegorz J. Nalepa. 2021. Anomaly detection in asset\ndegradation process using variational autoencoder and explanations.Sensors 22, 1 (2021), 291.\n[92] JakubJakubowski,Przemys\u0142awStanisz,SzymonBobek,andGrzegorzJ.Nalepa.2021.Explainableanomalydetection\nfor Hot-rolling industrial process. InProceedings of the 2021 IEEE 8th International Conference on Data Science and\nAdvanced Analytics (DSAA\u201921). IEEE, 1\u201310.\n[93] Jinchao Ji, Tian Bai, Chunguang Zhou, Chao Ma, and Zhe Wang. 2013. An improved k-prototypes clustering algo-\nrithm for mixed numeric and categorical data.Neurocomputing120 (2013), 590\u2013596.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "cc86fcbf-8c8c-4ba8-a8fa-bb577b3527ad": "A Survey on Explainable Anomaly Detection 23:49\n[94] Ian T. Jolliffe, Nickolay T. Trendafilov, and Mudassir Uddin. 2003. A modified principal component technique based\non the LASSO.Journal of Computational and Graphical Statistics12, 3 (2003), 531\u2013547.\n[95] NirmalSobhaKartha,Cl\u00e9mentGautrais,andVincentVercruyssen.2021.Whyareyouweird?Infusinginterpretabil-\nity in isolation forest for anomaly detection. InProceedings of the Explainable Agency in AI Workshop (AAAI\u201921).\n51\u201357.\n[96] JacobKauffmann,Klaus-RobertM\u00fcller,andGr\u00e9goireMontavon.2020.Towardsexplaininganomalies:AdeepTaylor\ndecomposition of one-class models.Pattern Recognition101 (2020), 107198.\n[97] JacobKauffmann,LukasRuff,Gr\u00e9goireMontavon,andKlaus-RobertM\u00fcller.2020.ThecleverHanseffectinanomaly\ndetection. CoRR abs/2006.10609 (2020). arXiv:2006.10609\n[98] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. Light-\ngbm: A highly efficient gradient boosting decision tree. InProceedings of the 31st International Conference on Neural\nInformation Processing Systems (Long Beach, California, USA) (NIPS\u201917). Curran Associates Inc., 3149\u20133157.\n[99] Fabian Keller, Emmanuel Muller, and Klemens Bohm. 2012.HiCS: High contrast subspaces for density-based outlier\nranking. InProceedings of the 2012 IEEE 28th International Conference on Data Engineering. IEEE, 1037\u20131048.\n[100] FabianKeller,EmmanuelM\u00fcller,AndreasWixler,andKlemensB\u00f6hm.2013.Flexibleandadaptivesubspacesearchfor\noutlieranalysis.In Proceedingsofthe22ndACMInternationalConferenceonInformationandKnowledgeManagement .\n1381\u20131390.\n[101] EamonnKeogh,JessicaLin,andAdaFu.2005.Hotsax:Efficientlyfindingthemostunusualtimeseriessubsequence.\nInProceedings of the 5th IEEE International Conference on Data Mining (ICDM\u201905). IEEE, 8\u2013pp.\n[102] SebastianKieferandG\u00fcnterPesch.2021.Unsupervisedanomalydetectionforfinancialauditingwithmodel-agnostic\nexplanations. In Proceedings of the German Conference on Artificial Intelligence (K\u00fcnstliche Intelligenz). Springer,\n291\u2013308.\n[103] Donghyun Kim, Gian Antariksa, Melia Putri Handayani, Sangbong Lee, and Jihwan Lee. 2021. Explainable anomaly\ndetection framework for maritime main engine sensor data.Sensors 21, 15 (2021), 5200.\n[104] Shogo Kitamura and Yuichi Nonaka. 2019. Explainable anomaly detection via feature-based localization. InProceed-\nings of the International Conference on Artificial Neural Networks. Springer, 408\u2013419.\n[105] Edwin M. Knorr and Raymond T. Ng. 1998. Algorithms for mining distance-based outliers in large datasets. InPro-\nceedings of the VLDB. Citeseer, 392\u2013403.\n[106] EdwinM.KnorrandRaymondT.Ng.1999.Findingintensionalknowledgeofdistance-basedoutliers.In Proceedings\nof the VLDB. Citeseer, 211\u2013222.\n[107] Martin Kopp, Tom\u00e1\u0161 Pevn`y, and Martin Holena. 2014. Interpreting and clustering outliers with sapling random\nforests. InProceedings of the European Conference on Information Technologies\u2013Applications and Theory.61\u201367.\n[108] Ines Ben Kraiem, Faiza Ghozzi, Andr\u00e9 P\u00e9ninou, Geoffrey Roman-Jimenez, and Olivier Teste. 2021. Human-\ninterpretable rules for anomaly detection in time-series.",
        "84252ad3-5b9c-4d01-a4b1-cf0411c2af4b": "1998. Algorithms for mining distance-based outliers in large datasets. InPro-\nceedings of the VLDB. Citeseer, 392\u2013403.\n[106] EdwinM.KnorrandRaymondT.Ng.1999.Findingintensionalknowledgeofdistance-basedoutliers.In Proceedings\nof the VLDB. Citeseer, 211\u2013222.\n[107] Martin Kopp, Tom\u00e1\u0161 Pevn`y, and Martin Holena. 2014. Interpreting and clustering outliers with sapling random\nforests. InProceedings of the European Conference on Information Technologies\u2013Applications and Theory.61\u201367.\n[108] Ines Ben Kraiem, Faiza Ghozzi, Andr\u00e9 P\u00e9ninou, Geoffrey Roman-Jimenez, and Olivier Teste. 2021. Human-\ninterpretable rules for anomaly detection in time-series. InProceedings of the International Conference on Extending\nDatabase Technology. 457\u2013462.\n[109] Hans-Peter Kriegel, Peer Kr\u00f6ger, Erich Schubert, and Arthur Zimek. 2012. Outlier detection in arbitrarily oriented\nsubspaces. InProceedings of the 2012 IEEE 12th International Conference on Data Mining. IEEE, 379\u2013388.\n[110] Chia-Tung Kuo and Ian Davidson. 2016. A framework for outlier description using constraint programming. In\nProceedings of the AAAI Conference on Artificial Intelligence.\n[111] Rocco Langone, Alfredo Cuzzocrea, and Nikolaos Skantzos. 2020. Interpretable anomaly prediction: Predicting\nanomalousbehaviorinindustry4.0settingsviaregularizedlogisticregressiontools. DataandKnowledgeEngineering\n130 (2020), 101850.\n[112] Sebastian Lapuschkin, Stephan W\u00e4ldchen, Alexander Binder, Gr\u00e9goire Montavon, Wojciech Samek, and Klaus-\nRobertM\u00fcller.2019.Unmaskingclever hanspredictors andassessing whatmachines reallylearn. Nature Communi-\ncations 10, 1 (2019), 1\u20138.\n[113] Stefan Leue and Mitra Tabaei Befrouei. 2012. Counterexample explanation by anomaly detection. InProceedings of\nthe International SPIN Workshop on Model Checking of Software. Springer, 24\u201342.\n[114] Efrat Levy, Nadav Maman, Asaf Shabtai, and Yuval Elovici. 2022. AnoMili: Spoofing prevention and explainable\nanomaly detection for the 1553 military avionic bus.CoRR abs/2202.06870 (2022). arXiv:2202.06870\n[115] Jiamu Li, Ji Zhang, Jian Wang, Youwen Zhu, Mohamed Jaward Bah, Gaoming Yang, and Yuquan Gan. 2021. VAGA:\nTowards accurate and interpretable outlier detection based on variational auto-encoder and genetic algorithm for\nhigh-dimensionaldata.In Proceedingsofthe2021IEEEInternationalConferenceonBigData(BigData\u201921) .IEEE,5956\u2013\n5958.\n[116] Wenkai Li, Wenbo Hu, Ning Chen, and Cheng Feng. 2022. Stacking VAE with graph neural networks for effective\nand interpretable time series anomaly detection.AI Open3 (2022), 101\u2013110.\n[117] ZhihanLi,YoujianZhao,JiaqiHan,YaSu,RuiJiao,XidaoWen,andDanPei.2021.Multivariatetimeseriesanomaly\ndetectionandinterpretationusinghierarchicalinter-metricandtemporalembedding.In Proceedingsofthe27thACM\nSIGKDD Conference on Knowledge Discovery and Data Mining. 3220\u20133230.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "40edd60c-2c65-4876-b5bb-f82138144635": "23:50 Z. Li et al.\n[118] Zhihan Li, Youjian Zhao, Rong Liu, and Dan Pei. 2018. Robust and rapid clustering of kpis for large-scale anomaly\ndetection. InProceedings of the 2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS\u201918). IEEE,\n1\u201310.\n[119] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022. A survey of transformers.AI Open3 (2022), 111\u2013\n132.\n[120] Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. 2020. Explainable AI: A review of machine\nlearning interpretability methods.Entropy23, 1 (2020), 18.\n[121] Zachary C. Lipton. 2018. The mythos of model interpretability: In machine learning, the concept of interpretability\nis both important and slippery.Queue 16, 3 (2018), 31\u201357.\n[122] FeiTonyLiu,KaiMingTing,andZhi-HuaZhou.2008.Isolationforest.In Proceedingsofthe20088thIEEEInternational\nConference on Data Mining. IEEE, 413\u2013422.\n[123] Ninghao Liu, Donghwa Shin, and Xia Hu. 2018. Contextual outlier interpretation. InProceedings of the 27th Interna-\ntional Joint Conference on Artificial Intelligence (Stockholm, Sweden) (IJCAI\u201918). AAAI Press, 2461\u20132467.\n[124] Wei Liu, Yu Zheng, Sanjay Chawla, Jing Yuan, and Xie Xing. 2011. Discovering spatio-temporal causal interactions\nin traffic data streams. InProceedings of the 17th ACMSIGKDD International Conference on Knowledge Discovery and\nData Mining. 1010\u20131018.\n[125] YinxiLiu,KaiYang,ShaoyuDou,andPanLuo.2022.Interpretableanomalydetectioninvariable-lengthco-evolving\nrhythmic sequences. In1st Workshop on Artificial Intelligence for Anomalies and Novelties (AI4AN).\n[126] PhilippLiznerski,LukasRuff,RobertA.Vandermeulen,BillyJoeFranks,MariusKloft,andKlaus-RobertM\u00fcller.2020.\nExplainable deep one-class classification. InInternational Conference on Learning Representations.\n[127] S. Lundberg and S. I. Lee. 2021. A game theoretic approach to explain the output of any machine learning model.\nAvailable online at:https://github.com/shap/shap(accessed on 19 May 2022).\n[128] Scott M. Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. InProceedings of\nthe 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS\u201917).\nCurran Associates Inc., Red Hook, NY, USA, 4768\u20134777.\n[129] Meghanath Macha and Leman Akoglu. 2018. Explaining anomalies in groups with characterizing subspace rules.\nData Mining and Knowledge Discovery32, 5 (2018), 1444\u20131480.\n[130] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. 2016. Adversarial autoen-\ncoders. ICLR Workshop track.\n[131] LeonardoMarianiandFabrizioPastore.2008.Automatedidentificationoffailurecausesinsystemlogs.In Proceedings\nof the 2008 19th International Symposium on Software Reliability Engineering (ISSRE\u201908). IEEE, 117\u2013126.\n[132] Daniel L. Marino, Chathurika S. Wickramasinghe, Craig Rieger, and Milos Manic. 2022. Self-supervised and inter-\npretable anomaly detection using network transformers.CoRR abs/2202.12997 (2022). arXiv:2202.12997\n[133] Ioulia Markou, Filipe Rodrigues, and Francisco C. Pereira. 2017.",
        "85b54741-3d5a-4fbb-9c87-717d74b5010d": "2016. Adversarial autoen-\ncoders. ICLR Workshop track.\n[131] LeonardoMarianiandFabrizioPastore.2008.Automatedidentificationoffailurecausesinsystemlogs.In Proceedings\nof the 2008 19th International Symposium on Software Reliability Engineering (ISSRE\u201908). IEEE, 117\u2013126.\n[132] Daniel L. Marino, Chathurika S. Wickramasinghe, Craig Rieger, and Milos Manic. 2022. Self-supervised and inter-\npretable anomaly detection using network transformers.CoRR abs/2202.12997 (2022). arXiv:2202.12997\n[133] Ioulia Markou, Filipe Rodrigues, and Francisco C. Pereira. 2017. Use of taxi-trip data in analysis of demand patterns\nfor detection and explanation of anomalies.Transportation Research Record2643, 1 (2017), 129\u2013138.\n[134] MarkosMarkouandSameerSingh.2003.Noveltydetection:Areview\u2013part1:statisticalapproaches. SignalProcessing\n83, 12 (2003), 2481\u20132497.\n[135] Markos Markou and Sameer Singh. 2003. Novelty detection: A review\u2013part 2: neural network based approaches.\nSignal Processing83, 12 (2003), 2499\u20132521.\n[136] ThabangMathonsiandTerenceL.vanZyl.2021.Astatisticsanddeeplearninghybridmethodformultivariatetime\nseries forecasting and mortality modeling.Forecasting4, 1 (2021), 1\u201325.\n[137] R. Daniel Mauldin, William D. Sudderth, and Stanley C. Williams. 1992. Polya trees and random distributions.The\nAnnals of Statistics(1992), 1203\u20131221.\n[138] Jacopo Mauro, Michael Nieke, Christoph Seidl, and Ingrid Chieh Yu. 2017. Anomaly detection and explanation in\ncontext-aware software product lines. InProceedings of the 21st International Systems and Software Product Line\nConference-Volume B. 18\u201321.\n[139] Manuel Mejia-Lavalle. 2010. Outlier detection with innovative explanation facility over a very large financial data-\nbase. InProceedings of the 2010 IEEE Electronics, Robotics and Automotive Mechanics Conference. IEEE, 23\u201327.\n[140] MiladMemarzadeh,BryanMatthews,andThomasTemplin.2022.MulticlassAnomalyDetectioninFlightDataUsing\nSemi-Supervised Explainable Deep Learning Model.Journal of Aerospace Information Systems19, 2 (2022), 83\u201397.\n[141] Barbora Micenkov\u00e1, Raymond T. Ng, Xuan-Hong Dang, and Ira Assent. 2013. Explaining outliers by subspace sepa-\nrability. InProceedings of the 2013 IEEE 13th International Conference on Data Mining. IEEE, 518\u2013527.\n[142] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences.Artificial Intelligence267\n(2019), 1\u201338.\n[143] TimMiller.2021.Contrastiveexplanation:Astructural-modelapproach. TheKnowledgeEngineeringReview 36(2021),\ne14.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "945aefbb-2a01-4474-94c0-e17a83915ae1": "A Survey on Explainable Anomaly Detection 23:51\n[144] Gr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller. 2017. Ex-\nplaining nonlinear classification decisions with deep taylor decomposition.Pattern Recognition65 (2017), 211\u2013222.\n[145] Gr\u00e9goire Montavon, Wojciech Samek, and Klaus-Robert M\u00fcller. 2018. Methods for interpreting and understanding\ndeep neural networks.Digital Signal Processing73 (2018), 1\u201315.\n[146] Brian Morris. 2019. Explainable anomaly and intrusion detection intelligence for platform information technology\nusing dimensionality reduction and ensemble learning. InProceedings of the 2019 IEEE AUTOTESTCON. IEEE, 1\u20135.\n[147] Pavol Mulinka, Pedro Casas, Kensuke Fukuda, and Lukas Kencl. 2020. HUMAN-hierarchical clustering for unsuper-\nvised anomaly detection and interpretation. InProceedings of the 2020 11th International Conference on Network of\nthe Future (NoF\u201920). IEEE, 132\u2013140.\n[148] Emmanuel M\u00fcller, Fabian Keller, Sebastian Blanc, and Klemens B\u00f6hm. 2012. OutRules: A framework for outlier\ndescriptions in multiple context spaces. InProceedings of the Joint European Conference on Machine Learning and\nKnowledge Discovery in Databases. Springer, 828\u2013832.\n[149] Emmanuel M\u00fcller, Matthias Schiffer, and Thomas Seidl. 2011. Statistical selection of relevant subspace projections\nforoutlier ranking.In Proceedings ofthe2011 IEEE27th International ConferenceonData Engineering. IEEE,434\u2013445.\n[150] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. 2019. Definitions, methods, and\napplications in interpretable machine learning.Proceedings of the National Academy of Sciences116, 44 (2019),\n22071\u201322080.\n[151] Quoc Phong Nguyen, Kar Wai Lim, Dinil Mon Divakaran, Kian Hsiang Low, and Mun Choon Chan. 2019. Gee: A\ngradient-based explainable variational autoencoder for network anomaly detection. InProceedings of the 2019 IEEE\nConference on Communications and Network Security (CNS\u201919). IEEE, 91\u201399.\n[152] Ahmad KamalMohd Nor, Srinivasa RaoPedapati,andMasdi Muhammad.2021.Applicationof explainableAI (XAI)\nfor anomaly detection and prognostic of gas turbines with uncertainty quantifica-tion. Preprints (September 2021).\nhttps://doi.org/10.20944/preprints202109.0034.v1\n[153] Harsha Nori, Samuel Jenkins, Paul Koch, and Rich Caruana. 2019. Interpretml: A unified framework for machine\nlearning interpretability.CoRR abs/1909.09223 (2019). arXiv:1909.09223\n[154] Keith Noto, Carla Brodley, and Donna Slonim. 2012. FRaC: A feature-modeling approach for semi-supervised and\nunsupervised anomaly detection.Data Mining and Knowledge Discovery25, 1 (2012), 109\u2013133.\n[155] Guansong Pang and Charu Aggarwal. 2021. Toward explainable deep anomaly detection. InProceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining. 4056\u20134057.\n[156] GuansongPang,LongbingCao,andLingChen.2016.Outlierdetectionincomplexcategoricaldatabymodellingthe\nfeature value couplings. InProceedings of the IJCAI International Joint Conference on Artificial Intelligence.\n[157] Guansong Pang, Longbing Cao, Ling Chen, Defu Lian, and Huan Liu. 2018. Sparse modeling-based sequential en-\nsemble learning for effective outlier detection in high-dimensional numeric data. InProceedings of the 32nd AAAI\nConference on Artificial Intelligence.",
        "b7a60f0b-88d6-4272-a4b8-84d179ef48d6": "[155] Guansong Pang and Charu Aggarwal. 2021. Toward explainable deep anomaly detection. InProceedings of the 27th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining. 4056\u20134057.\n[156] GuansongPang,LongbingCao,andLingChen.2016.Outlierdetectionincomplexcategoricaldatabymodellingthe\nfeature value couplings. InProceedings of the IJCAI International Joint Conference on Artificial Intelligence.\n[157] Guansong Pang, Longbing Cao, Ling Chen, Defu Lian, and Huan Liu. 2018. Sparse modeling-based sequential en-\nsemble learning for effective outlier detection in high-dimensional numeric data. InProceedings of the 32nd AAAI\nConference on Artificial Intelligence.\n[158] GuansongPang,LongbingCao,LingChen,andHuanLiu.2016.Unsupervisedfeatureselectionforoutlierdetection\nby modelling hierarchical value-feature couplings. InProceedings of the 2016 IEEE 16th International Conference on\nData Mining (ICDM\u201916). IEEE, 410\u2013419.\n[159] Guansong Pang, Longbing Cao, Ling Chen, and Huan Liu. 2017. Learning homophily couplings from Non-IID data\nfor joint feature selection and noise-resilient outlier detection. InProceedings of the IJCAI. 2585\u20132591.\n[160] GuansongPang,ChouboDing,ChunhuaShen,andAntonvandenHengel.2021.Explainabledeepfew-shotanomaly\ndetection with deviation networks.CoRR abs/2108.00462 (2021). arXiv:2108.00462\n[161] GuansongPang,ChunhuaShen,LongbingCao,andAntonVanDenHengel.2021.Deeplearningforanomalydetec-\ntion: A review.ACM Computing Surveys (CSUR)54, 2 (2021), 1\u201338.\n[162] Egawati Panjei, Le Gruenwald, Eleazar Leal, Christopher Nguyen, and Shejuti Silvia. 2022. A survey on outlier ex-\nplanations.The VLDB Journal31 (2022), 977\u20131008.\n[163] CheongHeeParkandJiilKim.2021.Anexplainableoutlierdetectionmethodusingregion-partitiontrees. TheJour-\nnal of Supercomputing77, 3 (2021), 3062\u20133076.\n[164] Sungwoo Park, Jihoon Moon, and Eenjun Hwang. 2020. Explainable anomaly detection for district heating based\non shapley additive explanations. InProceedings of the 2020 International Conference on Data Mining Workshops\n(ICDMW\u201920). IEEE, 762\u2013765.\n[165] Animesh Patcha and Jung-Min Park. 2007. An overview of anomaly detection techniques: Existing solutions and\nlatest technological trends.Computer Networks51, 12 (2007), 3448\u20133470.\n[166] AumPatil,AmeyWadekar,TanishqGupta,RohitVijan,andFarukKazi.2019.ExplainableLSTMmodelforanomaly\ndetection in HDFS log file using layerwise relevance propagation. InProceedings of the 2019 IEEE Bombay Section\nSignature Conference (IBSSC\u201919). IEEE, 1\u20136.\n[167] HeikoPaulheimandRobertMeusel.2015.Adecompositionoftheoutlierdetectionproblemintoasetofsupervised\nlearning problems.Machine Learning100, 2 (2015), 509\u2013531.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "4b5f754e-8a56-4768-b08d-92331c48518b": "23:52 Z. Li et al.\n[168] Deysy Galeana Perez and Manuel Mejia Lavalle. 2011. Outlier detection applying an innovative user transaction\nmodelingwithautomaticexplanation.In Proceedingsofthe2011IEEEElectronics,Robotics,andAutomotiveMechanics\nConference. IEEE, 41\u201346.\n[169] Tom\u00e1\u0161 Pevn`y. 2016. Loda: Lightweight on-line detector of anomalies.Machine Learning102, 2 (2016), 275\u2013304.\n[170] Tom\u00e1\u0161 Pevn`y and Martin Kopp. 2014. Explaining anomalies with sapling random forests. InProceedings of the Infor-\nmation Technologies-Applications and Theory Workshops, Posters, and Tutorials (ITAT\u201914).\n[171] Sreeraj Rajendran, Wannes Meert, Vincent Lenders, and Sofie Pollin. 2018. SAIFE: Unsupervised wireless spectrum\nanomaly detection with interpretable features. InProceedings of the 2018 IEEE International Symposium on Dynamic\nSpectrum Access Networks (DySPAN\u201918). IEEE, 1\u20139.\n[172] Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. 2000. Efficient algorithms for mining outliers from large\ndata sets. InProceedings of the 2000 ACM SIGMOD International Conference on Management of Data. 427\u2013438.\n[173] EricaRamirez,MarkusWimmer,andMartinAtzmueller.2019.Acomputationalframeworkforinterpretableanomaly\ndetection and classification of multivariate time series with application to human gait data analysis. InProceedings\noftheArtificialIntelligenceinMedicine:KnowledgeRepresentationandTransparentandExplainableSystems .Springer,\n132\u2013147.\n[174] MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2016.\u201cWhyshoulditrustyou?\u201dExplainingthepredictions\nof any classifier. InProceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining. 1135\u20131144.\n[175] MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2018.Anchors:High-precisionmodel-agnosticexplanations.\nIn Proceedings of the AAAI Conference on Artificial Intelligence.\n[176] KonradRieckandPavelLaskov.2009.Visualizationandexplanationofpayload-basedanomalydetection.In Proceed-\nings of the 2009 European Conference on Computer Network Defense. IEEE, 29\u201336.\n[177] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image\nsegmentation. InProceedings of the International Conference on Medical Image Computing and Computer-Assisted\nIntervention. Springer, 234\u2013241.\n[178] Khushnaseeb RoshanandAasim Zafar.2021.UtilizingXAI technique to improveautoencoder basedmodelfor com-\nputer network anomaly detection with shapley additive explanation (SHAP).International Journal of Computer Net-\nworks & Communications13 (2021), 1\u201320.\n[179] Daniel M. Roy and Yee Whye Teh. 2008. The mondrian process. InProceedings of the NIPS, Vol. 21.\n[180] Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use inter-\npretable models instead.Nature Machine Intelligence1, 5 (2019), 206\u2013215.\n[181] Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gr\u00e9goire Montavon, Wojciech Samek, Marius Kloft,\nThomas G. Dietterich, and Klaus-Robert M\u00fcller. 2021. A unifying review of deep and shallow anomaly detection.\nProceedings of the IEEE(2021).\n[182] Mao Saeki, Jun Ogata, Masahiro Murakawa, and Tetsuji Ogawa. 2019. Visual explanation of neural network based\nrotationmachineryanomalydetectionsystem.In Proceedingsofthe2019IEEEInternationalConferenceonPrognostics\nand Health Management (ICPHM\u201919). IEEE, 1\u20134.",
        "e72de9ef-0504-420f-8e58-6b9941a553a2": "Stop explaining black box machine learning models for high stakes decisions and use inter-\npretable models instead.Nature Machine Intelligence1, 5 (2019), 206\u2013215.\n[181] Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gr\u00e9goire Montavon, Wojciech Samek, Marius Kloft,\nThomas G. Dietterich, and Klaus-Robert M\u00fcller. 2021. A unifying review of deep and shallow anomaly detection.\nProceedings of the IEEE(2021).\n[182] Mao Saeki, Jun Ogata, Masahiro Murakawa, and Tetsuji Ogawa. 2019. Visual explanation of neural network based\nrotationmachineryanomalydetectionsystem.In Proceedingsofthe2019IEEEInternationalConferenceonPrognostics\nand Health Management (ICPHM\u201919). IEEE, 1\u20134.\n[183] Hojjat Salehinejad, Sharan Sankar, Joseph Barfett, Errol Colak, and Shahrokh Valaee. 2017. Recent advances in re-\ncurrent neural networks.CoRR abs/1801.01078 (2018). arXiv:1801.01078\n[184] DurgeshSamariya,SunilAryal,KaiMingTing,andJiangangMa.2020.Aneweffectiveandefficientmeasureforout-\nlyingaspectmining.In ProceedingsoftheInternationalConferenceonWebInformationSystemsEngineering .Springer,\n463\u2013474.\n[185] DurgeshSamariya,JiangangMa,SunilAryal,andKaiMingTing.2020.Acomprehensivesurveyonoutlyingaspect\nmining methods.CoRR abs/2005.02637 (2020). arXiv:2005.02637\n[186] Cetin Savkli and Catherine Schwartz. 2021. Random subspace mixture models for interpretable anomaly detection.\nCoRR abs/2108.06283 (2021). arXiv:2108.06283\n[187] ThomasSchlegl,StefanSchlegl,NikolaiWest,andJochenDeuse.2021.Scalableanomalydetectioninmanufacturing\nsystems using an interpretable deep learning approach.Procedia CIRP104 (2021), 1547\u20131552.\n[188] Bernhard Sch\u00f6lkopf, Robert C. Williamson, Alex Smola, John Shawe-Taylor, and John Platt. 1999. Support vector\nmethod for novelty detection. InProceedings of the 12th International Conference on Neural Information Processing\nSystems (Denver, CO) (NIPS\u201999, Vol. 12). MIT Press, Cambridge, MA, USA, 582\u2013588.\n[189] Jonas Herskind Sejr and Anna Schneider-Kamp. 2021. Explainable outlier detection: What, for whom and why?\nMachine Learning with Applications6 (2021), 100172.\n[190] RamprasaathR.Selvaraju,MichaelCogswell,AbhishekDas,RamakrishnaVedantam,DeviParikh,andDhruvBatra.\n2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. InProceedings of the IEEE\nInternational Conference on Computer Vision. 618\u2013626.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "8ea7091f-1992-468d-9a95-0b44b881be33": "A Survey on Explainable Anomaly Detection 23:53\n[191] Pavel Senin, Jessica Lin, Xing Wang, Tim Oates, Sunil Gandhi, Arnold P. Boedihardjo, Crystal Chen, and Susan\nFrankenstein. 2015. Time series anomaly discovery with grammar-based compression. InProceedings of the EDBT.\n481\u2013492.\n[192] Oscar Serradilla, Ekhi Zugasti, Julian Ramirez de Okariz, Jon Rodriguez, and Urko Zurutuza. 2021. Adaptable and\nexplainable predictive maintenance: Semi-supervised deep learning for anomaly detection and diagnosis in press\nmachine data.Applied Sciences11, 16 (2021), 7376.\n[193] MdAmranSiddiqui,AlanFern,ThomasG.Dietterich,andWeng-KeenWong.2019.Sequentialfeatureexplanations\nfor anomaly detection.ACM Transactions on Knowledge Discovery from Data (TKDD)13, 1 (2019), 1\u201322.\n[194] Md Amran Siddiqui, Jack W. Stokes, Christian Seifert, Evan Argyle, Robert McCann, Joshua Neil, and Justin Carroll.\n2019. Detecting cyber attacks using anomaly detection with explanations and expert feedback. InProceedings of\nthe ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP\u201919). IEEE,\n2872\u20132876.\n[195] ArnoSiebes,JillesVreeken,andMatthijsvanLeeuwen.2006.Itemsetsthatcompress.In Proceedingsofthe2006SIAM\nInternational Conference on Data Mining. SIAM, 395\u2013406.\n[196] JohnSipple.2020.Interpretable,multidimensional,multimodalanomalydetectionwithnegativesamplingfordetec-\ntion of device failure. InProceedings of the International Conference on Machine Learning. PMLR, 9016\u20139025.\n[197] JohnSippleandAbdouYoussef.2022.Ageneral-purposemethodforapplyingexplainableAIforanomalydetection.\nInProceedings of the Foundations of Intelligent Systems: 26th International Symposium, ISMIS 2022. Springer, 162\u2013174.\n[198] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. Fooling lime and shap: Ad-\nversarial attacks on post hoc explanation methods. InProceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety. 180\u2013186.\n[199] Giulia Slavic, Pablo Marin, David Martin, Lucio Marcenaro, and Carlo Regazzoni. 2021. Interpretable anomaly de-\ntection using a generalized Markov Jump particle filter. InProceedings of the 2021 IEEE International Conference on\nAutonomous Systems (ICAS\u201921). IEEE, 1\u20135.\n[200] Koen Smets and Jilles Vreeken. 2011. The odd one out: Identifying and characterising anomalies. InProceedings of\nthe 2011 SIAM International Conference on Data Mining. SIAM, 804\u2013815.\n[201] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\u00e9gas, and Martin Wattenberg. 2017. Smoothgrad: Removing\nnoise by adding noise. InICML Workshop on Visualization for Deep Learning ICML.\n[202] Gr\u00e9gorySmits,Marie-JeanneLesot,V\u00e9ronneYepmoTchaghe,andOlivierPivert.2022.PANDA:Human-in-the-loop\nanomaly detection and explanation. InProceedings of the Information Processing and Management of Uncertainty in\nKnowledge-Based Systems: 19th International Conference, IPMU 2022. Springer, 720\u2013732.\n[203] Fei Song, Yanlei Diao, Jesse Read, Arnaud Stiegler, and Albert Bifet. 2018. EXAD: A system for explainable anomaly\ndetection on big data traces. InProceedings of the 2018 IEEE International Conference on Data Mining Workshops\n(ICDMW\u201918).",
        "931c9110-7793-4da1-b75e-107d082fd7c2": "2017. Smoothgrad: Removing\nnoise by adding noise. InICML Workshop on Visualization for Deep Learning ICML.\n[202] Gr\u00e9gorySmits,Marie-JeanneLesot,V\u00e9ronneYepmoTchaghe,andOlivierPivert.2022.PANDA:Human-in-the-loop\nanomaly detection and explanation. InProceedings of the Information Processing and Management of Uncertainty in\nKnowledge-Based Systems: 19th International Conference, IPMU 2022. Springer, 720\u2013732.\n[203] Fei Song, Yanlei Diao, Jesse Read, Arnaud Stiegler, and Albert Bifet. 2018. EXAD: A system for explainable anomaly\ndetection on big data traces. InProceedings of the 2018 IEEE International Conference on Data Mining Workshops\n(ICDMW\u201918). 1435\u20131440.DOI:https://doi.org/10.1109/ICDMW.2018.00204\n[204] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. 2014. Striving for simplicity:\nThe all convolutional net. InICLR workshop track.\n[205] MukundSundararajanandAmirNajmi.2020.ThemanyShapleyvaluesformodelexplanation.In Proceedingsofthe\nInternational Conference on Machine Learning. PMLR, 9269\u20139278.\n[206] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. InProceedings of\nthe International Conference on Machine Learning. PMLR, 3319\u20133328.\n[207] StanislawSzymanowicz,JamesCharles,andRobertoCipolla.2021.X-MAN:Explainingmultiplesourcesofanomalies\nin video. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3224\u20133232.\n[208] Stanislaw Szymanowicz, James Charles, and Roberto Cipolla. 2022. Discrete neural representations for explainable\nanomaly detection. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 148\u2013156.\n[209] NaoyaTakeishi.2019.Shapleyvaluesofreconstructionerrorsofpcaforexplaininganomalydetection.In Proceedings\nof the 2019 International Conference on Data Mining Workshops (ICDMW\u201919). IEEE, 793\u2013798.\n[210] Jinying Zou and Ovanes L. Petrosian. 2020. Explainable AI: Using Shapley Value to Explain Complex Anomaly De-\ntection ML-Based Systems. InMachine Learning and Artificial Intelligence - Proceedings of MLIS 2020, Virtual Event,\nOctober 25-28, 2020 (Frontiers in Artificial Intelligence and Applications), Vol. 332. IOS Press, 152\u2013164.\n[211] SarahTan,MatveySoloviev,GilesHooker,andMartinT.Wells.2020.Treespaceprototypes:Anotherlookatmaking\ntree ensembles interpretable. InProceedings of the 2020 ACM-IMS on Foundations of Data Science Conference. 23\u201334.\n[212] Arijit Ukil, Soma Bandyoapdhyay, Chetanya Puri, and Arpan Pal. 2016. IoT healthcare analytics: The importance of\nanomalydetection.In Proceedingsofthe2016IEEE30thInternationalConferenceonAdvancedInformationNetworking\nand Applications (AINA\u201916). IEEE, 994\u2013997.\n[213] Karel Vacul\u00edk and Lubo\u0161 Popel\u00ednsk`y. 2016. DGRMiner: Anomaly detection and explanation in dynamic graphs. In\nProceedings of the International Symposium on Intelligent Data Analysis. Springer, 308\u2013319.\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023.",
        "f261beb1-b8da-4a92-b5f4-78baa915d383": "23:54 Z. Li et al.\n[214] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.Journal of Machine Learning\nResearch9, 11 (2008).\n[215] Shashanka Venkataramanan, Kuan-Chuan Peng, Rajat Vikram Singh, and Abhijit Mahalanobis. 2020. Attention\nguided anomaly localization in images. InProceedings of the European Conference on Computer Vision. Springer,\n485\u2013503.\n[216] NguyenXuanVinh,JeffreyChan,SimoneRomano,JamesBailey,ChristopherLeckie,KotagiriRamamohanarao,and\nJian Pei. 2016. Discovering outlying aspects in large datasets.Data Mining and Knowledge Discovery30, 6 (2016),\n1520\u20131555.\n[217] Paul Voigt and Axel Von dem Bussche. 2017. The eu general data protection regulation (gdpr).A Practical Guide, 1st\nEd., Cham: Springer International Publishing10, 3152676 (2017), 10\u20135555.\n[218] Laura von Rueden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach, Raoul Heese, Birgit\nKirsch,JuliusPfrommer,AnnikaPick,RajkumarRamamurthy,MichalWalczak,JochenGarcke,ChristianBauckhage,\nand Jannis Schuecker. 2023. Informed machine learning \u2013 A taxonomy and survey of integrating prior knowledge\ninto learning systems.IEEE Transactions on Knowledge and Data Engineering35, 1 (2023), 614\u2013633.\n[219] Chongke Wu, Sicong Shao, Cihan Tunc, Pratik Satam, and Salim Hariri. 2022. An explainable and efficient deep\nlearning framework for video anomaly detection.Cluster Computing25 (2022), 2715\u20132737.\n[220] Tung-Yu Wu and You-Ting Wang. 2021. Locally interpretable one-class anomaly detection for credit card fraud de-\ntection. InProceedings of the 2021 International Conference on Technologies and Applications of Artificial Intelligence\n(TAAI\u201921). IEEE, 25\u201330.\n[221] Hongzuo Xu, Yijie Wang, Songlei Jian, Zhenyu Huang, Yongjun Wang, Ning Liu, and Fei Li. 2021. Beyond outlier\ndetection:Outlierinterpretationbyattention-guidedtripletdeviationnetwork.In ProceedingsoftheWebConference\n2021. 1328\u20131339.\n[222] Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael I. Jordan. 2009. Detecting large-scale system\nproblemsbyminingconsolelogs.In ProceedingsoftheACMSIGOPS22ndSymposiumonOperatingSystemsPrinciples .\n117\u2013132.\n[223] Qibo Yang, Jaskaran Singh, and Jay Lee. 2019. Isolation-based feature selection for unsupervised outlier detection.\nIn Proceedings of the Annual Conference of the PHM Society.\n[224] EmmanuelYashchin.1993.PerformanceofCUSUMcontrolschemesforseriallycorrelatedobservations. Technomet-\nrics35, 1 (1993), 37\u201352.\n[225] V\u00e9ronne Yepmo, Gr\u00e9gory Smits, and Olivier Pivert. 2022. Anomaly explanation: A review.Data and Knowledge En-\ngineering 137 (2022), 101946.\n[226] Luca Zancato, Alessandro Achille, Giovanni Paolini, Alessandro Chiuso, and Stefano Soatto. 2021. STRIC:\nStacked residuals of interpretable components for time series anomaly detection.CoRR abs/2202.12457 (2021).\narXiv:2202.12457\n[227] Matthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. InProceedings of\nthe European Conference on Computer Vision. Springer, 818\u2013833.\n[228] Ji Zhang, Meng Lou, Tok Wang Ling, and Hai Wang. 2004. HOS-miner: A system for detecting outlying subspaces\nof high-dimensional data.",
        "16fae200-1494-4388-9838-b560f6c31582": "2022. Anomaly explanation: A review.Data and Knowledge En-\ngineering 137 (2022), 101946.\n[226] Luca Zancato, Alessandro Achille, Giovanni Paolini, Alessandro Chiuso, and Stefano Soatto. 2021. STRIC:\nStacked residuals of interpretable components for time series anomaly detection.CoRR abs/2202.12457 (2021).\narXiv:2202.12457\n[227] Matthew D. Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. InProceedings of\nthe European Conference on Computer Vision. Springer, 818\u2013833.\n[228] Ji Zhang, Meng Lou, Tok Wang Ling, and Hai Wang. 2004. HOS-miner: A system for detecting outlying subspaces\nof high-dimensional data. InProceedings of the 30th International Conference on Very Large Data Bases (VLDB\u201904).\nMorgan Kaufmann Publishers Inc., 1265\u20131268.\n[229] Jianlong Zhou, Amir H. Gandomi, Fang Chen, and Andreas Holzinger. 2021. Evaluating the quality of machine\nlearning explanations: A survey on methods and metrics.Electronics10, 5 (2021), 593.\n[230] YingyingZhu,NanditaM.Nayak,andAmitK.Roy-Chowdhury.2012.Context-awareactivityrecognitionandanom-\naly detection in video.IEEE Journal of Selected Topics in Signal Processing7, 1 (2012), 91\u2013101.\n[231] Arthur Zimek, Erich Schubert, and Hans-Peter Kriegel. 2012. A survey on unsupervised outlier detection in high-\ndimensionalnumericaldata. Statistical AnalysisandDataMining:TheASADataScience Journal 5,5(2012),363\u2013387.\n[232] Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the elastic net.Journal of the Royal\nStatistical Society: Series B (Statistical Methodology)67, 2 (2005), 301\u2013320.\nReceived 13 October 2022; revised 8 May 2023; accepted 10 July 2023\nACM Transactions on Knowledge Discovery from Data, Vol. 18, No. 1, Article 23. Publication date: September 2023."
    },
    "relevant_docs": {
        "b9b5d740-3d20-4d46-890b-0c18cbb7dc4a": [
            "f5e9b333-29ef-4258-8867-5f7a1ef70c58"
        ],
        "09d959c6-ec54-46bc-b6c1-4b1ace621611": [
            "98c9108d-d266-4df3-bfe7-81e4dd83c837"
        ],
        "2147e747-6233-494a-8c77-b739e6f52035": [
            "101a0975-b6e0-44e3-ad83-568263fa4a2a"
        ],
        "c5893a8a-4171-44c9-bffa-356b348eebf4": [
            "acc7a58c-7717-48ed-9a6e-7a821b85125d"
        ],
        "05909fc4-dd28-4c1c-aa28-13c6998ac6da": [
            "b2f6cb1f-ca2d-42ba-8377-6d3abb260c84"
        ],
        "01e8cbec-973e-4feb-bccb-81107df3d6d0": [
            "2d3e88a2-afb3-4ebc-8c48-1cc425215b85"
        ],
        "2388a10b-b564-4fca-a806-8d43cfec0a1b": [
            "ef9fc041-ed1e-4604-ab1a-4fe6dcfa2aac"
        ],
        "cb5ef1da-33c5-41de-9529-675d298b5bda": [
            "914b7e73-72e5-4267-aac5-7f58b4d26462"
        ],
        "22964743-9670-4038-8962-391669062862": [
            "af07a6df-4a49-4112-a27c-ae6d4e6a3cf4"
        ],
        "dd685878-78bc-436d-8d71-7fba439e7c75": [
            "e202dd06-ea79-4aac-9b10-ea8696b82ed0"
        ],
        "07df230a-a373-468a-9493-a22337ed75ca": [
            "b1d91889-c235-4928-aedb-72607da5c600"
        ],
        "995aa4b7-c7f1-4cc3-8327-2df3ba861a18": [
            "ac2bccbf-d5b0-4c1e-8662-8180632ce95d"
        ],
        "aa6ad1b1-72bc-4941-aae6-0f8d8e0fb94e": [
            "fd1e9c74-572d-4e55-b63a-0a27837561bc"
        ],
        "6c743cc9-bb68-47c8-bbfc-b9b7a75b4554": [
            "76eb3feb-d02d-4e0a-92b1-4ee08e4a8753"
        ],
        "f9e91ac5-bb19-4fd5-beb3-6c742e9dbcaf": [
            "1309d7b4-330c-43a5-bf6b-71428fb74425"
        ],
        "280e4ec6-f5e8-481d-89c8-7155403ba06a": [
            "8ea964a5-950c-4a5c-8439-9c4d65ac7e40"
        ],
        "b460307f-91f1-4057-ad51-20c42a0e4bcf": [
            "25c862eb-25d5-4893-b1d7-5532c2444447"
        ],
        "08224ff3-4300-4c9a-99bd-03b92f168c7a": [
            "b49939b4-6343-44b4-b3bd-5159108d3d92"
        ],
        "79afba5c-7b84-4dcd-b990-3c7a3b27d04a": [
            "dfdca2dd-1596-4ba0-a11c-de0c92753e26"
        ],
        "a6af8e49-25dc-4c5a-9f80-61bc476243f0": [
            "2ff9a8e3-8c89-4ed8-8803-043f07eec19c"
        ],
        "93d1a92c-9a3f-4b5d-80d0-2293da380d00": [
            "65115c8a-d8f8-443c-8a14-2d2503622cbd"
        ],
        "21a844cc-10ef-4257-b68b-87ab2d53c606": [
            "b4ec0ab9-90e9-4fb9-982a-4cb5d69ed573"
        ],
        "8057ceb3-cd30-470d-a3f3-4a8a55922b9d": [
            "7155fdc6-0f1f-4e63-a7fc-363bbb083fa7"
        ],
        "2d0adb33-d061-4f3d-9920-1a0d40826186": [
            "dba908b2-7821-48c3-8607-d73db324c200"
        ],
        "0ac8c3d8-a465-4430-9c41-225dcb9c6318": [
            "a7075e65-9f5a-4045-a8b8-9e36eac2d504"
        ],
        "175042ce-1cd9-427f-892b-ef26b3aa6f75": [
            "95169330-d160-4f6b-bc95-f4d216625120"
        ],
        "07df516b-d0c2-45c9-bbcf-79bd8157233c": [
            "0fca2d36-1cd3-44d0-b580-9f50a29fe9d5"
        ],
        "9edb8484-c9bf-4d79-b8dd-fb19e2f8e8ef": [
            "b341de35-568c-4366-86ae-8bbea5be7152"
        ],
        "3f49b118-c6a9-49f0-84a0-bf1f86a62394": [
            "5299651d-c045-4599-8f4b-b338b55212ba"
        ],
        "aa632ad6-427e-4933-a11e-450aceb4d198": [
            "1e61b6f3-864b-452e-8f9e-f06ba757c577"
        ],
        "6372015d-90bc-430c-87c7-38d608a805dd": [
            "cccd47cd-ca94-4fd1-a218-8f33b3387898"
        ],
        "7a5dbe1e-273b-4618-86e6-c5c4bdb21f54": [
            "eb1cc871-947a-4430-94be-36e0c72cbf09"
        ],
        "1501b40d-d1fd-4d42-b26c-e0ad8da03b9b": [
            "f38bdf2b-b902-4c9c-a533-6448ed26027f"
        ],
        "c339370a-a5cb-44e8-8a49-de516cbeefc2": [
            "5d097baf-4401-4e89-87bb-573e24ab7a9c"
        ],
        "9d7a180f-977b-42a6-b86d-9ba9725808b6": [
            "2007e46e-b291-4169-9933-fcd935c7879b"
        ],
        "8d64785e-e7e2-4fe5-a09d-6f5f2c2ae227": [
            "9c301e2e-dbee-4836-96d1-d681a8f607e5"
        ],
        "e6d0023e-a10d-4fa1-a9f7-ff644fe71e72": [
            "c76c1a4d-e5e1-4413-bf50-7f59b0237ba6"
        ],
        "e91f3272-439f-473e-acda-6e94104e86eb": [
            "fa6a13f9-a17a-40ac-a12c-803dd5b04c28"
        ],
        "582ad1a8-242b-4890-8483-c5c56f613495": [
            "dcd38308-b6fd-45ea-b9e1-24f38dd3781c"
        ],
        "e54c3929-f1fb-4bd0-8323-c2da517eac65": [
            "9301bd08-a990-4878-878a-fd3f9d8c26d5"
        ],
        "85536627-76b7-4dd2-a2b9-b2f225aa50ca": [
            "99c1729c-d025-4504-a2ae-775bf01fc9ea"
        ],
        "ef9807ae-f1d6-4466-9b09-d2cb963274a3": [
            "b3787f0a-38c3-4e4a-a0f9-a94a02e1ffa8"
        ],
        "a5b416f6-60d0-4092-ae30-d5f283b8288b": [
            "433496c2-2c99-47ce-a6b3-f04902d07f81"
        ],
        "e6ca1657-8d41-4d9a-94a8-4c39ea0b8b69": [
            "54918677-3e43-4ce4-9c7c-a79771f999e2"
        ],
        "b62d382a-ad7b-40c6-84b9-b53cd6694695": [
            "9c65a62d-bd3f-4e0b-9283-7cf949784f59"
        ],
        "5b712909-bc36-4395-ba16-a952d0d437b5": [
            "a0928b60-400d-42b5-b536-f9216a3e4a42"
        ],
        "765eb191-9773-4c8e-908b-c326cab2a203": [
            "7e876800-dbfe-496a-a9d0-7b07a076c44f"
        ],
        "636dff4c-d22c-4c75-8838-9bc95968f5cb": [
            "9ff61e16-98ac-4759-a25a-beaf123efd1c"
        ],
        "307dd3d0-4c5d-4a2a-bf34-8caeee36e0f0": [
            "d17cbaf5-d0fb-42ec-9f84-7eb368727f1b"
        ],
        "4b8ed432-4591-4b48-8c4d-7df0de2542b1": [
            "db112105-1dc6-440d-8f4b-c7a1518056ab"
        ],
        "f82b0d77-a0c0-4018-9c06-0335cdc6d72b": [
            "79cef667-a8be-41e6-9c12-8b702660af8a"
        ],
        "cc6b7390-7a6c-4899-a011-7a5b2d7134ca": [
            "0f4ced3b-2694-45b6-83d7-d9d91b03b578"
        ],
        "c35427f8-fb6b-4946-8821-3579537332ea": [
            "b811ca39-9ae9-4793-9138-bf94b3a34015"
        ],
        "320d29cf-4389-473a-a5e5-01558549a629": [
            "003bc32e-9559-41c2-bf7c-ef3418dc3389"
        ],
        "fdc4d8b3-75f6-4eba-a1c1-8cb234027539": [
            "bea90290-45df-4b13-8ae8-f605b1e1cba4"
        ],
        "1ea72e5c-584e-47d4-bea6-7fd81fc6407b": [
            "7ef56596-0b89-40ec-a5e2-5eff2bc1307b"
        ],
        "af381815-6128-405d-9c30-89e640dfac7c": [
            "3dbf6521-44ea-407f-acb2-4dcf131e9828"
        ],
        "09010e7e-f901-4c35-ab95-b8d0a32a5608": [
            "62e1c0b6-afe8-46f4-9144-16df8bcf82a0"
        ],
        "c0bb0766-cf16-4efe-b850-164dd62b7772": [
            "11751f37-026c-46cb-8bad-d0343ebcbd5d"
        ],
        "378affac-977a-4811-b085-3c048df38916": [
            "4f6fe7a0-5396-47b7-8fbb-e8857cbdb240"
        ],
        "981d752b-a75a-49d3-b7c7-da40acf4796e": [
            "66b413c3-f304-4eb3-bcc9-7b40f9a0926a"
        ],
        "08424a55-d242-4f8c-9e09-0109fd24abb0": [
            "5b2b0021-6130-4b1b-88fe-d5bac9c48778"
        ],
        "900f5db1-f5b2-4167-8b32-4d8056c91d19": [
            "942fcfb1-9910-487d-bd0c-aa5d095958cb"
        ],
        "1b778f33-2062-4865-8047-c2ebdeb413f2": [
            "cc86fcbf-8c8c-4ba8-a8fa-bb577b3527ad"
        ],
        "38214c0b-28a6-49ab-b858-572e97dc53ea": [
            "84252ad3-5b9c-4d01-a4b1-cf0411c2af4b"
        ],
        "cff4fd4b-6f27-490f-9b5e-657dc0f9cca1": [
            "40edd60c-2c65-4876-b5bb-f82138144635"
        ],
        "b26c98d4-c471-4ba0-b2db-d91f87d605fe": [
            "85b54741-3d5a-4fbb-9c87-717d74b5010d"
        ],
        "f90d20b3-b8d4-4906-bc90-0896858320f3": [
            "945aefbb-2a01-4474-94c0-e17a83915ae1"
        ],
        "47cf483e-f8f3-4a83-9e57-d6a24508d403": [
            "b7a60f0b-88d6-4272-a4b8-84d179ef48d6"
        ],
        "4c0546fc-f53e-406c-a9d8-a3cbbd3c7fd0": [
            "4b5f754e-8a56-4768-b08d-92331c48518b"
        ],
        "6ca70978-369b-4a59-9cad-f284f2aff38d": [
            "e72de9ef-0504-420f-8e58-6b9941a553a2"
        ],
        "8ce9943e-659a-43ee-941a-a32f846718d5": [
            "8ea7091f-1992-468d-9a95-0b44b881be33"
        ],
        "e1ecc853-99a1-42bf-a104-957818dca9b9": [
            "931c9110-7793-4da1-b75e-107d082fd7c2"
        ],
        "4db53b90-902e-426a-86b8-092158996fba": [
            "f261beb1-b8da-4a92-b5f4-78baa915d383"
        ],
        "ac4f0ac1-0a5b-470f-ad54-e6a76c34e65c": [
            "16fae200-1494-4388-9838-b560f6c31582"
        ]
    },
    "mode": "text"
}